try:
    import open3d
except:
    pass
import torch
import src.refine_net.options as options
import src.refine_net.util as util
import torch.optim as optim
import numpy as np
import pandas as pd
import os
from pathlib import Path
from src.refine_net.losses import chamfer_distance
from src.refine_net.data_handler import get_dataset, H5InferenceDataset
from torch.utils.data import DataLoader
from src.refine_net.models import PointNet2Generator
from tensorboardX import SummaryWriter
import time
from tqdm import tqdm
import glob
import re


def find_checkpoint_file(save_path, resume_from):
    """
    æ ¹æ®resume_fromå‚æ•°æŸ¥æ‰¾å¯¹åº”çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
    
    Args:
        save_path: ä¿å­˜ç›®å½•è·¯å¾„
        resume_from: æ¢å¤å‚æ•°ï¼Œæ”¯æŒlatestã€bestã€finalã€iteration_XXXXXXæˆ–æ–‡ä»¶è·¯å¾„
    
    Returns:
        str: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    """
    if os.path.isfile(resume_from):
        # ç›´æ¥æŒ‡å®šäº†æ–‡ä»¶è·¯å¾„
        return resume_from
    
    checkpoint_dir = save_path / 'generators'
    
    if resume_from == 'latest':
        # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
        pattern = str(checkpoint_dir / 'iteration_*.pt')
        checkpoint_files = glob.glob(pattern)
        if not checkpoint_files:
            raise FileNotFoundError(f"åœ¨ {checkpoint_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ£€æŸ¥ç‚¹æ–‡ä»¶")
        
        # æŒ‰iterationæ•°æ’åºï¼Œå–æœ€æ–°çš„
        iterations = []
        for f in checkpoint_files:
            match = re.search(r'iteration_(\d+)\.pt', f)
            if match:
                iterations.append((int(match.group(1)), f))
        
        if not iterations:
            raise FileNotFoundError(f"åœ¨ {checkpoint_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹æ–‡ä»¶")
        
        iterations.sort(key=lambda x: x[0])
        return iterations[-1][1]
    
    elif resume_from == 'best':
        # æŸ¥æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹
        best_file = checkpoint_dir / 'best.pt'
        if best_file.exists():
            return str(best_file)
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ€ä½³æ£€æŸ¥ç‚¹æ–‡ä»¶: {best_file}")
    
    elif resume_from == 'final':
        # æŸ¥æ‰¾æœ€ç»ˆæ£€æŸ¥ç‚¹
        final_file = checkpoint_dir / 'final.pt'
        if final_file.exists():
            return str(final_file)
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ€ç»ˆæ£€æŸ¥ç‚¹æ–‡ä»¶: {final_file}")
    
    elif resume_from.startswith('iteration_'):
        # æŒ‡å®šiterationçš„æ£€æŸ¥ç‚¹
        checkpoint_file = checkpoint_dir / f'{resume_from}.pt'
        if checkpoint_file.exists():
            return str(checkpoint_file)
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_file}")
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„resume_fromå‚æ•°: {resume_from}")


def load_checkpoint(checkpoint_path, model, optimizer=None, device='cuda'):
    """
    åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        model: æ¨¡å‹å¯¹è±¡
        optimizer: ä¼˜åŒ–å™¨å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
        device: è®¾å¤‡
    
    Returns:
        dict: åŒ…å«iterationã€lossç­‰ä¿¡æ¯çš„å­—å…¸
    """
    print(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"æ¨¡å‹çŠ¶æ€å·²åŠ è½½")
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚æœæä¾›ï¼‰
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
    
    # æå–è®­ç»ƒä¿¡æ¯
    start_iteration = checkpoint.get('iteration', 0)
    best_loss = checkpoint.get('best_loss', float('inf'))
    
    print(f"æ£€æŸ¥ç‚¹ä¿¡æ¯:")
    print(f"  - è®­ç»ƒè¿­ä»£æ•°: {start_iteration}")
    print(f"  - æœ€ä½³æŸå¤±: {best_loss:.6f}")
    
    if 'timestamp' in checkpoint:
        print(f"  - ä¿å­˜æ—¶é—´: {checkpoint['timestamp']}")
    
    return {
        'start_iteration': start_iteration,
        'best_loss': best_loss,
        'checkpoint_info': checkpoint
    }


def validate(model, val_dataset, device, args, iteration, writer=None):
    """
    éªŒè¯å‡½æ•°ï¼Œè¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        val_dataset: éªŒè¯æ•°æ®é›†
        device: è®¡ç®—è®¾å¤‡
        args: å‚æ•°é…ç½®
        iteration: å½“å‰è®­ç»ƒè¿­ä»£æ¬¡æ•°
        writer: TensorBoard writer
    """
    model.eval()
    val_losses = []
    val_results = []
    
    print(f"\n=== å¼€å§‹ç¬¬ {iteration} æ¬¡è¿­ä»£éªŒè¯ ===")
    
    with torch.no_grad():
        for i in range(len(val_dataset.sample_indices)):
            gt_pc, noisy_pc = val_dataset[i]
            gt_pc, noisy_pc = gt_pc.to(device), noisy_pc.to(device)
            
            # æ·»åŠ batchç»´åº¦
            noisy_batch = noisy_pc.unsqueeze(0)
            gt_batch = gt_pc.unsqueeze(0)
            
            # æ¨¡å‹é¢„æµ‹
            refined_pc = model(noisy_batch)
            
            # è®¡ç®—æŸå¤±
            val_loss = chamfer_distance(refined_pc, gt_batch, mse=args.mse)
            val_losses.append(val_loss.item())
            
            # ä¿å­˜éªŒè¯ç»“æœåˆ°CSV
            if args.val_save_csv:
                save_validation_to_csv(
                    gt_pc.cpu().numpy(),
                    noisy_pc.cpu().numpy(), 
                    refined_pc[0].cpu().numpy(),
                    args.save_path,
                    iteration,
                    i
                )
            
            # è®°å½•ç»“æœ
            val_results.append({
                'iteration': iteration,
                'sample': i,
                'loss': val_loss.item()
            })
    
    avg_val_loss = np.mean(val_losses)
    print(f"éªŒè¯å¹³å‡æŸå¤±: {avg_val_loss:.6f}")
    
    # TensorBoardè®°å½•
    if writer:
        writer.add_scalar('Loss/Validation', avg_val_loss, iteration)
    
    print(f"=== ç¬¬ {iteration} æ¬¡è¿­ä»£éªŒè¯å®Œæˆ ===\n")
    
    # ä¿å­˜éªŒè¯æ—¥å¿—
    save_validation_log(val_results, args.save_path, iteration)
    
    return avg_val_loss


def save_validation_to_csv(gt_pc, noisy_pc, refined_pc, save_path, iteration, sample_idx):
    """
    ä¿å­˜éªŒè¯ç»“æœç‚¹äº‘åˆ°CSVæ–‡ä»¶
    
    Args:
        gt_pc: GTç‚¹äº‘ (3, N)
        noisy_pc: å™ªå£°ç‚¹äº‘ (3, N)  
        refined_pc: ä¿®æ­£åç‚¹äº‘ (3, N)
        save_path: ä¿å­˜è·¯å¾„
        iteration: è®­ç»ƒè¿­ä»£æ¬¡æ•°
        sample_idx: æ ·æœ¬ç´¢å¼•
    """
    val_dir = save_path / 'validation_results'
    val_dir.mkdir(exist_ok=True)
    
    iteration_dir = val_dir / f'iteration_{iteration:06d}'
    iteration_dir.mkdir(exist_ok=True)
    
    # è½¬æ¢ä¸º (N, 3) æ ¼å¼
    gt_points = gt_pc.T  # (N, 3)
    noisy_points = noisy_pc.T
    refined_points = refined_pc.T
    
    # ä¿å­˜åˆ°CSV
    pd.DataFrame(gt_points, columns=['x', 'y', 'z']).to_csv(
        iteration_dir / f'gt_sample_{sample_idx:03d}.csv', index=False)
    pd.DataFrame(noisy_points, columns=['x', 'y', 'z']).to_csv(
        iteration_dir / f'noisy_sample_{sample_idx:03d}.csv', index=False)
    pd.DataFrame(refined_points, columns=['x', 'y', 'z']).to_csv(
        iteration_dir / f'refined_sample_{sample_idx:03d}.csv', index=False)


def save_validation_log(val_results, save_path, iteration):
    """ä¿å­˜éªŒè¯æ—¥å¿—"""
    log_path = save_path / 'validation_log.csv'
    
    df = pd.DataFrame(val_results)
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™è¿½åŠ ï¼›å¦åˆ™åˆ›å»ºæ–°æ–‡ä»¶
    if log_path.exists():
        existing_df = pd.read_csv(log_path)
        df = pd.concat([existing_df, df], ignore_index=True)
    
    df.to_csv(log_path, index=False)


def inference_iteration(model, inference_dataset, device, args, iteration, writer=None):
    """
    æ¨ç†å‡½æ•°ï¼Œå¯¹éªŒè¯é›†å‰4ä¸ªæ ·æœ¬è¿›è¡Œå®Œæ•´æ¨ç†
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        inference_dataset: æ¨ç†æ•°æ®é›†
        device: è®¡ç®—è®¾å¤‡
        args: å‚æ•°é…ç½®
        iteration: å½“å‰è®­ç»ƒè¿­ä»£æ¬¡æ•°
        writer: TensorBoard writer
    """
    model.eval()
    
    print(f"\n=== å¼€å§‹ç¬¬ {iteration} æ¬¡è¿­ä»£æ¨ç† ===")
    
    inference_dir = args.save_path / 'inference_results' / f'iteration_{iteration:06d}'
    inference_dir.mkdir(parents=True, exist_ok=True)
    
    with torch.no_grad():
        for i in range(len(inference_dataset)):
            data = inference_dataset[i]
            
            # è·å–æ•°æ®
            gt_normalized = data['gt_normalized'].to(device)  # (3, N)
            noisy_normalized = data['noisy_normalized'].to(device)  # (3, N)
            gt_original = data['gt_original']  # (N, 3)
            noisy_original = data['noisy_original']  # (N, 3)
            sample_idx = data['sample_idx']
            num_points = data['num_points']
            
            print(f"å¤„ç†æ ·æœ¬ {sample_idx}, ç‚¹æ•°: {num_points}")
            
            # åˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡ç‚¹äº‘
            if num_points > args.sample_points:
                refined_normalized = process_large_pointcloud(
                    model, noisy_normalized, args.sample_points, device
                )
            else:
                # ç›´æ¥å¤„ç†
                refined_normalized = model(noisy_normalized.unsqueeze(0))[0]  # (3, N)
            
            # åå½’ä¸€åŒ–: (3, N) -> (N, 3)
            refined_normalized_np = refined_normalized.transpose(0, 1).cpu().numpy()
            refined_original = inference_dataset.denormalize_pointcloud(refined_normalized_np)
            
            # ä¿å­˜ç»“æœåˆ°CSV
            pd.DataFrame(gt_original, columns=['x', 'y', 'z']).to_csv(
                inference_dir / f'gt_sample_{sample_idx:03d}.csv', index=False)
            pd.DataFrame(noisy_original, columns=['x', 'y', 'z']).to_csv(
                inference_dir / f'noisy_sample_{sample_idx:03d}.csv', index=False)
            pd.DataFrame(refined_original, columns=['x', 'y', 'z']).to_csv(
                inference_dir / f'refined_sample_{sample_idx:03d}.csv', index=False)
            
            print(f"æ ·æœ¬ {sample_idx} æ¨ç†å®Œæˆ")
    
    print(f"=== ç¬¬ {iteration} æ¬¡è¿­ä»£æ¨ç†å®Œæˆ ===\n")


def process_large_pointcloud(model, noisy_pc, batch_size, device):
    """
    åˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡ç‚¹äº‘
    
    Args:
        model: æ¨¡å‹
        noisy_pc: å™ªå£°ç‚¹äº‘ (3, N)
        batch_size: æ‰¹å¤„ç†å¤§å°
        device: è®¾å¤‡
        
    Returns:
        torch.Tensor: ä¿®æ­£åçš„ç‚¹äº‘ (3, N)
    """
    num_points = noisy_pc.shape[1]
    refined_batches = []
    
    for start_idx in range(0, num_points, batch_size):
        end_idx = min(start_idx + batch_size, num_points)
        
        # è·å–æ‰¹æ¬¡
        batch = noisy_pc[:, start_idx:end_idx]  # (3, batch_size)
        
        # å¦‚æœæ‰¹æ¬¡å¤ªå°ï¼Œè¿›è¡Œpadding
        if batch.shape[1] < batch_size:
            padding_size = batch_size - batch.shape[1]
            # é‡å¤æœ€åå‡ ä¸ªç‚¹è¿›è¡Œpadding
            repeat_indices = torch.randint(0, batch.shape[1], (padding_size,))
            padding = batch[:, repeat_indices]
            batch = torch.cat([batch, padding], dim=1)
        
        # æ¨¡å‹æ¨ç†
        refined_batch = model(batch.unsqueeze(0))[0]  # (3, batch_size)
        
        # ç§»é™¤padding
        if end_idx - start_idx < batch_size:
            refined_batch = refined_batch[:, :end_idx - start_idx]
        
        refined_batches.append(refined_batch)
    
    # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡
    refined_pc = torch.cat(refined_batches, dim=1)
    return refined_pc


def train(args):
    device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu'))
    print(f'device: {device}')

    # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    dummy_pc = torch.zeros((100, 3))  # å ä½ç¬¦
    train_dataset = get_dataset(args.sampling_mode)(dummy_pc, device, args, split='train')
    val_dataset = get_dataset(args.sampling_mode)(dummy_pc, device, args, split='val')
    inference_dataset = H5InferenceDataset(dummy_pc, device, args)
    
    print(f"ä½¿ç”¨æ•°æ®é›†æ¨¡å¼: {args.sampling_mode}")
    print(f"GT H5æ–‡ä»¶: {args.gt_h5_path}")
    print(f"å™ªå£°H5æ–‡ä»¶: {args.noisy_h5_path}")
    print(f"é‡‡æ ·ç‚¹æ•°: {args.sample_points}")

    # åˆå§‹åŒ–æ¨¡å‹
    model = PointNet2Generator(device, args)
    print(f'æ¨¡å‹å‚æ•°æ•°é‡: {util.n_params(model)}')
    model.initialize_params(args.init_var)
    model.to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    
    # TensorBoard
    writer = None
    if args.use_tensorboard:
        writer = SummaryWriter(log_dir=args.save_path / 'tensorboard')
        print(f"TensorBoardæ—¥å¿—ç›®å½•: {args.save_path / 'tensorboard'}")
    
    # æ£€æŸ¥ç‚¹æ¢å¤
    start_iteration = 0
    best_loss = float('inf')
    
    if args.resume_from:
        try:
            # æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_path = find_checkpoint_file(args.save_path, args.resume_from)
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            resume_optimizer = args.resume_optimizer if hasattr(args, 'resume_optimizer') else True
            checkpoint_info = load_checkpoint(
                checkpoint_path, 
                model, 
                optimizer if resume_optimizer else None, 
                device
            )
            
            # è®¾ç½®èµ·å§‹è¿­ä»£
            if args.resume_start_iteration > 0:
                start_iteration = args.resume_start_iteration
                print(f"å¼ºåˆ¶è®¾ç½®èµ·å§‹iterationä¸º: {start_iteration}")
            else:
                start_iteration = checkpoint_info['start_iteration']
            
            best_loss = checkpoint_info['best_loss']
            
            print(f"âœ… æˆåŠŸä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
            print(f"   - èµ·å§‹iteration: {start_iteration}")
            print(f"   - å†å²æœ€ä½³æŸå¤±: {best_loss:.6f}")
            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            print(f"   å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            start_iteration = 0
            best_loss = float('inf')
    
    # è®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, num_workers=0,
                          batch_size=args.batch_size, shuffle=False, drop_last=False)
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    total_iterations = int(len(train_dataset) / args.batch_size)
    global_step = start_iteration
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œæ€»è¿­ä»£æ¬¡æ•°: {total_iterations}")
    if start_iteration > 0:
        print(f"ä»ç¬¬ {start_iteration} æ¬¡è¿­ä»£æ¢å¤è®­ç»ƒ")
    
    start_time = time.time()
    current_best_loss = best_loss
    
    for i, (gt_pc, noisy_pc) in enumerate(tqdm(train_loader, desc="è®­ç»ƒè¿›åº¦")):
        # è°ƒæ•´å®é™…è¿­ä»£è®¡æ•°
        actual_iteration = i + start_iteration
        
        # # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒä¸”å½“å‰è¿­ä»£å°äºèµ·å§‹è¿­ä»£ï¼Œè·³è¿‡
        # if i < start_iteration and start_iteration > 0:
        #     continue
        
        gt_pc, noisy_pc = gt_pc.to(device), noisy_pc.to(device)
        
        model.train()
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­ï¼šåœ¨å™ªå£°ç‚¹äº‘ä¸Šé¢„æµ‹ä¿®æ­£
        refined_pc = model(noisy_pc)
        
        # è®¡ç®—æŸå¤±ï¼šä¿®æ­£åç‚¹äº‘ä¸GTç‚¹äº‘çš„å·®å¼‚
        loss = chamfer_distance(refined_pc, gt_pc, mse=args.mse)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        global_step = actual_iteration

        # TensorBoardè®°å½•è®­ç»ƒæŸå¤±
        if writer and global_step % 10 == 0:
            writer.add_scalar('Loss/Train', loss.item(), global_step)
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if actual_iteration % 1 == 0:
            elapsed_time = time.time() - start_time
            print(f'Iteration: {actual_iteration}; Total: {actual_iteration} / {total_iterations + start_iteration}; '
                  f'Loss: {util.show_truncated(loss.item(), 6)}; '
                  f'Time: {elapsed_time:.2f}s')
        
        # éªŒè¯
        if i > 0 and i % args.val_interval == 0:
            val_loss = validate(model, val_dataset, device, args, actual_iteration, writer)
            
            # æ¨ç†
            if i >= args.inference_start_iteration and i % args.inference_interval == 0:
                inference_iteration(model, inference_dataset, device, args, actual_iteration, writer)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if actual_iteration > 0 and actual_iteration % args.checkpoint_interval == 0:
            checkpoint_path = args.save_path / f'checkpoints/iteration_{actual_iteration:06d}.pt'
            checkpoint_path.parent.mkdir(exist_ok=True)
            torch.save({
                'iteration': actual_iteration,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': loss.item(),
                'best_loss': current_best_loss,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }, checkpoint_path)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = args.save_path / 'final_model.pt'
    torch.save(model.state_dict(), final_model_path)
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹
    final_checkpoint_path = args.save_path / 'final_checkpoint.pt'
    torch.save({
        'iteration': total_iterations + start_iteration,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_loss': current_best_loss,
        'args': vars(args),
        'total_iterations': total_iterations,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }, final_checkpoint_path)
    print(f"ğŸ æœ€ç»ˆæ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")
    
    # å…³é—­TensorBoard
    if writer:
        writer.close()
    
    print("è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    parser = options.get_parser('Train Self-Sampling generator')
    args = options.parse_args(parser)
    train(args)
