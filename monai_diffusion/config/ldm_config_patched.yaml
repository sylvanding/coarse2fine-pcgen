# MONAI 3D Latent Diffusion Model Configuration - Patch-Based Training
# ä½¿ç”¨Patch-Basedè®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æå‡è®­ç»ƒé€Ÿåº¦ï¼ˆ5-10å€ï¼‰
# 
# â­ æ ¸å¿ƒä¼˜åŒ–ï¼šè®­ç»ƒæ—¶ä½¿ç”¨å°patchï¼ˆ96Ã—96Ã—48ï¼‰ï¼Œè€Œä¸æ˜¯å¤„ç†æ•´ä¸ªå¤§ä½“ç§¯
#
# æ¨ç†æ—¶å¯ä»¥ä½¿ç”¨SlidingWindowInfererå¤„ç†ä»»æ„å¤§å°çš„ä½“ç§¯

# æ•°æ®é…ç½®
data:
  # è®­ç»ƒå’ŒéªŒè¯æ•°æ®è·¯å¾„
  train_data_dir: "/repos/datasets/batch_simulation_microtubule_20251017_2048_nifti/train"
  val_data_dir: "/repos/datasets/batch_simulation_microtubule_20251017_2048_nifti/val"
  
  # â­â­â­ å…³é”®ï¼šè¿™æ˜¯è®­ç»ƒpatchçš„å¤§å°ï¼Œä¸æ˜¯åŸå§‹æ•°æ®å¤§å°ï¼
  # å¯¹äº256Ã—256Ã—64çš„æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨96Ã—96Ã—48çš„patchè¿›è¡Œè®­ç»ƒ
  # è¿™æ ·å¯ä»¥ï¼š
  #   1. è®­ç»ƒé€Ÿåº¦æå‡5-10å€
  #   2. batch_sizeå¯ä»¥å¢å¤§åˆ°4-8
  #   3. æ˜¾å­˜å ç”¨é™ä½70-80%
  #   4. æ¯ä¸ªepochçœ‹åˆ°æ›´å¤šæ ·åŒ–çš„æ•°æ®ï¼ˆæ•°æ®å¢å¼ºæ•ˆæœï¼‰
  voxel_size: [96, 96, 48]  # Patchå¤§å°
  # å¯é€‰é…ç½®:
  # voxel_size: [128, 128, 64]  # æ›´å¤§çš„patchï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰
  # voxel_size: [64, 64, 32]    # æ›´å°çš„patchï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰
  
  # æ•°æ®åŠ è½½è®¾ç½®
  cache_rate: 0.1  # ç¼“å­˜10%æ•°æ®åŠ é€Ÿè®­ç»ƒ
  num_workers: 4   # å¢åŠ workeræ•°é‡
  pin_memory: true # å¯ç”¨pin_memoryåŠ é€Ÿæ•°æ®ä¼ è¾“
  
  # æ•°æ®å¢å¼ºé…ç½®
  augmentation:
    enabled: true
    use_patch_based: true  # â­ å¯ç”¨Patch-Basedè®­ç»ƒ
    use_center_crop_for_val: true  # éªŒè¯æ—¶ä½¿ç”¨ä¸­å¿ƒè£å‰ª
    random_flip_prob: 0.5

# AutoencoderKL é…ç½®
autoencoder:
  # ç½‘ç»œæ¶æ„
  spatial_dims: 3
  in_channels: 1
  out_channels: 1
  
  # å¯¹äº96Ã—96Ã—48çš„patchï¼Œä½¿ç”¨é€‚ä¸­çš„é€šé“æ•°
  num_channels: [32, 64, 64]  # å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
  # å¦‚æœæ˜¾å­˜å……è¶³ï¼Œå¯ä»¥ä½¿ç”¨: [32, 64, 128]
  # å¦‚æœæ˜¾å­˜ç´§å¼ ï¼Œå¯ä»¥ä½¿ç”¨: [16, 32, 64]
  
  latent_channels: 3
  num_res_blocks: 1
  norm_num_groups: 16
  
  # Attentioné…ç½®ï¼šåªåœ¨æœ€ä½åˆ†è¾¨ç‡å±‚ä½¿ç”¨
  attention_levels: [false, false, true]
  
  # â­ ä¸‹é‡‡æ ·é…ç½®ï¼šå¯¹äº96Ã—96Ã—48ï¼Œä½¿ç”¨é€‚ä¸­çš„ä¸‹é‡‡æ ·
  # æ€»ä¸‹é‡‡æ ·ï¼š2 Ã— 2 Ã— 2 = 8å€
  # æœ€ç»ˆlatentå°ºå¯¸ï¼š[96/8, 96/8, 48/8] = [12, 12, 6]
  downsample_factors: [2, 2]
  initial_downsample_factor: 2
  
  # å¦‚æœpatchæ›´å¤§ï¼ˆå¦‚128Ã—128Ã—64ï¼‰ï¼Œå¯ä»¥è€ƒè™‘æ›´æ¿€è¿›çš„ä¸‹é‡‡æ ·ï¼š
  # downsample_factors: [4, 2]
  # initial_downsample_factor: 2
  # æ€»ä¸‹é‡‡æ ·ï¼š2 Ã— 4 Ã— 2 = 16å€
  
  # è®­ç»ƒé…ç½®
  training:
    n_epochs: 500
    
    # â­ Batch sizeå¯ä»¥æ˜¾è‘—å¢å¤§ï¼ˆä»1å¢åŠ åˆ°4-8ï¼‰
    batch_size: 4
    # å¦‚æœæ˜¾å­˜å……è¶³: batch_size: 8
    # å¦‚æœæ˜¾å­˜ç´§å¼ : batch_size: 2
    
    learning_rate: 1.0e-4
    
    # æŸå¤±æƒé‡
    adv_weight: 0.01
    perceptual_weight: 0.001
    kl_weight: 1.0e-6
    
    # æ˜¾å­˜ä¼˜åŒ–é€‰é¡¹
    use_perceptual_loss: true  # ç°åœ¨å¯ä»¥å¯ç”¨ï¼ˆpatchå°ï¼Œæ˜¾å­˜å……è¶³ï¼‰
    use_gradient_checkpointing: false  # é€šå¸¸ä¸éœ€è¦äº†
    gradient_accumulation_steps: 1  # å¦‚éœ€æ¨¡æ‹Ÿæ›´å¤§batchï¼Œå¯è®¾ä¸º2-4
    
    # å¯¹æŠ—è®­ç»ƒ
    autoencoder_warm_up_n_epochs: 5
    
    # åˆ¤åˆ«å™¨é…ç½®
    discriminator:
      num_layers_d: 3
      num_channels: 32
    
    # éªŒè¯å’Œä¿å­˜
    val_interval: 10
    save_interval: 50
    
    # å¿«é€Ÿå¼€å‘æ¨¡å¼ï¼ˆè°ƒè¯•ç”¨ï¼‰
    fast_dev_run: false
    fast_dev_run_batches: 2
    
  # Checkpointè·¯å¾„
  checkpoints:
    output_dir: "outputs/ckpt/autoencoder_patched"
    save_best: true
    resume_from: null
  
  # æ—¥å¿—é…ç½®
  logging:
    log_dir: "outputs/logs/autoencoder_patched"
    log_interval: 20
    visualize_interval: 10
    num_visualize_samples: 4
    
    # æ»‘åŠ¨çª—å£æ¨ç†é…ç½®ï¼ˆç”¨äºå¯è§†åŒ–å®Œæ•´å›¾åƒé‡å»ºï¼‰
    # â­ å¯¹äºpatch-basedè®­ç»ƒï¼Œè¿™ä¸ªç‰¹åˆ«é‡è¦ï¼
    # è®­ç»ƒæ—¶ä½¿ç”¨å°patchï¼ˆå¦‚96Ã—96Ã—48ï¼‰ï¼Œæ¨ç†æ—¶å¯ä»¥å¤„ç†å®Œæ•´å¤§å°ï¼ˆå¦‚256Ã—256Ã—64ï¼‰
    use_sliding_window: true  # æ˜¯å¦å¯ç”¨æ»‘åŠ¨çª—å£æ¨ç†è¿›è¡Œå¯è§†åŒ–
    sliding_window_roi_size: [96, 96, 48]  # ROIå¤§å°ï¼Œåº”è¯¥ä¸è®­ç»ƒpatchå¤§å°ä¸€è‡´
    sliding_window_batch_size: 4  # æ»‘åŠ¨çª—å£æ¨ç†çš„æ‰¹æ¬¡å¤§å°

# Diffusion Model é…ç½®
diffusion:
  # UNetæ¶æ„ï¼ˆåœ¨latent spaceä¸Šè®­ç»ƒï¼‰
  spatial_dims: 3
  in_channels: 3  # å¿…é¡»åŒ¹é…autoencoderçš„latent_channels
  out_channels: 3
  num_channels: [32, 64, 64]
  attention_levels: [false, true, true]
  num_head_channels: [0, 64, 64]
  num_res_blocks: 1
  
  # è°ƒåº¦å™¨é…ç½®
  scheduler:
    num_train_timesteps: 1000
    schedule: "scaled_linear_beta"
    beta_start: 0.0015
    beta_end: 0.0195
  
  # è®­ç»ƒé…ç½®
  training:
    n_epochs: 500
    batch_size: 4  # Latent spaceæ›´å°ï¼Œå¯ä»¥ç”¨æ›´å¤§çš„batch
    learning_rate: 1.0e-4
    
    # éªŒè¯å’Œä¿å­˜
    val_interval: 10
    save_interval: 50
    
    # æ ·æœ¬ç”Ÿæˆ
    generate_samples_interval: 10
    num_samples_to_generate: 4
    
    # å¿«é€Ÿå¼€å‘æ¨¡å¼
    fast_dev_run: false
    fast_dev_run_batches: 2
  
  # Checkpointè·¯å¾„
  checkpoints:
    autoencoder_path: "outputs/ckpt/autoencoder_patched/best_model.pt"
    output_dir: "outputs/ckpt/diffusion_patched"
    save_best: true
    resume_from: null
  
  # æ—¥å¿—é…ç½®
  logging:
    log_dir: "outputs/logs/diffusion_patched"
    log_interval: 20
    visualize_interval: 10
    num_visualize_samples: 4

# é‡‡æ ·é…ç½®
sampling:
  num_inference_steps: 1000
  num_samples: 16
  
  # è¾“å‡ºé…ç½®
  output_dir: "outputs/samples/diffusion_patched"
  save_format: "nifti"
  
  # ç‚¹äº‘è½¬æ¢é…ç½®
  pointcloud:
    num_points: 100000
    threshold: 0.1
    method: "probabilistic"

# è®¾å¤‡é…ç½®
device:
  use_cuda: true
  gpu_id: 0
  mixed_precision: true  # å¿…é¡»å¯ç”¨ï¼ŒèŠ‚çœçº¦50%æ˜¾å­˜

# éšæœºç§å­
seed: 42

# ================================
# ğŸš€ Patch-Basedè®­ç»ƒä¼˜åŠ¿æ€»ç»“
# ================================
#
# 1. **è®­ç»ƒé€Ÿåº¦**: æ¯”å¤„ç†256Ã—256Ã—64å¿«5-10å€
# 2. **Batch size**: å¯ä»¥ä»1å¢åŠ åˆ°4-8ï¼ŒGPUåˆ©ç”¨ç‡æå‡
# 3. **æ˜¾å­˜å ç”¨**: é™ä½70-80%
# 4. **æ•°æ®å¢å¼º**: æ¯ä¸ªepochçœ‹åˆ°ä¸åŒçš„è£å‰ªåŒºåŸŸ
# 5. **è´¨é‡**: ä¸å—å½±å“ï¼ˆç”šè‡³å¯èƒ½æ›´å¥½ï¼‰
#
# ================================
# ğŸ“‹ æ¨ç†æ—¶å¤„ç†å®Œæ•´ä½“ç§¯
# ================================
#
# è®­ç»ƒåœ¨å°patchä¸Šè¿›è¡Œï¼Œä½†æ¨ç†æ—¶å¯ä»¥å¤„ç†ä»»æ„å¤§å°çš„ä½“ç§¯ã€‚
# ä½¿ç”¨SlidingWindowInfererè¿›è¡Œæ»‘åŠ¨çª—å£æ¨ç†ï¼š
#
# ```python
# from monai.inferers import SlidingWindowInferer
#
# inferer = SlidingWindowInferer(
#     roi_size=(96, 96, 48),  # ä¸è®­ç»ƒpatchä¸€è‡´
#     sw_batch_size=4,
#     overlap=0.25,
#     mode="gaussian"
# )
#
# with torch.no_grad():
#     # å¯ä»¥å¤„ç†256Ã—256Ã—64æˆ–ä»»æ„å¤§å°
#     reconstruction = inferer(full_volume, autoencoder)
# ```
#
# ================================
# ğŸ¯ è®­ç»ƒæ­¥éª¤
# ================================
#
# 1. è®­ç»ƒAutoencoderKL:
#    python monai_diffusion/3d_ldm/train_autoencoder.py --config monai_diffusion/config/ldm_config_patched.yaml
#
# 2. è®­ç»ƒDiffusion Model:
#    python monai_diffusion/3d_ldm/train_diffusion.py --config monai_diffusion/config/ldm_config_patched.yaml
#
# 3. ç”Ÿæˆæ ·æœ¬:
#    python monai_diffusion/3d_ldm/generate_samples.py --config monai_diffusion/config/ldm_config_patched.yaml
#

