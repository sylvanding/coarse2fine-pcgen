"""
Diffusion Modelè®­ç»ƒè„šæœ¬

åŸºäºMONAI Generative Modelsçš„3D Diffusion Modelè®­ç»ƒï¼Œ
ç”¨äºLatent Diffusion Modelçš„ç¬¬äºŒé˜¶æ®µè®­ç»ƒã€‚
"""

import sys
from pathlib import Path
import argparse
import logging
from tqdm import tqdm
import yaml
import shutil
import importlib.util

# æ·»åŠ GenerativeModelsåˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "GenerativeModels"))
sys.path.insert(0, str(project_root))

import torch
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
from torch.utils.tensorboard import SummaryWriter
from monai.utils import set_determinism, first
import numpy as np

from generative.inferers import LatentDiffusionInferer
from generative.networks.nets import AutoencoderKL, DiffusionModelUNet
from generative.networks.schedulers import DDPMScheduler

from monai_diffusion.datasets import create_train_val_dataloaders

# ä»åŒç›®å½•çš„train_autoencoderå¯¼å…¥add_noiseå‡½æ•°
train_ae_path = Path(__file__).parent / "train_autoencoder.py"
spec = importlib.util.spec_from_file_location("train_autoencoder", train_ae_path)
train_ae_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(train_ae_module)
add_noise = train_ae_module.add_noise

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path: str) -> dict:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def load_autoencoder(checkpoint_path: str, device: torch.device, ae_config: dict):
    """åŠ è½½é¢„è®­ç»ƒçš„AutoencoderKL"""
    autoencoder = AutoencoderKL(
        spatial_dims=ae_config['spatial_dims'],
        in_channels=ae_config['in_channels'],
        out_channels=ae_config['out_channels'],
        num_channels=tuple(ae_config['num_channels']),
        latent_channels=ae_config['latent_channels'],
        num_res_blocks=ae_config['num_res_blocks'],
        norm_num_groups=ae_config.get('norm_num_groups', 16),
        attention_levels=tuple(ae_config['attention_levels'])
    )
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    autoencoder.load_state_dict(checkpoint['autoencoder_state_dict'])
    autoencoder.to(device)
    autoencoder.eval()
    
    logger.info(f"ä» {checkpoint_path} åŠ è½½AutoencoderKL")
    return autoencoder


def compute_scale_factor(
    autoencoder: torch.nn.Module,
    train_loader,
    device: torch.device
) -> float:
    """
    è®¡ç®—æ½œåœ¨ç©ºé—´çš„ç¼©æ”¾å› å­
    
    æ ¹æ®Rombach et al. (2022)çš„å»ºè®®ï¼Œè®¡ç®—æ½œåœ¨ç©ºé—´çš„æ ‡å‡†å·®
    ä½œä¸ºç¼©æ”¾å› å­ï¼Œä»¥ç¡®ä¿æ½œåœ¨ç©ºé—´åˆ†å¸ƒæ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚
    """
    logger.info("è®¡ç®—æ½œåœ¨ç©ºé—´ç¼©æ”¾å› å­...")
    
    autoencoder.eval()
    with torch.no_grad():
        # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
        check_data = first(train_loader)
        images = check_data["image"].to(device)
        
        with autocast(enabled=True):
            z = autoencoder.encode_stage_2_inputs(images)
        
        scale_factor = 1 / torch.std(z)
    
    logger.info(f"ç¼©æ”¾å› å­: {scale_factor:.4f}")
    return scale_factor.item()


def save_checkpoint(
    epoch: int,
    unet: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    scale_factor: float,
    best_val_loss: float,
    output_dir: str,
    is_best: bool = False
):
    """ä¿å­˜checkpoint"""
    checkpoint = {
        'epoch': epoch,
        'unet_state_dict': unet.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scale_factor': scale_factor,
        'best_val_loss': best_val_loss
    }
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æœ€æ–°checkpoint
    latest_path = output_path / "latest_checkpoint.pt"
    torch.save(checkpoint, latest_path)
    logger.info(f"ä¿å­˜æœ€æ–°checkpointåˆ°: {latest_path}")
    
    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¿å­˜best checkpoint
    if is_best:
        best_path = output_path / "best_model.pt"
        torch.save(checkpoint, best_path)
        logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")


def load_checkpoint(
    checkpoint_path: str,
    unet: torch.nn.Module,
    optimizer: torch.optim.Optimizer
):
    """åŠ è½½checkpointæ¢å¤è®­ç»ƒ"""
    checkpoint = torch.load(checkpoint_path)
    
    unet.load_state_dict(checkpoint['unet_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    start_epoch = checkpoint['epoch'] + 1
    best_val_loss = checkpoint['best_val_loss']
    scale_factor = checkpoint['scale_factor']
    
    logger.info(f"ä»checkpointæ¢å¤è®­ç»ƒ: epoch {start_epoch}")
    return start_epoch, best_val_loss, scale_factor


def validate(
    unet: torch.nn.Module,
    autoencoder: torch.nn.Module,
    inferer: LatentDiffusionInferer,
    val_loader,
    device: torch.device,
    use_input_noise: bool = False,
    noise_type: str = "gaussian",
    noise_std: float = 0.1,
    dropout_prob: float = 0.1
):
    """
    éªŒè¯å‡½æ•°
    
    Args:
        use_input_noise: æ˜¯å¦åœ¨è¾“å…¥å›¾åƒä¸Šæ·»åŠ å™ªå£°
        noise_type: å™ªå£°ç±»å‹ ("gaussian", "dropout", "mixed")
        noise_std: é«˜æ–¯å™ªå£°æ ‡å‡†å·®
        dropout_prob: dropoutæ¦‚ç‡
    """
    unet.eval()
    val_loss = 0
    
    with torch.no_grad():
        for batch in val_loader:
            clean_images = batch["image"].to(device)
            
            # å¦‚æœå¯ç”¨è¾“å…¥å™ªå£°ï¼Œç»™å›¾åƒæ·»åŠ å™ªå£°
            if use_input_noise:
                images = add_noise(
                    clean_images,
                    noise_type=noise_type,
                    noise_std=noise_std,
                    dropout_prob=dropout_prob
                )
            else:
                images = clean_images
            
            with autocast(enabled=True):
                # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
                z = autoencoder.encode_stage_2_inputs(images)
                
                # ç”Ÿæˆéšæœºå™ªå£°
                noise = torch.randn_like(z).to(device)
                
                # éšæœºæ—¶é—´æ­¥
                timesteps = torch.randint(
                    0, inferer.scheduler.num_train_timesteps,
                    (images.shape[0],), device=device
                ).long()
                
                # é¢„æµ‹å™ªå£°
                noise_pred = inferer(
                    inputs=images,
                    autoencoder_model=autoencoder,
                    diffusion_model=unet,
                    noise=noise,
                    timesteps=timesteps
                )
                
                loss = F.mse_loss(noise_pred.float(), noise.float())
                val_loss += loss.item()
    
    return val_loss / len(val_loader)


def generate_samples(
    unet: torch.nn.Module,
    autoencoder: torch.nn.Module,
    inferer: LatentDiffusionInferer,
    scheduler,
    num_samples: int,
    latent_shape: tuple,
    device: torch.device
):
    """ç”Ÿæˆæ ·æœ¬ç”¨äºç›‘æ§è®­ç»ƒè¿›åº¦"""
    unet.eval()
    autoencoder.eval()
    
    with torch.no_grad():
        # ç”Ÿæˆéšæœºå™ªå£°
        noise = torch.randn((num_samples, *latent_shape)).to(device)
        
        # è®¾ç½®é‡‡æ ·æ­¥æ•°
        scheduler.set_timesteps(num_inference_steps=1000)
        
        # é‡‡æ ·
        synthetic_images = inferer.sample(
            input_noise=noise,
            autoencoder_model=autoencoder,
            diffusion_model=unet,
            scheduler=scheduler
        )
    
    return synthetic_images


def visualize_samples(
    unet: torch.nn.Module,
    autoencoder: torch.nn.Module,
    inferer: LatentDiffusionInferer,
    scheduler,
    val_loader,
    latent_shape: tuple,
    device: torch.device,
    writer: SummaryWriter,
    epoch: int,
    num_samples: int = 4
):
    """
    å¯è§†åŒ–ç”Ÿæˆæ ·æœ¬ä¸çœŸå®æ ·æœ¬çš„å¯¹æ¯”
    
    å°†éªŒè¯é›†çš„å‰num_samplesä¸ªçœŸå®æ ·æœ¬å’Œç”Ÿæˆçš„æ ·æœ¬åœ¨zæ–¹å‘å †å ï¼Œ
    ä¼ é€’ç»™TensorBoardè¿›è¡Œå¯è§†åŒ–ã€‚
    
    Args:
        unet: Diffusion UNetæ¨¡å‹
        autoencoder: AutoencoderKLæ¨¡å‹
        inferer: LatentDiffusionInferer
        scheduler: è°ƒåº¦å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        latent_shape: æ½œåœ¨ç©ºé—´å½¢çŠ¶
        device: è®¾å¤‡
        writer: TensorBoard writer
        epoch: å½“å‰epoch
        num_samples: å¯è§†åŒ–çš„æ ·æœ¬æ•°é‡
    """
    unet.eval()
    autoencoder.eval()
    
    with torch.no_grad():
        # è·å–çœŸå®æ ·æœ¬
        batch = next(iter(val_loader))
        real_images = batch["image"].to(device)[:num_samples]
        
        # ç”Ÿæˆåˆæˆæ ·æœ¬
        noise = torch.randn((num_samples, *latent_shape)).to(device)
        scheduler.set_timesteps(num_inference_steps=1000)  # ä½¿ç”¨è¾ƒå°‘çš„æ­¥æ•°åŠ å¿«å¯è§†åŒ–
        
        synthetic_images = inferer.sample(
            input_noise=noise,
            autoencoder_model=autoencoder,
            diffusion_model=unet,
            scheduler=scheduler
        )
        
        # ç§»åˆ°CPUå¹¶è½¬æ¢ä¸ºnumpy
        real_images_np = real_images.cpu().numpy()  # (B, C, H, W, D)
        synthetic_images_np = synthetic_images.cpu().numpy()
        
        # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
        for i in range(min(num_samples, real_images_np.shape[0])):
            # å–å‡ºå•ä¸ªæ ·æœ¬ (C, H, W, D)
            real_vol = real_images_np[i, 0]  # (H, W, D)
            synthetic_vol = synthetic_images_np[i, 0]  # (H, W, D)
            
            # å°†3Dä½“ç´ æ²¿zè½´æŠ•å½±æˆ2Då›¾åƒï¼ˆç´¯åŠ æ‰€æœ‰zå±‚ï¼‰
            real_proj = np.sum(real_vol, axis=2)  # (H, W)
            synthetic_proj = np.sum(synthetic_vol, axis=2)  # (H, W)
            
            # åˆ†åˆ«å½’ä¸€åŒ–çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬çš„æŠ•å½±
            real_proj = (real_proj - real_proj.min()) / (real_proj.max() - real_proj.min() + 1e-8)
            synthetic_proj = (synthetic_proj - synthetic_proj.min()) / (synthetic_proj.max() - synthetic_proj.min() + 1e-8)
            
            # å‚ç›´å †å çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬
            combined = np.hstack([real_proj, synthetic_proj])  # (H, 2*W)
            
            # æ·»åŠ åˆ°TensorBoard
            writer.add_image(
                f"comparison/sample_{i}",
                combined,
                epoch,
                dataformats='HW'
            )
        
        logger.info(f"å·²ä¿å­˜ {min(num_samples, real_images_np.shape[0])} ä¸ªå¯¹æ¯”å¯è§†åŒ–ç»“æœåˆ°TensorBoard")


def train_diffusion(config_path: str):
    """
    è®­ç»ƒDiffusion Model
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # æå–é…ç½®å‚æ•°
    diff_config = config['diffusion']
    checkpoint_config = diff_config['checkpoints']
    log_config = diff_config['logging']
    
    # æ¸…ç†ä¹‹å‰çš„è¾“å‡ºç›®å½•
    output_dir = Path(checkpoint_config['output_dir'])
    log_dir = Path(log_config['log_dir'])
    
    if output_dir.exists():
        logger.info(f"åˆ é™¤ä¹‹å‰çš„è¾“å‡ºç›®å½•: {output_dir}")
        shutil.rmtree(output_dir)
    
    if log_dir.exists():
        logger.info(f"åˆ é™¤ä¹‹å‰çš„æ—¥å¿—ç›®å½•: {log_dir}")
        shutil.rmtree(log_dir)
    
    # åˆ›å»ºæ–°çš„ç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    set_determinism(config.get('seed', 42))
    
    # è®¾ç½®è®¾å¤‡
    device_config = config.get('device', {})
    use_cuda = device_config.get('use_cuda', True) and torch.cuda.is_available()
    device = torch.device(f"cuda:{device_config.get('gpu_id', 0)}" if use_cuda else "cpu")
    mixed_precision = device_config.get('mixed_precision', True)
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    logger.info(f"æ··åˆç²¾åº¦è®­ç»ƒ: {mixed_precision}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_config = diff_config['training']
    train_loader, val_loader = create_train_val_dataloaders(config, batch_size=train_config.get('batch_size', None))
    
    # æå–å…¶ä»–é…ç½®å‚æ•°
    ae_config = config['autoencoder']
    scheduler_config = diff_config['scheduler']
    
    # åŠ è½½é¢„è®­ç»ƒçš„AutoencoderKL
    autoencoder_path = checkpoint_config['autoencoder_path']
    if not Path(autoencoder_path).exists():
        raise FileNotFoundError(
            f"AutoencoderKL checkpointä¸å­˜åœ¨: {autoencoder_path}\n"
            "è¯·å…ˆè®­ç»ƒAutoencoderKLæˆ–æŒ‡å®šæ­£ç¡®çš„checkpointè·¯å¾„"
        )
    
    autoencoder = load_autoencoder(autoencoder_path, device, ae_config)
    
    # åˆ›å»ºDiffusion Model UNet
    unet = DiffusionModelUNet(
        spatial_dims=diff_config['spatial_dims'],
        in_channels=diff_config['in_channels'],
        out_channels=diff_config['out_channels'],
        num_channels=tuple(diff_config['num_channels']),
        attention_levels=tuple(diff_config['attention_levels']),
        num_head_channels=tuple(diff_config['num_head_channels']),
        num_res_blocks=diff_config.get('num_res_blocks', 1)
    )
    unet.to(device)
    logger.info("åˆ›å»ºDiffusionModelUNet")
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = DDPMScheduler(
        num_train_timesteps=scheduler_config['num_train_timesteps'],
        schedule=scheduler_config['schedule'],
        beta_start=scheduler_config['beta_start'],
        beta_end=scheduler_config['beta_end']
    )
    logger.info(f"åˆ›å»ºDDPMScheduler: {scheduler_config['num_train_timesteps']} timesteps")
    
    # è®¡ç®—ç¼©æ”¾å› å­
    scale_factor = compute_scale_factor(autoencoder, train_loader, device)
    
    # åˆ›å»ºInferer
    inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)
    logger.info(f"åˆ›å»ºLatentDiffusionInferer (scale_factor={scale_factor:.4f})")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        params=unet.parameters(),
        lr=train_config['learning_rate']
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if mixed_precision else None
    
    # TensorBoard
    writer = SummaryWriter(log_dir=str(log_dir))
    
    # è®­ç»ƒå‚æ•°
    n_epochs = train_config['n_epochs']
    val_interval = train_config['val_interval']
    save_interval = train_config['save_interval']
    log_interval = log_config['log_interval']
    visualize_interval = log_config.get('visualize_interval', 10)  # é»˜è®¤æ¯10ä¸ªepochå¯è§†åŒ–ä¸€æ¬¡
    num_visualize_samples = log_config.get('num_visualize_samples', 4)  # é»˜è®¤å¯è§†åŒ–4ä¸ªæ ·æœ¬
    
    # ==================== è¾“å…¥å™ªå£°å¢å¼ºé…ç½® ====================
    input_noise_config = train_config.get('input_noise', {})
    use_input_noise = input_noise_config.get('enabled', False)
    noise_type = input_noise_config.get('noise_type', 'gaussian')
    noise_std = input_noise_config.get('noise_std', 0.1)
    dropout_prob = input_noise_config.get('dropout_prob', 0.1)
    
    if use_input_noise:
        logger.info("=" * 60)
        logger.info("ğŸ”¥ å¯ç”¨è¾“å…¥å™ªå£°å¢å¼º (Input Noise Augmentation)")
        logger.info(f"  å™ªå£°ç±»å‹: {noise_type}")
        logger.info(f"  é«˜æ–¯å™ªå£°æ ‡å‡†å·®: {noise_std}")
        logger.info(f"  Dropoutæ¦‚ç‡: {dropout_prob}")
        logger.info("  Diffusionæ¨¡å‹å°†ä»å¸¦å™ªå£°çš„è¾“å…¥ä¸­å­¦ä¹ ç”Ÿæˆï¼Œæé«˜é²æ£’æ€§ï¼")
        logger.info("=" * 60)
    else:
        logger.info("æœªå¯ç”¨è¾“å…¥å™ªå£°å¢å¼º")
    
    # å¿«é€Ÿå¼€å‘æ¨¡å¼
    fast_dev_run = train_config.get('fast_dev_run', False)
    fast_dev_run_batches = train_config.get('fast_dev_run_batches', 2)
    
    if fast_dev_run:
        logger.info(f"**å¿«é€Ÿå¼€å‘æ¨¡å¼**: æ¯ä¸ªepochåªè¿è¡Œ {fast_dev_run_batches} ä¸ªbatch")
        n_epochs = 2  # å¿«é€Ÿæ¨¡å¼åªè¿è¡Œ2ä¸ªepoch
        val_interval = 1
        save_interval = 1
        log_interval = 1
        visualize_interval = 1
        num_visualize_samples = 2
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_val_loss = float('inf')
    
    resume_from = checkpoint_config.get('resume_from')
    if resume_from and Path(resume_from).exists():
        start_epoch, best_val_loss, loaded_scale_factor = load_checkpoint(
            resume_from, unet, optimizer
        )
        # ä½¿ç”¨åŠ è½½çš„scale_factor
        scale_factor = loaded_scale_factor
        inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)
        logger.info(f"ä½¿ç”¨åŠ è½½çš„scale_factor: {scale_factor:.4f}")
    
    # è·å–æ½œåœ¨ç©ºé—´å½¢çŠ¶ï¼ˆç”¨äºç”Ÿæˆæ ·æœ¬ï¼‰
    with torch.no_grad():
        check_data = first(train_loader)
        z = autoencoder.encode_stage_2_inputs(check_data["image"].to(device))
        latent_shape = z.shape[1:]  # (C, D, H, W)
    
    logger.info(f"æ½œåœ¨ç©ºé—´å½¢çŠ¶: {latent_shape}")
    
    # è®­ç»ƒå¾ªç¯
    logger.info(f"å¼€å§‹è®­ç»ƒDiffusion Model: {n_epochs} epochs")
    
    for epoch in range(start_epoch, n_epochs):
        unet.train()
        autoencoder.eval()
        
        epoch_loss = 0
        
        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=100)
        progress_bar.set_description(f"Epoch {epoch}/{n_epochs}")
        
        for step, batch in progress_bar:
            # å¿«é€Ÿå¼€å‘æ¨¡å¼ï¼šåªè¿è¡ŒæŒ‡å®šæ•°é‡çš„batch
            if fast_dev_run and step >= fast_dev_run_batches:
                break
            
            clean_images = batch["image"].to(device)
            
            # ============ è¾“å…¥å™ªå£°å¢å¼ºï¼šæ·»åŠ å™ªå£° ============
            if use_input_noise:
                images = add_noise(
                    clean_images,
                    noise_type=noise_type,
                    noise_std=noise_std,
                    dropout_prob=dropout_prob
                )
            else:
                images = clean_images
            
            optimizer.zero_grad(set_to_none=True)
            
            with autocast(enabled=mixed_precision):
                # ç”Ÿæˆéšæœºå™ªå£°
                with torch.no_grad():
                    z = autoencoder.encode_stage_2_inputs(images)
                noise = torch.randn_like(z).to(device)
                
                # éšæœºæ—¶é—´æ­¥
                timesteps = torch.randint(
                    0, inferer.scheduler.num_train_timesteps,
                    (images.shape[0],), device=images.device
                ).long()
                
                # è·å–æ¨¡å‹é¢„æµ‹
                noise_pred = inferer(
                    inputs=images,
                    autoencoder_model=autoencoder,
                    diffusion_model=unet,
                    noise=noise,
                    timesteps=timesteps
                )
                
                loss = F.mse_loss(noise_pred.float(), noise.float())
            
            if mixed_precision:
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()
            
            epoch_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({"loss": f"{epoch_loss / (step + 1):.4f}"})
            
            # TensorBoardæ—¥å¿—
            if step % log_interval == 0:
                global_step = epoch * len(train_loader) + step
                writer.add_scalar("train/step/loss", loss.item(), global_step)
        
        # è®°å½•epochå¹³å‡æŸå¤±
        n_steps = step + 1
        avg_loss = epoch_loss / n_steps
        writer.add_scalar("train/epoch/loss", avg_loss, epoch)
        logger.info(f"Epoch {epoch} è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
        
        # éªŒè¯
        if (epoch + 1) % val_interval == 0 or epoch == n_epochs - 1:
            val_loss = validate(
                unet, autoencoder, inferer, val_loader, device,
                use_input_noise=use_input_noise,
                noise_type=noise_type,
                noise_std=noise_std,
                dropout_prob=dropout_prob
            )
            writer.add_scalar("val/epoch/loss", val_loss, epoch)
            logger.info(f"Epoch {epoch} éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
                logger.info(f"æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        else:
            is_best = False
        
        # å¯è§†åŒ–ç”Ÿæˆæ ·æœ¬ä¸çœŸå®æ ·æœ¬çš„å¯¹æ¯”
        if (epoch + 1) % visualize_interval == 0 or epoch == n_epochs - 1:
            logger.info("ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–ç»“æœ...")
            visualize_samples(
                unet, autoencoder, inferer, scheduler, val_loader,
                latent_shape, device, writer, epoch, num_visualize_samples
            )
        
        # ä¿å­˜checkpoint
        if (epoch + 1) % save_interval == 0 or epoch == n_epochs - 1 or is_best:
            save_checkpoint(
                epoch=epoch,
                unet=unet,
                optimizer=optimizer,
                scale_factor=scale_factor,
                best_val_loss=best_val_loss,
                output_dir=checkpoint_config['output_dir'],
                is_best=is_best
            )
    
    writer.close()
    logger.info("Diffusion Modelè®­ç»ƒå®Œæˆ!")


def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒDiffusion Model")
    parser.add_argument(
        '--config',
        type=str,
        default='monai_diffusion/config/ldm_config_local.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    
    args = parser.parse_args()
    train_diffusion(args.config)


if __name__ == "__main__":
    main()

