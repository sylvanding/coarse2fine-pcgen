"""
AutoencoderKLËÆ≠ÁªÉËÑöÊú¨

Âü∫‰∫éMONAI Generative ModelsÁöÑ3D AutoencoderKLËÆ≠ÁªÉÔºå
Áî®‰∫éLatent Diffusion ModelÁöÑÁ¨¨‰∏ÄÈò∂ÊÆµËÆ≠ÁªÉ„ÄÇ
"""

import os
import sys
from pathlib import Path
import argparse
import logging
from tqdm import tqdm
import yaml
import shutil
from PIL import Image
from torch.utils.data import DataLoader
# Ê∑ªÂä†GenerativeModelsÂà∞PythonË∑ØÂæÑ
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "GenerativeModels"))
sys.path.insert(0, str(project_root))

import torch
import torch.nn.functional as F
from torch.nn import L1Loss, MSELoss
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import autocast, GradScaler
from monai.utils import set_determinism
from monai import transforms
import numpy as np

from generative.losses import PatchAdversarialLoss, PerceptualLoss
from generative.networks.nets import AutoencoderKL, PatchDiscriminator

from monai_diffusion.datasets import create_train_val_dataloaders
from monai_diffusion.utils.sliding_window_inference import AutoencoderSlidingWindowInferer

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==================== ÊçüÂ§±ÂáΩÊï∞ÂÆö‰πâ ====================

class DiceLoss(torch.nn.Module):
    """
    Dice LossÁî®‰∫éÂ§ÑÁêÜ‰∏çÂπ≥Ë°°Êï∞ÊçÆ
    
    ÁâπÂà´ÈÄÇÁî®‰∫éÂâçÊôØÔºàÂæÆÁÆ°ÔºâÂÉèÁ¥†ËæÉÂ∞ëÁöÑÊÉÖÂÜµ„ÄÇ
    Dice LossÂÖ≥Ê≥®ÁöÑÊòØÂâçÊôØÂå∫ÂüüÁöÑÈáçÂè†Â∫¶ÔºåÂØπÂâçÊôØÂÉèÁ¥†Êõ¥ÊïèÊÑü„ÄÇ
    """
    def __init__(self, smooth: float = 1e-5, sigmoid: bool = False):
        """
        Args:
            smooth: Âπ≥ÊªëÈ°πÔºåÈÅøÂÖçÈô§Èõ∂
            sigmoid: ÊòØÂê¶ÂØπËæìÂÖ•Â∫îÁî®sigmoidÔºàÂ¶ÇÊûúÊ®°ÂûãËæìÂá∫Êú™ÂΩí‰∏ÄÂåñÔºâ
        """
        super().__init__()
        self.smooth = smooth
        self.sigmoid = sigmoid
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        ËÆ°ÁÆóDice Loss
        
        Args:
            pred: È¢ÑÊµãÂº†Èáè (B, C, H, W, D)
            target: ÁõÆÊ†áÂº†Èáè (B, C, H, W, D)
            
        Returns:
            Dice LossÂÄº (Ê†áÈáè)
        """
        if self.sigmoid:
            pred = torch.sigmoid(pred)
        
        # Â±ïÂπ≥‰∏∫ (B, N)
        pred_flat = pred.view(pred.size(0), -1)
        target_flat = target.view(target.size(0), -1)
        
        # ËÆ°ÁÆó‰∫§ÈõÜÂíåÂπ∂ÈõÜ
        intersection = (pred_flat * target_flat).sum(dim=1)
        union = pred_flat.sum(dim=1) + target_flat.sum(dim=1)
        
        # DiceÁ≥ªÊï∞
        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)
        
        # Dice Loss = 1 - DiceÁ≥ªÊï∞
        return 1.0 - dice.mean()


class WeightedReconstructionLoss(torch.nn.Module):
    """
    Âä†ÊùÉÈáçÂª∫ÊçüÂ§±ÔºàL1ÊàñMSEÔºâ
    
    ÂØπÂâçÊôØÂÉèÁ¥†ÔºàÂæÆÁÆ°ÔºâÁªô‰∫àÊõ¥È´òÁöÑÊùÉÈáçÔºåÂØπËÉåÊôØÂÉèÁ¥†Áªô‰∫àÊõ¥‰ΩéÁöÑÊùÉÈáç„ÄÇ
    ËøôÊ†∑ÂèØ‰ª•Ëø´‰ΩøÊ®°ÂûãÂÖ≥Ê≥®ÂâçÊôØÔºåÈÅøÂÖçÈ¢ÑÊµãÂÖ®Èªë„ÄÇ
    """
    def __init__(
        self,
        loss_type: str = "l1",  # "l1" or "mse"
        foreground_weight: float = 10.0,
        background_weight: float = 1.0,
        threshold: float = 0.1,  # Áî®‰∫éÂå∫ÂàÜÂâçÊôØÂíåËÉåÊôØÁöÑÈòàÂÄº
    ):
        """
        Args:
            loss_type: ÊçüÂ§±Á±ªÂûãÔºå"l1" Êàñ "mse"
            foreground_weight: ÂâçÊôØÂÉèÁ¥†ÊùÉÈáçÔºàÂ∫îËØ• >> 1Ôºâ
            background_weight: ËÉåÊôØÂÉèÁ¥†ÊùÉÈáçÔºàÈÄöÂ∏∏‰∏∫1.0Ôºâ
            threshold: ÂÉèÁ¥†ÂÄºÈòàÂÄºÔºåÈ´ò‰∫éÊ≠§ÂÄºËÆ§‰∏∫ÊòØÂâçÊôØ
        """
        super().__init__()
        self.loss_type = loss_type
        self.foreground_weight = foreground_weight
        self.background_weight = background_weight
        self.threshold = threshold
        
        logger.info(f"ÂàùÂßãÂåñÂä†ÊùÉÈáçÂª∫ÊçüÂ§±:")
        logger.info(f"  ÊçüÂ§±Á±ªÂûã: {loss_type}")
        logger.info(f"  ÂâçÊôØÊùÉÈáç: {foreground_weight}x")
        logger.info(f"  ËÉåÊôØÊùÉÈáç: {background_weight}x")
        logger.info(f"  ÂâçÊôØÈòàÂÄº: {threshold}")
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        ËÆ°ÁÆóÂä†ÊùÉÈáçÂª∫ÊçüÂ§±
        
        Args:
            pred: È¢ÑÊµãÂº†Èáè (B, C, H, W, D)
            target: ÁõÆÊ†áÂº†Èáè (B, C, H, W, D)
            
        Returns:
            Âä†ÊùÉÊçüÂ§±ÂÄºÔºàÊ†áÈáèÔºâ
        """
        # ËÆ°ÁÆóÈÄêÂÉèÁ¥†ËØØÂ∑Æ
        if self.loss_type == "l1":
            pixel_loss = torch.abs(pred - target)
        elif self.loss_type == "mse":
            pixel_loss = (pred - target) ** 2
        else:
            raise ValueError(f"‰∏çÊîØÊåÅÁöÑÊçüÂ§±Á±ªÂûã: {self.loss_type}")
        
        # ÂàõÂª∫ÊùÉÈáçmaskÔºöÊ†πÊçÆÁõÆÊ†áÂõæÂÉèÂå∫ÂàÜÂâçÊôØÂíåËÉåÊôØ
        # Ê≥®ÊÑèÔºöËøôÈáå‰ΩøÁî®ÁõÆÊ†áÂõæÂÉèÂà§Êñ≠ÂâçÊôØ/ËÉåÊôØ
        foreground_mask = (target > self.threshold).float()
        background_mask = 1.0 - foreground_mask
        
        # Â∫îÁî®ÊùÉÈáç
        weighted_loss = (
            pixel_loss * foreground_mask * self.foreground_weight +
            pixel_loss * background_mask * self.background_weight
        )
        
        # ËøîÂõûÂπ≥ÂùáÊçüÂ§±
        return weighted_loss.mean()


class CombinedReconstructionLoss(torch.nn.Module):
    """
    ÁªÑÂêàÊçüÂ§±ÔºöDice Loss + Âä†ÊùÉÈáçÂª∫ÊçüÂ§±
    
    ÁªìÂêà‰∏§ÁßçÊçüÂ§±ÁöÑ‰ºòÂäøÔºö
    - Dice Loss: ÂÖ≥Ê≥®ÂâçÊôØÂå∫ÂüüÁöÑÊï¥‰ΩìÈáçÂè†Â∫¶
    - Âä†ÊùÉÈáçÂª∫ÊçüÂ§±: ÂØπÂâçÊôØÂÉèÁ¥†Áªô‰∫àÊõ¥È´òÁöÑÈÄêÂÉèÁ¥†ÈáçÂª∫ÊùÉÈáç
    """
    def __init__(
        self,
        dice_weight: float = 1.0,
        recon_weight: float = 1.0,
        recon_loss_type: str = "l1",
        foreground_weight: float = 10.0,
        background_weight: float = 1.0,
        threshold: float = 0.1,
        dice_smooth: float = 1e-5,
    ):
        """
        Args:
            dice_weight: Dice LossÁöÑÊùÉÈáç
            recon_weight: ÈáçÂª∫ÊçüÂ§±ÁöÑÊùÉÈáç
            recon_loss_type: ÈáçÂª∫ÊçüÂ§±Á±ªÂûã "l1" Êàñ "mse"
            foreground_weight: ÂâçÊôØÂÉèÁ¥†ÊùÉÈáç
            background_weight: ËÉåÊôØÂÉèÁ¥†ÊùÉÈáç
            threshold: ÂâçÊôØÈòàÂÄº
            dice_smooth: DiceÂπ≥ÊªëÈ°π
        """
        super().__init__()
        self.dice_weight = dice_weight
        self.recon_weight = recon_weight
        
        self.dice_loss = DiceLoss(smooth=dice_smooth, sigmoid=False)
        self.weighted_recon_loss = WeightedReconstructionLoss(
            loss_type=recon_loss_type,
            foreground_weight=foreground_weight,
            background_weight=background_weight,
            threshold=threshold
        )
        
        logger.info(f"ÂàùÂßãÂåñÁªÑÂêàÈáçÂª∫ÊçüÂ§±:")
        logger.info(f"  Dice LossÊùÉÈáç: {dice_weight}")
        logger.info(f"  ÈáçÂª∫ÊçüÂ§±ÊùÉÈáç: {recon_weight}")
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        ËÆ°ÁÆóÁªÑÂêàÊçüÂ§±
        
        Args:
            pred: È¢ÑÊµãÂº†Èáè (B, C, H, W, D)
            target: ÁõÆÊ†áÂº†Èáè (B, C, H, W, D)
            
        Returns:
            ÁªÑÂêàÊçüÂ§±ÂÄºÔºàÊ†áÈáèÔºâ
        """
        dice = self.dice_loss(pred, target)
        recon = self.weighted_recon_loss(pred, target)
        
        return self.dice_weight * dice + self.recon_weight * recon


# ==================== ÂéªÂô™Ëá™ÁºñÁ†ÅÂô®ÔºöÂô™Â£∞ÁîüÊàêÂáΩÊï∞ ====================

def add_gaussian_noise(images: torch.Tensor, noise_std: float = 0.1) -> torch.Tensor:
    """
    Ê∑ªÂä†È´òÊñØÂô™Â£∞
    
    Args:
        images: ËæìÂÖ•ÂõæÂÉèÂº†Èáè (B, C, H, W, D)ÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
        noise_std: Âô™Â£∞Ê†áÂáÜÂ∑ÆÔºàÁõ∏ÂØπ‰∫éÂõæÂÉèËåÉÂõ¥Ôºâ
        
    Returns:
        Â∏¶Âô™Â£∞ÁöÑÂõæÂÉèÂº†ÈáèÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
    """
    noise = torch.randn_like(images) * noise_std
    noisy_images = images + noise
    # ‰øùÊåÅ‰∏éÂéüÂßãÂõæÂÉèÁõ∏ÂêåÁöÑÂÄºÂüüËåÉÂõ¥ [0, 1]
    noisy_images = torch.clamp(noisy_images, 0.0, 1.0)
    return noisy_images


def add_dropout_noise(images: torch.Tensor, dropout_prob: float = 0.1) -> torch.Tensor:
    """
    ÈöèÊú∫‰∏¢ÂºÉÔºàÁΩÆÈõ∂Ôºâ‰∏Ä‰∫õ‰ΩìÁ¥†
    
    Args:
        images: ËæìÂÖ•ÂõæÂÉèÂº†Èáè (B, C, H, W, D)ÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
        dropout_prob: ‰∏¢ÂºÉÊ¶ÇÁéá
        
    Returns:
        Â∏¶Âô™Â£∞ÁöÑÂõæÂÉèÂº†ÈáèÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
    """
    mask = torch.rand_like(images) > dropout_prob
    noisy_images = images * mask.float()
    return noisy_images


def add_mixed_noise(
    images: torch.Tensor, 
    noise_std: float = 0.1, 
    dropout_prob: float = 0.05
) -> torch.Tensor:
    """
    Ê∑∑ÂêàÂô™Â£∞ÔºöÂêåÊó∂Ê∑ªÂä†È´òÊñØÂô™Â£∞ÂíådropoutÂô™Â£∞
    
    Args:
        images: ËæìÂÖ•ÂõæÂÉèÂº†Èáè (B, C, H, W, D)ÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
        noise_std: È´òÊñØÂô™Â£∞Ê†áÂáÜÂ∑Æ
        dropout_prob: ‰∏¢ÂºÉÊ¶ÇÁéá
        
    Returns:
        Â∏¶Âô™Â£∞ÁöÑÂõæÂÉèÂº†ÈáèÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
    """
    # ÂÖàÊ∑ªÂä†dropout
    noisy_images = add_dropout_noise(images, dropout_prob)
    # ÂÜçÊ∑ªÂä†È´òÊñØÂô™Â£∞
    noisy_images = add_gaussian_noise(noisy_images, noise_std)
    return noisy_images


def add_noise(
    images: torch.Tensor,
    noise_type: str = "gaussian",
    noise_std: float = 0.1,
    dropout_prob: float = 0.1
) -> torch.Tensor:
    """
    Áªü‰∏ÄÁöÑÂô™Â£∞Ê∑ªÂä†Êé•Âè£
    
    Args:
        images: ËæìÂÖ•ÂõæÂÉèÂº†Èáè (B, C, H, W, D)ÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
        noise_type: Âô™Â£∞Á±ªÂûãÔºåÂèØÈÄâ "gaussian", "dropout", "mixed"
        noise_std: È´òÊñØÂô™Â£∞Ê†áÂáÜÂ∑Æ
        dropout_prob: dropoutÊ¶ÇÁéá
        
    Returns:
        Â∏¶Âô™Â£∞ÁöÑÂõæÂÉèÂº†ÈáèÔºåÂÄºÂüüËåÉÂõ¥[0, 1]
    """
    if noise_type == "gaussian":
        return add_gaussian_noise(images, noise_std)
    elif noise_type == "dropout":
        return add_dropout_noise(images, dropout_prob)
    elif noise_type == "mixed":
        return add_mixed_noise(images, noise_std, dropout_prob)
    else:
        raise ValueError(f"‰∏çÊîØÊåÅÁöÑÂô™Â£∞Á±ªÂûã: {noise_type}")
        

# ==================== ÂéüÊúâ‰ª£Á†ÅÁªßÁª≠ ====================


def visualize_data_loader(data_loader: DataLoader, data_name: str):
    # Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™batch
    cpu_device = torch.device("cpu")
    batch = next(iter(data_loader))
    images = batch["image"].to(cpu_device)
    
    logger.info(f"ËæìÂÖ•patchÂΩ¢Áä∂: {images.shape}")
    
    images_np = images.numpy()  # (B, C, H, W, D)
    
    file_path = f"outputs/images_for_data_loader/{data_name}"
    os.makedirs(file_path, exist_ok=True)
    
    # ÂèØËßÜÂåñÁªìÊûú
    for i in range(images_np.shape[0]):
        # ÂèñÂá∫Âçï‰∏™Ê†∑Êú¨ (C, H, W, D)
        input_vol = images_np[i, 0]  # (H, W, D)
        
        # Â∞Ü3D‰ΩìÁ¥†Ê≤øzËΩ¥ÊäïÂΩ±Êàê2DÂõæÂÉèÔºàÁ¥ØÂä†ÊâÄÊúâzÂ±ÇÔºâ
        input_proj = np.sum(input_vol, axis=2)  # (H, W)
        
        # ÂΩí‰∏ÄÂåñ
        input_proj = (input_proj - input_proj.min()) / (input_proj.max() - input_proj.min() + 1e-8)
        
        # save image
        image = Image.fromarray(np.uint8(input_proj*255.0))
        image.save(f"{file_path}/sample_{i}.png")
        logger.info(f"‰øùÂ≠òÊ†∑Êú¨ {i} Âà∞ {file_path}/sample_{i}.png")


def load_config(config_path: str) -> dict:
    """Âä†ËΩΩYAMLÈÖçÁΩÆÊñá‰ª∂"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def KL_loss(z_mu, z_sigma):
    """KLÊï£Â∫¶ÊçüÂ§±"""
    kl_loss = 0.5 * torch.sum(
        z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1,
        dim=[1, 2, 3, 4]
    )
    return torch.sum(kl_loss) / kl_loss.shape[0]


def save_checkpoint(
    epoch: int,
    autoencoder: torch.nn.Module,
    discriminator: torch.nn.Module,
    optimizer_g: torch.optim.Optimizer,
    optimizer_d: torch.optim.Optimizer,
    best_val_loss: float,
    output_dir: str,
    is_best: bool = False
):
    """‰øùÂ≠òcheckpoint"""
    checkpoint = {
        'epoch': epoch,
        'autoencoder_state_dict': autoencoder.state_dict(),
        'discriminator_state_dict': discriminator.state_dict(),
        'optimizer_g_state_dict': optimizer_g.state_dict(),
        'optimizer_d_state_dict': optimizer_d.state_dict(),
        'best_val_loss': best_val_loss
    }
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ‰øùÂ≠òÊúÄÊñ∞checkpoint
    latest_path = output_path / "latest_checkpoint.pt"
    torch.save(checkpoint, latest_path)
    logger.info(f"‰øùÂ≠òÊúÄÊñ∞checkpointÂà∞: {latest_path}")
    
    # Â¶ÇÊûúÊòØÊúÄ‰Ω≥Ê®°ÂûãÔºå‰øùÂ≠òbest checkpoint
    if is_best:
        best_path = output_path / "best_model.pt"
        torch.save(checkpoint, best_path)
        logger.info(f"‰øùÂ≠òÊúÄ‰Ω≥Ê®°ÂûãÂà∞: {best_path}")


def load_checkpoint(
    checkpoint_path: str,
    autoencoder: torch.nn.Module,
    discriminator: torch.nn.Module,
    optimizer_g: torch.optim.Optimizer,
    optimizer_d: torch.optim.Optimizer
):
    """Âä†ËΩΩcheckpointÊÅ¢Â§çËÆ≠ÁªÉ"""
    checkpoint = torch.load(checkpoint_path)
    
    autoencoder.load_state_dict(checkpoint['autoencoder_state_dict'])
    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
    optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])
    optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])
    
    start_epoch = checkpoint['epoch'] + 1
    best_val_loss = checkpoint['best_val_loss']
    
    logger.info(f"‰ªécheckpointÊÅ¢Â§çËÆ≠ÁªÉ: epoch {start_epoch}")
    return start_epoch, best_val_loss


def validate(
    autoencoder: torch.nn.Module,
    val_loader,
    device: torch.device,
    kl_weight: float,
    fast_dev_run: bool,
    fast_dev_run_batches: int,
    recon_loss_fn: torch.nn.Module,  # ÈáçÂª∫ÊçüÂ§±ÂáΩÊï∞
    # ÂéªÂô™ÂèÇÊï∞
    use_denoising: bool = False,
    noise_type: str = "gaussian",
    noise_std: float = 0.1,
    dropout_prob: float = 0.1
):
    """
    È™åËØÅÂáΩÊï∞ÔºàÊîØÊåÅÂéªÂô™Ëá™ÁºñÁ†ÅÂô®Ôºâ
    
    Args:
        use_denoising: ÊòØÂê¶‰ΩøÁî®ÂéªÂô™Ê®°Âºè
        noise_type: Âô™Â£∞Á±ªÂûã
        noise_std: È´òÊñØÂô™Â£∞Ê†áÂáÜÂ∑Æ
        dropout_prob: dropoutÊ¶ÇÁéá
    """
    autoencoder.eval()
    val_loss = 0
    val_recon_loss = 0
    val_kl_loss = 0
    
    progress_bar = tqdm(val_loader, total=len(val_loader), ncols=120)
    progress_bar.set_description(f"Validating...")
    
    with torch.no_grad():
        for step, batch in enumerate(progress_bar):
            if fast_dev_run and step >= fast_dev_run_batches:
                break
            
            # ÂéüÂßãÂπ≤ÂáÄÂõæÂÉè
            clean_images = batch["image"].to(device)
            
            # Â¶ÇÊûú‰ΩøÁî®ÂéªÂô™Ê®°ÂºèÔºåÁªôËæìÂÖ•Ê∑ªÂä†Âô™Â£∞
            if use_denoising:
                noisy_images = add_noise(
                    clean_images,
                    noise_type=noise_type,
                    noise_std=noise_std,
                    dropout_prob=dropout_prob
                )
                # Ê®°ÂûãËæìÂÖ•ÔºöÂ∏¶Âô™Â£∞ÁöÑÂõæÂÉè
                # ÁõÆÊ†áËæìÂá∫ÔºöÂéüÂßãÂπ≤ÂáÄÁöÑÂõæÂÉè
                reconstruction, z_mu, z_sigma = autoencoder(noisy_images)
                recons_loss = recon_loss_fn(reconstruction.float(), clean_images.float())
            else:
                # Ê†áÂáÜËá™ÁºñÁ†ÅÂô®ÔºöËæìÂÖ•=ËæìÂá∫
                reconstruction, z_mu, z_sigma = autoencoder(clean_images)
                recons_loss = recon_loss_fn(reconstruction.float(), clean_images.float())
            
            kl = KL_loss(z_mu, z_sigma)
            loss = recons_loss + kl_weight * kl
            
            val_loss += loss.item()
            val_recon_loss += recons_loss.item()
            val_kl_loss += kl.item()
            
            progress_bar.set_postfix({
                "loss": f"{val_loss / (step + 1):.4f}",
                "recon": f"{val_recon_loss / (step + 1):.4f}",
                "kl": f"{val_kl_loss / (step + 1):.4f}"
            })
    n_batches = len(val_loader)
    return val_loss / n_batches, val_recon_loss / n_batches, val_kl_loss / n_batches


def create_full_image_dataloader(config: dict, num_samples: int = 4, num_workers: int = 0, pin_memory: bool = False):
    """
    ÂàõÂª∫Áî®‰∫éÂä†ËΩΩÂÆåÊï¥ÂõæÂÉèÔºà‰∏çË£ÅÂâ™ÔºâÁöÑÊï∞ÊçÆÂä†ËΩΩÂô®
    
    Args:
        config: ÈÖçÁΩÆÂ≠óÂÖ∏
        num_samples: Âä†ËΩΩÁöÑÊ†∑Êú¨Êï∞Èáè
        num_workers: DataLoaderÂ∑•‰ΩúËøõÁ®ãÊï∞ÔºàÈªòËÆ§0ÈÅøÂÖçÂÖ±‰∫´ÂÜÖÂ≠òÈóÆÈ¢òÔºâ
        pin_memory: ÊòØÂê¶‰ΩøÁî®Âõ∫ÂÆöÂÜÖÂ≠òÔºàÈªòËÆ§FalseËäÇÁúÅÂÜÖÂ≠òÔºâ
        
    Returns:
        DataLoader: ÂÆåÊï¥ÂõæÂÉèÊï∞ÊçÆÂä†ËΩΩÂô®
    """
    from monai.data import Dataset
    from torch.utils.data import DataLoader
    
    data_config = config['data']
    val_dir = Path(data_config['val_data_dir'])
    
    # Êî∂ÈõÜNIfTIÊñá‰ª∂
    nifti_files = []
    for pattern in ['*.nii', '*.nii.gz']:
        nifti_files.extend(val_dir.glob(pattern))
    nifti_files.sort()
    
    # Âè™ÂèñÂâçnum_samples‰∏™
    nifti_files = nifti_files[:num_samples]
    
    logger.info(f"ÂàõÂª∫ÂÆåÊï¥ÂõæÂÉèÊï∞ÊçÆÂä†ËΩΩÂô®ÔºåÂÖ± {len(nifti_files)} ‰∏™Ê†∑Êú¨")
    
    # Ëé∑Âèñvoxel_resizeÈÖçÁΩÆ
    data_config = config['data']
    voxel_resize = data_config.get('voxel_resize', None)
    
    # Â§ÑÁêÜvoxel_resize
    if voxel_resize is not None:
        if isinstance(voxel_resize, (list, tuple)):
            if len(voxel_resize) != 3:
                raise ValueError(f"voxel_resize‰Ωú‰∏∫ÂàóË°®Êó∂ÂøÖÈ°ªÂåÖÂê´3‰∏™ÂÖÉÁ¥†[X, Y, Z]Ôºå‰ΩÜÂæóÂà∞‰∫Ü{len(voxel_resize)}‰∏™ÂÖÉÁ¥†")
            voxel_resize_tuple = tuple(voxel_resize)
        else:
            voxel_resize_tuple = (voxel_resize, voxel_resize, voxel_resize)
        logger.info(f"ÂÆåÊï¥ÂõæÂÉèÂ∞ÜÂÖàresizeÂà∞: {voxel_resize_tuple}")
    else:
        voxel_resize_tuple = None
        logger.info("ÂÆåÊï¥ÂõæÂÉè‰∏çËøõË°åÈ¢Ñresize")
    
    # ÂàõÂª∫transformsÔºàÊ†πÊçÆÊòØÂê¶Êúâvoxel_resizeË∞ÉÊï¥ÊµÅÁ®ãÔºâ
    transform_list = [
        transforms.LoadImaged(keys=["image"]),
        transforms.EnsureChannelFirstd(keys=["image"]),
        transforms.Spacingd(
            keys=["image"],
            pixdim=(1.0, 1.0, 1.0),
            mode="bilinear"
        ),
    ]
    
    # Â¶ÇÊûúÈÖçÁΩÆ‰∫Üvoxel_resizeÔºåÊ∑ªÂä†resizeÂèòÊç¢
    if voxel_resize_tuple is not None:
        transform_list.append(
            transforms.Resized(
                keys=["image"],
                spatial_size=voxel_resize_tuple,
                mode="trilinear"
            )
        )
    
    # Ê∑ªÂä†ÂΩí‰∏ÄÂåñÂíåÁ±ªÂûãËΩ¨Êç¢
    transform_list.extend([
        # ÂΩí‰∏ÄÂåñÂà∞[-1, 1]
        transforms.ScaleIntensityRanged(
            keys="image",
            a_min=0.0,
            a_max=255.0,
            b_min=-1.0,
            b_max=1.0,
            clip=True
        ),
        transforms.EnsureTyped(keys=["image"], dtype=torch.float32)
    ])
    
    full_image_transforms = transforms.Compose(transform_list)
    
    # ÂàõÂª∫Êï∞ÊçÆÂàóË°®
    data_list = [{"image": str(f)} for f in nifti_files]
    
    # ÂàõÂª∫Êï∞ÊçÆÈõÜ
    full_image_dataset = Dataset(
        data=data_list,
        transform=full_image_transforms
    )
    
    # ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®Ôºàbatch_size=1ÔºåÂõ†‰∏∫ÂÆåÊï¥ÂõæÂÉèÂ§ßÂ∞èÂèØËÉΩ‰∏ç‰∏ÄËá¥Ôºâ
    # ‰ΩøÁî®num_workers=0ÈÅøÂÖçÂ§öËøõÁ®ãÂÖ±‰∫´ÂÜÖÂ≠òÈóÆÈ¢ò
    full_image_loader = DataLoader(
        full_image_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    logger.info(f"DataLoaderÈÖçÁΩÆ: num_workers={num_workers}, pin_memory={pin_memory}")
    
    return full_image_loader


def visualize_reconstruction(
    autoencoder: torch.nn.Module,
    val_loader,
    device: torch.device,
    writer: SummaryWriter,
    epoch: int,
    config: dict,
    num_samples: int = 4,
    use_sliding_window: bool = True,
    roi_size: tuple = None,
    sw_batch_size: int = 4,
    # ÂéªÂô™ÂèÇÊï∞
    use_denoising: bool = False,
    noise_type: str = "gaussian",
    noise_std: float = 0.1,
    dropout_prob: float = 0.1
):
    """
    ÂèØËßÜÂåñÈáçÂª∫ÁªìÊûúÔºàÊîØÊåÅÂéªÂô™Ëá™ÁºñÁ†ÅÂô®Ôºâ
    
    ÂàÜ‰∏§ÈÉ®ÂàÜÔºö
    1. ‰ΩøÁî®patch-basedÈ™åËØÅÈõÜËøõË°åÁõ¥Êé•ÈáçÂª∫
    2. ÂàõÂª∫ÂÆåÊï¥ÂõæÂÉèÊï∞ÊçÆÂä†ËΩΩÂô®Ôºå‰ΩøÁî®ÊªëÂä®Á™óÂè£Êé®ÁêÜÈáçÂª∫
    
    Args:
        autoencoder: AutoencoderKLÊ®°Âûã
        val_loader: È™åËØÅÊï∞ÊçÆÂä†ËΩΩÂô®Ôºàpatch-basedÔºâ
        device: ËÆæÂ§á
        writer: TensorBoard writer
        epoch: ÂΩìÂâçepoch
        config: ÈÖçÁΩÆÂ≠óÂÖ∏
        num_samples: ÂèØËßÜÂåñÁöÑÊ†∑Êú¨Êï∞Èáè
        use_sliding_window: ÊòØÂê¶‰ΩøÁî®ÊªëÂä®Á™óÂè£Êé®ÁêÜÔºàÁî®‰∫éÂÆåÊï¥ÂõæÂÉèÔºâ
        roi_size: ÊªëÂä®Á™óÂè£ÁöÑROIÂ§ßÂ∞èÔºåÂ¶ÇÊûú‰∏∫NoneÂàô‰ΩøÁî®patchÂ§ßÂ∞è
        sw_batch_size: ÊªëÂä®Á™óÂè£ÊâπÊ¨°Â§ßÂ∞è
        use_denoising: ÊòØÂê¶‰ΩøÁî®ÂéªÂô™Ê®°Âºè
        noise_type: Âô™Â£∞Á±ªÂûã
        noise_std: È´òÊñØÂô™Â£∞Ê†áÂáÜÂ∑Æ
        dropout_prob: dropoutÊ¶ÇÁéá
    """
    autoencoder.eval()
    
    with torch.no_grad():
        # ============ Á¨¨‰∏ÄÈÉ®ÂàÜÔºöPatch-basedÁõ¥Êé•ÈáçÂª∫ ============
        logger.info("=" * 60)
        if use_denoising:
            logger.info("Á¨¨‰∏ÄÈÉ®ÂàÜÔºöPatch-basedÂéªÂô™ÈáçÂª∫")
            logger.info(f"Âô™Â£∞Á±ªÂûã: {noise_type}, È´òÊñØÂô™Â£∞std: {noise_std}, dropoutÊ¶ÇÁéá: {dropout_prob}")
        else:
            logger.info("Á¨¨‰∏ÄÈÉ®ÂàÜÔºöPatch-basedÁõ¥Êé•ÈáçÂª∫")
        
        # Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™batch
        batch = next(iter(val_loader))
        clean_images = batch["image"].to(device)
        
        # Âè™ÂèñÂâçnum_samples‰∏™Ê†∑Êú¨
        clean_images = clean_images[:num_samples]
        
        logger.info(f"ËæìÂÖ•patchÂΩ¢Áä∂: {clean_images.shape}")
        
        # Â¶ÇÊûú‰ΩøÁî®ÂéªÂô™Ê®°ÂºèÔºåÊ∑ªÂä†Âô™Â£∞
        if use_denoising:
            noisy_images = add_noise(
                clean_images,
                noise_type=noise_type,
                noise_std=noise_std,
                dropout_prob=dropout_prob
            )
            # Ê®°ÂûãËæìÂÖ•Â∏¶Âô™Â£∞ÁöÑÂõæÂÉèÔºåÁõÆÊ†áÊòØÊÅ¢Â§çÂπ≤ÂáÄÂõæÂÉè
            reconstruction_direct, _, _ = autoencoder(noisy_images)
            
            # ÁßªÂà∞CPUÂπ∂ËΩ¨Êç¢‰∏∫numpy
            noisy_images_np = noisy_images.cpu().numpy()  # (B, C, H, W, D)
            clean_images_np = clean_images.cpu().numpy()  # (B, C, H, W, D)
            reconstruction_direct_np = reconstruction_direct.cpu().numpy()
            
            # ÂèØËßÜÂåñÂéªÂô™ÈáçÂª∫ÁªìÊûú
            for i in range(min(num_samples, clean_images_np.shape[0])):
                # ÂèñÂá∫Âçï‰∏™Ê†∑Êú¨ (C, H, W, D)
                noisy_vol = noisy_images_np[i, 0]  # (H, W, D)
                clean_vol = clean_images_np[i, 0]  # (H, W, D)
                recon_vol = reconstruction_direct_np[i, 0]  # (H, W, D)
                
                # Â∞Ü3D‰ΩìÁ¥†Ê≤øzËΩ¥ÊäïÂΩ±Êàê2DÂõæÂÉèÔºàÁ¥ØÂä†ÊâÄÊúâzÂ±ÇÔºâ
                noisy_proj = np.sum(noisy_vol, axis=2)  # (H, W)
                clean_proj = np.sum(clean_vol, axis=2)  # (H, W)
                recon_proj = np.sum(recon_vol, axis=2)  # (H, W)
                
                # ÂΩí‰∏ÄÂåñ
                noisy_proj = (noisy_proj - noisy_proj.min()) / (noisy_proj.max() - noisy_proj.min() + 1e-8)
                clean_proj = (clean_proj - clean_proj.min()) / (clean_proj.max() - clean_proj.min() + 1e-8)
                recon_proj = (recon_proj - recon_proj.min()) / (recon_proj.max() - recon_proj.min() + 1e-8)
                
                # Ê∞¥Âπ≥Â†ÜÂè†: Âô™Â£∞ËæìÂÖ• | Ê®°ÂûãÈáçÂª∫ | Âπ≤ÂáÄÁõÆÊ†á
                combined = np.hstack([noisy_proj, recon_proj, clean_proj])  # (H, 3*W)
                
                # Ê∑ªÂä†Âà∞TensorBoard
                writer.add_image(
                    f"patch_denoising/sample_{i}",
                    combined,
                    epoch,
                    dataformats='HW'
                )
                
                # ÈáçÂª∫ËØØÂ∑ÆÂõæÔºöÊ®°ÂûãËæìÂá∫ vs Âπ≤ÂáÄÁõÆÊ†á
                error = np.abs(recon_proj - clean_proj)
                writer.add_image(
                    f"patch_denoising/sample_{i}_error",
                    error,
                    epoch,
                    dataformats='HW'
                )
        else:
            # Ê†áÂáÜÈáçÂª∫Ê®°Âºè
            reconstruction_direct, _, _ = autoencoder(clean_images)
            
            # ÁßªÂà∞CPUÂπ∂ËΩ¨Êç¢‰∏∫numpy
            images_np = clean_images.cpu().numpy()  # (B, C, H, W, D)
            reconstruction_direct_np = reconstruction_direct.cpu().numpy()
            
            # ÂèØËßÜÂåñpatchÈáçÂª∫ÁªìÊûú
            for i in range(min(num_samples, images_np.shape[0])):
                # ÂèñÂá∫Âçï‰∏™Ê†∑Êú¨ (C, H, W, D)
                input_vol = images_np[i, 0]  # (H, W, D)
                recon_vol = reconstruction_direct_np[i, 0]  # (H, W, D)
                
                # Â∞Ü3D‰ΩìÁ¥†Ê≤øzËΩ¥ÊäïÂΩ±Êàê2DÂõæÂÉèÔºàÁ¥ØÂä†ÊâÄÊúâzÂ±ÇÔºâ
                input_proj = np.sum(input_vol, axis=2)  # (H, W)
                recon_proj = np.sum(recon_vol, axis=2)  # (H, W)
                
                # ÂΩí‰∏ÄÂåñ
                input_proj = (input_proj - input_proj.min()) / (input_proj.max() - input_proj.min() + 1e-8)
                recon_proj = (recon_proj - recon_proj.min()) / (recon_proj.max() - recon_proj.min() + 1e-8)
                
                # Ê∞¥Âπ≥Â†ÜÂè†: ËæìÂÖ• | ÈáçÂª∫
                combined = np.hstack([input_proj, recon_proj])  # (H, 2*W)
                
                # Ê∑ªÂä†Âà∞TensorBoard
                writer.add_image(
                    f"patch_reconstruction/sample_{i}",
                    combined,
                    epoch,
                    dataformats='HW'
                )
                
                # ËØØÂ∑ÆÂõæ
                error = np.abs(input_proj - recon_proj)
                writer.add_image(
                    f"patch_reconstruction/sample_{i}_error",
                    error,
                    epoch,
                    dataformats='HW'
                )
        
        logger.info(f"Â∑≤‰øùÂ≠ò {min(num_samples, clean_images.shape[0])} ‰∏™patchÈáçÂª∫ÂèØËßÜÂåñÁªìÊûú")
        
        # ============ Á¨¨‰∫åÈÉ®ÂàÜÔºöÂÆåÊï¥ÂõæÂÉèÊªëÂä®Á™óÂè£Êé®ÁêÜ ============
        if use_sliding_window:
            logger.info("=" * 60)
            logger.info("Á¨¨‰∫åÈÉ®ÂàÜÔºöÂÆåÊï¥ÂõæÂÉèÊªëÂä®Á™óÂè£Êé®ÁêÜ")
            
            try:
                # Ê∏ÖÁêÜGPUÁºìÂ≠òÔºåÈáäÊîæÂÜÖÂ≠ò
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info(f"Ê∏ÖÁêÜGPUÁºìÂ≠òÔºåÂΩìÂâçÊòæÂ≠ò‰ΩøÁî®: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
                
                # ÂàõÂª∫ÂÆåÊï¥ÂõæÂÉèÊï∞ÊçÆÂä†ËΩΩÂô®Ôºà‰ΩøÁî®ÂÜÖÂ≠ò‰ºòÂåñÂèÇÊï∞Ôºâ
                full_image_loader = create_full_image_dataloader(
                    config, 
                    num_samples,
                    num_workers=0,  # ÈÅøÂÖçÂÖ±‰∫´ÂÜÖÂ≠òÈóÆÈ¢ò
                    pin_memory=False  # ËäÇÁúÅÂÜÖÂ≠ò
                )
                
                # Êé®Êñ≠ROIÂ§ßÂ∞èÔºà‰ΩøÁî®ËÆ≠ÁªÉpatchÂ§ßÂ∞èÔºâ
                if roi_size is None:
                    roi_size = clean_images.shape[2:]  # (H, W, D)
                    logger.info(f"‰ΩøÁî®ËÆ≠ÁªÉpatchÂ§ßÂ∞è‰Ωú‰∏∫ÊªëÂä®Á™óÂè£ROI: {roi_size}")
                
                # ÂàõÂª∫ÊªëÂä®Á™óÂè£Êé®ÁêÜÂô®
                sw_inferer = AutoencoderSlidingWindowInferer(
                    autoencoder=autoencoder,
                    roi_size=roi_size,
                    sw_batch_size=sw_batch_size,
                    overlap=0.25,
                    mode="gaussian",
                    device=device
                )
                
                # ÂØπÊØè‰∏™ÂÆåÊï¥ÂõæÂÉèËøõË°åÊªëÂä®Á™óÂè£Êé®ÁêÜ
                for idx, batch in enumerate(full_image_loader):
                    if idx >= num_samples:
                        break
                    
                    try:
                        full_image = batch["image"].to(device)  # (1, C, H, W, D)
                        logger.info(f"ÂÆåÊï¥ÂõæÂÉè {idx} ÂΩ¢Áä∂: {full_image.shape}")
                        
                        # ÊªëÂä®Á™óÂè£Êé®ÁêÜ
                        reconstruction_sw = sw_inferer.reconstruct(full_image, return_latent=False)
                        logger.info(f"ÊªëÂä®Á™óÂè£ÈáçÂª∫ÂÆåÊàêÔºåËæìÂá∫ÂΩ¢Áä∂: {reconstruction_sw.shape}")
                        
                        # ÁßªÂà∞CPUÂπ∂ËΩ¨Êç¢‰∏∫numpy
                        full_image_np = full_image.cpu().numpy()[0, 0]  # (H, W, D)
                        reconstruction_sw_np = reconstruction_sw.cpu().numpy()[0, 0]  # (H, W, D)
                        
                        # ÈáäÊîæGPUÂÜÖÂ≠ò
                        del full_image, reconstruction_sw
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        
                        # Â∞Ü3D‰ΩìÁ¥†Ê≤øzËΩ¥ÊäïÂΩ±Êàê2DÂõæÂÉè
                        full_image_proj = np.sum(full_image_np, axis=2)  # (H, W)
                        recon_sw_proj = np.sum(reconstruction_sw_np, axis=2)  # (H, W)
                        
                        # ÂΩí‰∏ÄÂåñ
                        full_image_proj = (full_image_proj - full_image_proj.min()) / (full_image_proj.max() - full_image_proj.min() + 1e-8)
                        recon_sw_proj = (recon_sw_proj - recon_sw_proj.min()) / (recon_sw_proj.max() - recon_sw_proj.min() + 1e-8)
                        
                        # Ê∞¥Âπ≥Â†ÜÂè†: ËæìÂÖ•ÂÆåÊï¥ÂõæÂÉè | ÊªëÂä®Á™óÂè£ÈáçÂª∫
                        combined = np.hstack([full_image_proj, recon_sw_proj])  # (H, 2*W)
                        
                        # Ê∑ªÂä†Âà∞TensorBoard
                        writer.add_image(
                            f"full_image_sliding_window/sample_{idx}",
                            combined,
                            epoch,
                            dataformats='HW'
                        )
                        
                        # ËØØÂ∑ÆÂõæ
                        error_sw = np.abs(full_image_proj - recon_sw_proj)
                        writer.add_image(
                            f"full_image_sliding_window/sample_{idx}_error",
                            error_sw,
                            epoch,
                            dataformats='HW'
                        )
                        
                        logger.info(f"‚úì ÂÆåÊàêÊ†∑Êú¨ {idx}")
                        
                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            logger.error(f"‚úó Ê†∑Êú¨ {idx} Êé®ÁêÜÂ§±Ë¥•ÔºöGPUÊòæÂ≠ò‰∏çË∂≥")
                            logger.error(f"  Âª∫ËÆÆ: 1) ÂáèÂ∞èsw_batch_size (ÂΩìÂâç={sw_batch_size})")
                            logger.error(f"        2) ÂáèÂ∞èROIÂ§ßÂ∞è (ÂΩìÂâç={roi_size})")
                            logger.error(f"        3) Â¢ûÂä†GPUÊòæÂ≠òÊàñ‰ΩøÁî®Êõ¥Â∞èÁöÑÂõæÂÉè")
                            # Ê∏ÖÁêÜÂπ∂ÁªßÁª≠‰∏ã‰∏Ä‰∏™Ê†∑Êú¨
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                        else:
                            raise
                
                logger.info(f"Â∑≤‰øùÂ≠ò {min(idx + 1, num_samples)} ‰∏™ÂÆåÊï¥ÂõæÂÉèÊªëÂä®Á™óÂè£ÈáçÂª∫ÂèØËßÜÂåñÁªìÊûú")
                
            except Exception as e:
                logger.error(f"ÊªëÂä®Á™óÂè£Êé®ÁêÜËøáÁ®ãÂá∫Èîô: {e}")
                logger.error("Ë∑≥ËøáÊªëÂä®Á™óÂè£ÂèØËßÜÂåñ")
            finally:
                # ÊúÄÁªàÊ∏ÖÁêÜ
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        logger.info("=" * 60)


def train_autoencoder(config_path: str):
    """
    ËÆ≠ÁªÉAutoencoderKL
    
    Args:
        config_path: ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
    """
    # Âä†ËΩΩÈÖçÁΩÆ
    config = load_config(config_path)
    
    # ÊèêÂèñÈÖçÁΩÆÂèÇÊï∞
    ae_config = config['autoencoder']
    checkpoint_config = ae_config['checkpoints']
    log_config = ae_config['logging']
    
    # Ê∏ÖÁêÜ‰πãÂâçÁöÑËæìÂá∫ÁõÆÂΩï
    output_dir = Path(checkpoint_config['output_dir'])
    log_dir = Path(log_config['log_dir'])
    
    if output_dir.exists():
        logger.info(f"Âà†Èô§‰πãÂâçÁöÑËæìÂá∫ÁõÆÂΩï: {output_dir}")
        shutil.rmtree(output_dir)
    
    if log_dir.exists():
        logger.info(f"Âà†Èô§‰πãÂâçÁöÑÊó•ÂøóÁõÆÂΩï: {log_dir}")
        shutil.rmtree(log_dir)
    
    # ÂàõÂª∫Êñ∞ÁöÑÁõÆÂΩï
    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê
    set_determinism(config.get('seed', 42))
    
    # ËÆæÁΩÆËÆæÂ§á
    device_config = config.get('device', {})
    use_cuda = device_config.get('use_cuda', True) and torch.cuda.is_available()
    device = torch.device(f"cuda:{device_config.get('gpu_id', 0)}" if use_cuda else "cpu")
    logger.info(f"‰ΩøÁî®ËÆæÂ§á: {device}")
    
    # ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®
    train_loader, val_loader = create_train_val_dataloaders(config)
    
    # Ê∏≤ÊüìÁ¨¨‰∏Ä‰∏™batchÔºåÈ™åËØÅÊï∞ÊçÆÂä†ËΩΩÂô®ÁöÑÊ≠£Á°ÆÊÄß
    visualize_data_loader(train_loader, "train")
    visualize_data_loader(val_loader, "val")
    
    # ÊèêÂèñËÆ≠ÁªÉÈÖçÁΩÆÂèÇÊï∞
    train_config = ae_config['training']
    
    # ÂàõÂª∫AutoencoderKL
    # Ëé∑Âèñdownsample_factorsÔºàÂ¶ÇÊûúÈÖçÁΩÆ‰∏≠Â≠òÂú®Ôºâ
    downsample_factors = ae_config.get('downsample_factors', None)
    initial_downsample_factor = ae_config.get('initial_downsample_factor', 1)
    use_conv_downsample = ae_config.get('use_conv_downsample', True)
    use_convtranspose = ae_config.get('use_convtranspose', False)
    
    if downsample_factors is not None:
        downsample_factors = tuple(downsample_factors)
        total_downsample = initial_downsample_factor
        for factor in downsample_factors:
            total_downsample *= factor
        logger.info(f"‰ΩøÁî®Ëá™ÂÆö‰πâ‰∏ãÈááÊ†∑Âõ†Â≠ê: initial={initial_downsample_factor}, layers={downsample_factors}")
        logger.info(f"ÊÄª‰∏ãÈááÊ†∑ÂÄçÊï∞: {total_downsample}x")
    else:
        # ÈªòËÆ§ÊØèÂ±Ç2ÂÄç‰∏ãÈááÊ†∑
        total_downsample = initial_downsample_factor * (2 ** (len(ae_config['num_channels']) - 1))
        logger.info(f"‰ΩøÁî®ÈªòËÆ§‰∏ãÈááÊ†∑ÈÖçÁΩÆ: initial={initial_downsample_factor}, ÊÄª‰∏ãÈááÊ†∑ÂÄçÊï∞: {total_downsample}x")
    
    # ËÆ∞ÂΩïÈááÊ†∑ÊñπÊ≥ï
    downsample_method = "Âç∑ÁßØ‰∏ãÈááÊ†∑" if use_conv_downsample else "Âπ≥ÂùáÊ±†Âåñ‰∏ãÈááÊ†∑"
    upsample_method = "ËΩ¨ÁΩÆÂç∑ÁßØ‰∏äÈááÊ†∑" if use_convtranspose else "ÊúÄËøëÈÇªÊèíÂÄº+Âç∑ÁßØ‰∏äÈááÊ†∑"
    logger.info(f"‰∏ãÈááÊ†∑ÊñπÊ≥ï: {downsample_method}")
    logger.info(f"‰∏äÈááÊ†∑ÊñπÊ≥ï: {upsample_method}")
    
    autoencoder = AutoencoderKL(
        spatial_dims=ae_config['spatial_dims'],
        in_channels=ae_config['in_channels'],
        out_channels=ae_config['out_channels'],
        num_channels=tuple(ae_config['num_channels']),
        latent_channels=ae_config['latent_channels'],
        num_res_blocks=ae_config['num_res_blocks'],
        norm_num_groups=ae_config.get('norm_num_groups', 16),
        attention_levels=tuple(ae_config['attention_levels']),
        downsample_factors=downsample_factors,
        initial_downsample_factor=initial_downsample_factor,
        use_conv_downsample=use_conv_downsample,
        use_convtranspose=use_convtranspose
    )
    
    # ÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ‰ª•ËäÇÁúÅÊòæÂ≠ò
    if train_config.get('use_gradient_checkpointing', False):
        logger.info("ÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàGradient CheckpointingÔºâ")
        if hasattr(autoencoder, 'enable_gradient_checkpointing'):
            autoencoder.enable_gradient_checkpointing()
    
    autoencoder.to(device)
    logger.info("ÂàõÂª∫AutoencoderKLÊ®°Âûã")
    
    # ÂàõÂª∫Âà§Âà´Âô®
    disc_config = train_config['discriminator']
    discriminator = PatchDiscriminator(
        spatial_dims=ae_config['spatial_dims'],
        num_layers_d=disc_config['num_layers_d'],
        num_channels=disc_config['num_channels'],
        in_channels=ae_config['in_channels'],
        out_channels=ae_config['out_channels']
    )
    discriminator.to(device)
    logger.info("ÂàõÂª∫PatchDiscriminator")
    
    # ==================== ÂÆö‰πâÈáçÂª∫ÊçüÂ§±ÂáΩÊï∞ ====================
    loss_config = train_config.get('loss', {})
    recon_loss_type = loss_config.get('reconstruction_loss_type', 'l1')  # 'l1', 'weighted', 'dice', 'combined'
    
    logger.info("=" * 60)
    logger.info(f"ÈÖçÁΩÆÈáçÂª∫ÊçüÂ§±ÂáΩÊï∞: {recon_loss_type}")
    
    if recon_loss_type == 'l1':
        # Ê†áÂáÜL1ÊçüÂ§±
        recon_loss_fn = L1Loss()
        logger.info("‰ΩøÁî®Ê†áÂáÜL1Loss")
    
    elif recon_loss_type == 'mse':
        # Ê†áÂáÜMSEÊçüÂ§±
        recon_loss_fn = MSELoss()
        logger.info("‰ΩøÁî®Ê†áÂáÜMSELoss")
    
    elif recon_loss_type == 'weighted':
        # Âä†ÊùÉÈáçÂª∫ÊçüÂ§±
        weighted_config = loss_config.get('weighted', {})
        recon_loss_fn = WeightedReconstructionLoss(
            loss_type=weighted_config.get('loss_type', 'l1'),
            foreground_weight=weighted_config.get('foreground_weight', 10.0),
            background_weight=weighted_config.get('background_weight', 1.0),
            threshold=weighted_config.get('threshold', 0.1)
        )
    
    elif recon_loss_type == 'dice':
        # Á∫ØDiceÊçüÂ§±
        dice_config = loss_config.get('dice', {})
        recon_loss_fn = DiceLoss(
            smooth=dice_config.get('smooth', 1e-5),
            sigmoid=dice_config.get('sigmoid', False)
        )
        logger.info(f"‰ΩøÁî®Dice Loss (smooth={dice_config.get('smooth', 1e-5)})")
    
    elif recon_loss_type == 'combined':
        # ÁªÑÂêàÊçüÂ§±ÔºöDice + Âä†ÊùÉÈáçÂª∫
        combined_config = loss_config.get('combined', {})
        recon_loss_fn = CombinedReconstructionLoss(
            dice_weight=combined_config.get('dice_weight', 1.0),
            recon_weight=combined_config.get('recon_weight', 1.0),
            recon_loss_type=combined_config.get('recon_loss_type', 'l1'),
            foreground_weight=combined_config.get('foreground_weight', 10.0),
            background_weight=combined_config.get('background_weight', 1.0),
            threshold=combined_config.get('threshold', 0.1),
            dice_smooth=combined_config.get('dice_smooth', 1e-5)
        )
    
    else:
        raise ValueError(f"‰∏çÊîØÊåÅÁöÑÈáçÂª∫ÊçüÂ§±Á±ªÂûã: {recon_loss_type}")
    
    logger.info("=" * 60)
    
    # ÂØπÊäóÊçüÂ§±
    adv_loss = PatchAdversarialLoss(criterion="least_squares")
    
    # Âà§Êñ≠ÊòØÂê¶‰ΩøÁî®ÊÑüÁü•ÊçüÂ§±ÔºàÂ§ßÂàÜËæ®ÁéáÊó∂ÂèØ‰ª•Á¶ÅÁî®‰ª•ËäÇÁúÅÊòæÂ≠òÔºâ
    use_perceptual_loss = train_config.get('use_perceptual_loss', True)
    if use_perceptual_loss:
        loss_perceptual = PerceptualLoss(
            spatial_dims=ae_config['spatial_dims'],
            network_type="squeeze",
            is_fake_3d=True,
            fake_3d_ratio=0.2
        )
        loss_perceptual.to(device)
        logger.info("ÂêØÁî®ÊÑüÁü•ÊçüÂ§±ÔºàPerceptualLossÔºâ")
    else:
        loss_perceptual = None
        logger.info("Á¶ÅÁî®ÊÑüÁü•ÊçüÂ§±‰ª•ËäÇÁúÅÊòæÂ≠ò")
    
    # ÊçüÂ§±ÊùÉÈáç
    adv_weight = train_config['adv_weight']
    perceptual_weight = train_config.get('perceptual_weight', 0.001) if use_perceptual_loss else 0.0
    kl_weight = train_config['kl_weight']
    autoencoder_warm_up_n_epochs = train_config['autoencoder_warm_up_n_epochs']
    
    # ÂàõÂª∫‰ºòÂåñÂô®
    optimizer_g = torch.optim.Adam(
        params=autoencoder.parameters(),
        lr=train_config['learning_rate']
    )
    optimizer_d = torch.optim.Adam(
        params=discriminator.parameters(),
        lr=train_config['learning_rate']
    )
    
    # Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ
    use_amp = device_config.get('mixed_precision', False) and torch.cuda.is_available()
    if use_amp:
        scaler_g = GradScaler()
        scaler_d = GradScaler()
        logger.info("ÂêØÁî®Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÔºàAMPÔºâ")
    else:
        scaler_g = None
        scaler_d = None
        logger.info("‰ΩøÁî®FP32ËÆ≠ÁªÉ")
    
    # Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞
    gradient_accumulation_steps = train_config.get('gradient_accumulation_steps', 1)
    if gradient_accumulation_steps > 1:
        logger.info(f"‰ΩøÁî®Ê¢ØÂ∫¶Á¥ØÁßØ: {gradient_accumulation_steps} Ê≠•")
    
    # TensorBoard
    writer = SummaryWriter(log_dir=str(log_dir))
    
    # ËÆ≠ÁªÉÂèÇÊï∞
    n_epochs = train_config['n_epochs']
    val_interval = train_config['val_interval']
    save_interval = train_config['save_interval']
    log_interval = log_config['log_interval']
    visualize_interval = log_config.get('visualize_interval', 10)  # ÈªòËÆ§ÊØè10‰∏™epochÂèØËßÜÂåñ‰∏ÄÊ¨°
    num_visualize_samples = log_config.get('num_visualize_samples', 4)  # ÈªòËÆ§ÂèØËßÜÂåñ4‰∏™Ê†∑Êú¨
    
    # ÊªëÂä®Á™óÂè£Êé®ÁêÜÈÖçÁΩÆ
    use_sliding_window_vis = log_config.get('use_sliding_window', False)  # ÈªòËÆ§ÂêØÁî®ÊªëÂä®Á™óÂè£Êé®ÁêÜ
    sw_roi_size = log_config.get('sliding_window_roi_size', None)  # NoneË°®Á§∫Ëá™Âä®Êé®Êñ≠
    sw_batch_size = log_config.get('sliding_window_batch_size', 4)  # ÊªëÂä®Á™óÂè£ÊâπÊ¨°Â§ßÂ∞è
    
    # ==================== ÂéªÂô™Ëá™ÁºñÁ†ÅÂô®ÈÖçÁΩÆ ====================
    denoising_config = train_config.get('denoising', {})
    use_denoising = denoising_config.get('enabled', False)
    noise_type = denoising_config.get('noise_type', 'gaussian')
    noise_std = denoising_config.get('noise_std', 0.1)
    dropout_prob = denoising_config.get('dropout_prob', 0.1)
    
    if use_denoising:
        logger.info("=" * 60)
        logger.info("üî• ÂêØÁî®ÂéªÂô™Ëá™ÁºñÁ†ÅÂô®Ê®°Âºè (Denoising Autoencoder)")
        logger.info(f"  Âô™Â£∞Á±ªÂûã: {noise_type}")
        logger.info(f"  È´òÊñØÂô™Â£∞Ê†áÂáÜÂ∑Æ: {noise_std}")
        logger.info(f"  DropoutÊ¶ÇÁéá: {dropout_prob}")
        logger.info("  Ê®°ÂûãÂ∞ÜÂ≠¶‰π†‰ªéÂô™Â£∞‰∏≠ÊÅ¢Â§çÂπ≤ÂáÄÂõæÂÉèÔºåËø´‰ΩøÂÖ∂Â≠¶‰π†Êï∞ÊçÆÁöÑÊ∑±Â±ÇÁâπÂæÅÔºÅ")
        logger.info("=" * 60)
    else:
        logger.info("‰ΩøÁî®Ê†áÂáÜËá™ÁºñÁ†ÅÂô®Ê®°Âºè")
    
    # Âø´ÈÄüÂºÄÂèëÊ®°Âºè
    fast_dev_run = train_config.get('fast_dev_run', False)
    fast_dev_run_batches = train_config.get('fast_dev_run_batches', 2)
    
    if fast_dev_run:
        logger.info(f"**Âø´ÈÄüÂºÄÂèëÊ®°Âºè**: ÊØè‰∏™epochÂè™ËøêË°å {fast_dev_run_batches} ‰∏™batch")
        n_epochs = 5  # Âø´ÈÄüÊ®°ÂºèÂè™ËøêË°å2‰∏™epoch
        val_interval = 1
        save_interval = 1
        log_interval = 1
        visualize_interval = 1
        num_visualize_samples = 2
    
    # ÊÅ¢Â§çËÆ≠ÁªÉ
    start_epoch = 0
    best_val_loss = float('inf')
    
    resume_from = checkpoint_config.get('resume_from')
    if resume_from and Path(resume_from).exists():
        start_epoch, best_val_loss = load_checkpoint(
            resume_from, autoencoder, discriminator, optimizer_g, optimizer_d
        )
    
    # ËÆ≠ÁªÉÂæ™ÁéØ
    logger.info(f"ÂºÄÂßãËÆ≠ÁªÉAutoencoderKL: {n_epochs} epochs")
    
    for epoch in range(start_epoch, n_epochs):
        autoencoder.train()
        discriminator.train()
        
        epoch_loss = 0
        epoch_recon_loss = 0
        epoch_kl_loss = 0
        gen_epoch_loss = 0
        disc_epoch_loss = 0
        
        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=120)
        progress_bar.set_description(f"Epoch {epoch}/{n_epochs}")
        
        for step, batch in progress_bar:
            # Âø´ÈÄüÂºÄÂèëÊ®°ÂºèÔºöÂè™ËøêË°åÊåáÂÆöÊï∞ÈáèÁöÑbatch
            if fast_dev_run and step >= fast_dev_run_batches:
                break
            
            # ÂéüÂßãÂπ≤ÂáÄÂõæÂÉè
            clean_images = batch["image"].to(device)
            
            # ============ ÂéªÂô™Ëá™ÁºñÁ†ÅÂô®ÔºöÊ∑ªÂä†Âô™Â£∞ ============
            if use_denoising:
                # ÁªôËæìÂÖ•Ê∑ªÂä†Âô™Â£∞
                noisy_images = add_noise(
                    clean_images,
                    noise_type=noise_type,
                    noise_std=noise_std,
                    dropout_prob=dropout_prob
                )
                # Ê®°ÂûãËæìÂÖ•ÔºöÂ∏¶Âô™Â£∞ÁöÑÂõæÂÉè
                input_images = noisy_images
                # ÁõÆÊ†áËæìÂá∫ÔºöÂéüÂßãÂπ≤ÂáÄÁöÑÂõæÂÉè
                target_images = clean_images
            else:
                # Ê†áÂáÜËá™ÁºñÁ†ÅÂô®ÔºöËæìÂÖ•=ËæìÂá∫
                input_images = clean_images
                target_images = clean_images
            
            # ============ GeneratorÈÉ®ÂàÜ ============
            # Ê¢ØÂ∫¶Ê∏ÖÈõ∂ÔºàÂú®Á¥ØÁßØÂºÄÂßãÊó∂Ôºâ
            if step % gradient_accumulation_steps == 0:
                optimizer_g.zero_grad(set_to_none=True)
            
            # Ê∑∑ÂêàÁ≤æÂ∫¶ÂâçÂêë‰º†Êí≠
            with autocast(enabled=use_amp):
                reconstruction, z_mu, z_sigma = autoencoder(input_images)
                kl = KL_loss(z_mu, z_sigma)
                
                # ÈáçÂª∫ÊçüÂ§±ÔºöÂØπÊØîÈáçÂª∫ÁªìÊûúÂíåÂπ≤ÂáÄÁõÆÊ†á
                recons_loss = recon_loss_fn(reconstruction.float(), target_images.float())
                
                # ÊÑüÁü•ÊçüÂ§±ÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
                if use_perceptual_loss:
                    p_loss = loss_perceptual(reconstruction.float(), target_images.float())
                    loss_g = recons_loss + kl_weight * kl + perceptual_weight * p_loss
                else:
                    loss_g = recons_loss + kl_weight * kl
                
                # ÂØπÊäóÊçüÂ§±Ôºàwarm-upÂêéÔºâ
                generator_loss_val = 0.0
                if epoch >= autoencoder_warm_up_n_epochs:
                    logits_fake = discriminator(reconstruction.contiguous().float())[-1]
                    generator_loss = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)
                    loss_g += adv_weight * generator_loss
                    generator_loss_val = generator_loss.item()
                    gen_epoch_loss += generator_loss_val
                
                # Ê¢ØÂ∫¶Á¥ØÁßØÔºöÈô§‰ª•Á¥ØÁßØÊ≠•Êï∞
                loss_g = loss_g / gradient_accumulation_steps
            
            # ÂèçÂêë‰º†Êí≠
            if use_amp:
                scaler_g.scale(loss_g).backward()
                # Âú®Á¥ØÁßØÁªìÊùüÊó∂Êõ¥Êñ∞ÂèÇÊï∞
                if (step + 1) % gradient_accumulation_steps == 0:
                    scaler_g.step(optimizer_g)
                    scaler_g.update()
            else:
                loss_g.backward()
                # Âú®Á¥ØÁßØÁªìÊùüÊó∂Êõ¥Êñ∞ÂèÇÊï∞
                if (step + 1) % gradient_accumulation_steps == 0:
                    optimizer_g.step()
            
            # ============ DiscriminatorÈÉ®ÂàÜ ============
            discriminator_loss_val = 0.0
            if epoch >= autoencoder_warm_up_n_epochs:
                # Ê¢ØÂ∫¶Ê∏ÖÈõ∂ÔºàÂú®Á¥ØÁßØÂºÄÂßãÊó∂Ôºâ
                if step % gradient_accumulation_steps == 0:
                    optimizer_d.zero_grad(set_to_none=True)
                
                with autocast(enabled=use_amp):
                    logits_fake = discriminator(reconstruction.contiguous().detach())[-1]
                    loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)
                    
                    # Âà§Âà´Âô®Âà§Êñ≠ÁöÑÊòØÁõÆÊ†áÔºàÂπ≤ÂáÄÔºâÂõæÂÉèÁöÑÁúüÂÅá
                    logits_real = discriminator(target_images.contiguous().detach())[-1]
                    loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)
                    
                    discriminator_loss = (loss_d_fake + loss_d_real) * 0.5
                    loss_d = adv_weight * discriminator_loss / gradient_accumulation_steps
                    discriminator_loss_val = discriminator_loss.item()
                
                # ÂèçÂêë‰º†Êí≠
                if use_amp:
                    scaler_d.scale(loss_d).backward()
                    # Âú®Á¥ØÁßØÁªìÊùüÊó∂Êõ¥Êñ∞ÂèÇÊï∞
                    if (step + 1) % gradient_accumulation_steps == 0:
                        scaler_d.step(optimizer_d)
                        scaler_d.update()
                else:
                    loss_d.backward()
                    # Âú®Á¥ØÁßØÁªìÊùüÊó∂Êõ¥Êñ∞ÂèÇÊï∞
                    if (step + 1) % gradient_accumulation_steps == 0:
                        optimizer_d.step()
                
                disc_epoch_loss += discriminator_loss_val
            
            # ËÆ∞ÂΩïÊçüÂ§±ÔºàÊ≥®ÊÑèloss_gÂ∑≤ÁªèÈô§‰ª•‰∫Ügradient_accumulation_stepsÔºâ
            epoch_loss += loss_g.item() * gradient_accumulation_steps
            epoch_recon_loss += recons_loss.item()
            epoch_kl_loss += kl.item()
            
            # Êõ¥Êñ∞ËøõÂ∫¶Êù°
            progress_bar.set_postfix({
                "loss": f"{epoch_loss / (step + 1):.4f}",
                "recon": f"{epoch_recon_loss / (step + 1):.4f}",
                "kl": f"{epoch_kl_loss / (step + 1):.4f}",
                "gen": f"{gen_epoch_loss / (step + 1):.4f}" if epoch >= autoencoder_warm_up_n_epochs else "N/A",
                "disc": f"{disc_epoch_loss / (step + 1):.4f}" if epoch >= autoencoder_warm_up_n_epochs else "N/A"
            })
            
            # TensorBoardÊó•Âøó
            if step % log_interval == 0:
                global_step = epoch * len(train_loader) + step
                writer.add_scalar("train/step/total_loss", loss_g.item() * gradient_accumulation_steps, global_step)
                writer.add_scalar("train/step/recon_loss", recons_loss.item(), global_step)
                writer.add_scalar("train/step/kl_loss", kl.item(), global_step)
                if epoch >= autoencoder_warm_up_n_epochs:
                    writer.add_scalar("train/step/gen_loss", generator_loss_val, global_step)
                    writer.add_scalar("train/step/disc_loss", discriminator_loss_val, global_step)
        
        # ËÆ∞ÂΩïepochÂπ≥ÂùáÊçüÂ§±
        n_steps = step + 1
        avg_loss = epoch_loss / n_steps
        avg_recon = epoch_recon_loss / n_steps
        avg_kl = epoch_kl_loss / n_steps
        
        writer.add_scalar("train/epoch/total_loss", avg_loss, epoch)
        writer.add_scalar("train/epoch/recon_loss", avg_recon, epoch)
        writer.add_scalar("train/epoch/kl_loss", avg_kl, epoch)
        
        logger.info(f"Epoch {epoch} ËÆ≠ÁªÉÊçüÂ§±: total={avg_loss:.4f}, recon={avg_recon:.4f}, kl={avg_kl:.4f}")
        
        # È™åËØÅ
        if (epoch + 1) % val_interval == 0 or epoch == n_epochs - 1:
            val_loss, val_recon, val_kl = validate(
                autoencoder, val_loader, device, kl_weight, fast_dev_run, fast_dev_run_batches,
                recon_loss_fn=recon_loss_fn,  # ‰º†ÂÖ•ÈáçÂª∫ÊçüÂ§±ÂáΩÊï∞
                # ÂéªÂô™ÂèÇÊï∞
                use_denoising=use_denoising,
                noise_type=noise_type,
                noise_std=noise_std,
                dropout_prob=dropout_prob
            )
            
            writer.add_scalar("val/epoch/total_loss", val_loss, epoch)
            writer.add_scalar("val/epoch/recon_loss", val_recon, epoch)
            writer.add_scalar("val/epoch/kl_loss", val_kl, epoch)
            
            logger.info(f"Epoch {epoch} È™åËØÅÊçüÂ§±: total={val_loss:.4f}, recon={val_recon:.4f}, kl={val_kl:.4f}")
            
            # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
                logger.info(f"Êñ∞ÁöÑÊúÄ‰Ω≥È™åËØÅÊçüÂ§±: {best_val_loss:.4f}")
        else:
            is_best = False
        
        # ÂèØËßÜÂåñÈáçÂª∫ÁªìÊûú
        if (epoch + 1) % visualize_interval == 0 or epoch == n_epochs - 1:
            logger.info("ÁîüÊàêÈáçÂª∫ÂèØËßÜÂåñÁªìÊûú...")
            visualize_reconstruction(
                autoencoder=autoencoder,
                val_loader=val_loader,
                device=device,
                writer=writer,
                epoch=epoch,
                config=config,
                num_samples=num_visualize_samples,
                use_sliding_window=use_sliding_window_vis,
                roi_size=sw_roi_size,
                sw_batch_size=sw_batch_size,
                # ÂéªÂô™ÂèÇÊï∞
                use_denoising=use_denoising,
                noise_type=noise_type,
                noise_std=noise_std,
                dropout_prob=dropout_prob
            )
        
        # ‰øùÂ≠òcheckpoint
        if (epoch + 1) % save_interval == 0 or epoch == n_epochs - 1 or is_best:
            save_checkpoint(
                epoch=epoch,
                autoencoder=autoencoder,
                discriminator=discriminator,
                optimizer_g=optimizer_g,
                optimizer_d=optimizer_d,
                best_val_loss=best_val_loss,
                output_dir=checkpoint_config['output_dir'],
                is_best=is_best
            )
    
    writer.close()
    logger.info("AutoencoderKLËÆ≠ÁªÉÂÆåÊàê!")


def main():
    parser = argparse.ArgumentParser(description="ËÆ≠ÁªÉAutoencoderKL")
    parser.add_argument(
        '--config',
        type=str,
        default='monai_diffusion/config/ldm_config_local.yaml',
        help='ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ'
    )
    
    args = parser.parse_args()
    train_autoencoder(args.config)


if __name__ == "__main__":
    main()

