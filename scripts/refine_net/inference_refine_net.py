#!/usr/bin/env python3
"""
RefineNetæ¨ç†è„šæœ¬

ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„RefineNetæ¨¡å‹è¿›è¡Œç‚¹äº‘ä¿®æ­£æ¨ç†ã€‚
æ”¯æŒå¤§è§„æ¨¡ç‚¹äº‘åˆ†æ‰¹å¤„ç†å’Œç»“æœä¿å­˜ä¸ºCSVæ ¼å¼ã€‚
"""

import sys
import os
from pathlib import Path
import argparse
import torch
import numpy as np
import pandas as pd
import h5py
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥RefineNetæ¨¡å—
from src.refine_net import options, models, util
from src.refine_net.data_handler import H5InferenceDataset

def load_model(model_path, device, args):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = models.PointNet2Generator(device, args)
    
    if model_path.suffix == '.pt':
        # ç›´æ¥åŠ è½½æ¨¡å‹æƒé‡
        model.load_state_dict(torch.load(model_path, map_location=device))
    else:
        # åŠ è½½å®Œæ•´æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
    
    model.to(device)
    model.eval()
    return model

def process_large_pointcloud(model, noisy_pc, batch_size, device):
    """
    åˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡ç‚¹äº‘
    
    Args:
        model: æ¨¡å‹
        noisy_pc: å™ªå£°ç‚¹äº‘ (3, N)
        batch_size: æ‰¹å¤„ç†å¤§å°
        device: è®¾å¤‡
        
    Returns:
        torch.Tensor: ä¿®æ­£åçš„ç‚¹äº‘ (3, N)
    """
    num_points = noisy_pc.shape[1]
    refined_batches = []
    
    for start_idx in tqdm(range(0, num_points, batch_size), desc="å¤„ç†æ‰¹æ¬¡"):
        end_idx = min(start_idx + batch_size, num_points)
        
        # è·å–æ‰¹æ¬¡
        batch = noisy_pc[:, start_idx:end_idx]  # (3, batch_size)
        
        # å¦‚æœæ‰¹æ¬¡å¤ªå°ï¼Œè¿›è¡Œpadding
        if batch.shape[1] < batch_size:
            padding_size = batch_size - batch.shape[1]
            # é‡å¤æœ€åå‡ ä¸ªç‚¹è¿›è¡Œpadding
            repeat_indices = torch.randint(0, batch.shape[1], (padding_size,), device=device)
            padding = batch[:, repeat_indices]
            batch = torch.cat([batch, padding], dim=1)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            refined_batch = model(batch.unsqueeze(0))[0]  # (3, batch_size)
        
        # ç§»é™¤padding
        if end_idx - start_idx < batch_size:
            refined_batch = refined_batch[:, :end_idx - start_idx]
        
        refined_batches.append(refined_batch)
    
    # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡
    refined_pc = torch.cat(refined_batches, dim=1)
    return refined_pc

def inference_single_sample(model, inference_dataset, sample_idx, device, args, output_dir):
    """å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†"""
    
    data = inference_dataset[sample_idx]
    
    # è·å–æ•°æ®
    gt_normalized = data['gt_normalized'].to(device)  # (3, N)
    noisy_normalized = data['noisy_normalized'].to(device)  # (3, N)
    gt_original = data['gt_original']  # (N, 3)
    noisy_original = data['noisy_original']  # (N, 3)
    sample_id = data['sample_idx']
    num_points = data['num_points']
    
    print(f"\nå¤„ç†æ ·æœ¬ {sample_id}, ç‚¹æ•°: {num_points}")
    
    # æ¨ç†
    with torch.no_grad():
        if num_points > args.sample_points:
            print(f"å¤§è§„æ¨¡ç‚¹äº‘ï¼Œåˆ†æ‰¹å¤„ç†ï¼ˆæ‰¹å¤§å°: {args.sample_points}ï¼‰")
            refined_normalized = process_large_pointcloud(
                model, noisy_normalized, args.sample_points, device
            )
        else:
            # ç›´æ¥å¤„ç†
            refined_normalized = model(noisy_normalized.unsqueeze(0))[0]  # (3, N)
    
    # åå½’ä¸€åŒ–: (3, N) -> (N, 3)
    refined_normalized_np = refined_normalized.transpose(0, 1).cpu().numpy()
    refined_original = inference_dataset.denormalize_pointcloud(refined_normalized_np)
    
    # ä¿å­˜ç»“æœåˆ°CSV
    sample_dir = output_dir / f'sample_{sample_id:03d}'
    sample_dir.mkdir(exist_ok=True)
    
    pd.DataFrame(gt_original, columns=['x', 'y', 'z']).to_csv(
        sample_dir / 'gt.csv', index=False)
    pd.DataFrame(noisy_original, columns=['x', 'y', 'z']).to_csv(
        sample_dir / 'noisy.csv', index=False)
    pd.DataFrame(refined_original, columns=['x', 'y', 'z']).to_csv(
        sample_dir / 'refined.csv', index=False)
    
    print(f"æ ·æœ¬ {sample_id} æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {sample_dir}")
    
    return {
        'sample_id': sample_id,
        'num_points': num_points,
        'output_dir': sample_dir
    }

def main_inference():
    """ä¸»æ¨ç†å‡½æ•°"""
    parser = argparse.ArgumentParser(description='RefineNetç‚¹äº‘ä¿®æ­£æ¨ç†')
    
    # æ¨¡å‹å’Œæ•°æ®å‚æ•°
    parser.add_argument('--model-path', type=str, required=True,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--gt-h5-path', type=str, required=True,
                        help='GTç‚¹äº‘H5æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--noisy-h5-path', type=str, required=True,
                        help='å™ªå£°ç‚¹äº‘H5æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, required=True,
                        help='æ¨ç†ç»“æœè¾“å‡ºç›®å½•')
    
    # æ•°æ®æ ¼å¼å‚æ•°
    parser.add_argument('--gt-data-key', type=str, default='point_clouds',
                        help='GTæ•°æ®åœ¨H5æ–‡ä»¶ä¸­çš„é”®å')
    parser.add_argument('--noisy-data-key', type=str, default='point_clouds',
                        help='å™ªå£°æ•°æ®åœ¨H5æ–‡ä»¶ä¸­çš„é”®å')
    
    # å¤„ç†å‚æ•°
    parser.add_argument('--sample-points', type=int, default=8192,
                        help='æ‰¹å¤„ç†é‡‡æ ·ç‚¹æ•°')
    parser.add_argument('--train-ratio', type=float, default=0.8,
                        help='è®­ç»ƒæ•°æ®å æ¯”ï¼ˆç”¨äºæ•°æ®é›†åˆ†å‰²ï¼‰')
    parser.add_argument('--sample-idx', type=int, default=-1,
                        help='è¦å¤„ç†çš„æ ·æœ¬ç´¢å¼•ï¼Œ-1è¡¨ç¤ºå¤„ç†æ‰€æœ‰æ¨ç†æ ·æœ¬')
    
    # å½’ä¸€åŒ–å‚æ•°
    parser.add_argument('--volume-dims', type=int, nargs=3, default=[20000, 20000, 2500],
                        help='ä½“ç§¯ç»´åº¦ [x, y, z]')
    parser.add_argument('--padding', type=int, nargs=3, default=[0, 0, 100],
                        help='è¾¹ç•Œå¡«å…… [x, y, z]')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--init-var', type=float, default=0.2,
                        help='æ¨¡å‹å‚æ•°åˆå§‹åŒ–æ–¹å·®')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
    
    if not os.path.exists(args.gt_h5_path):
        raise FileNotFoundError(f"GT H5æ–‡ä»¶ä¸å­˜åœ¨: {args.gt_h5_path}")
    
    if not os.path.exists(args.noisy_h5_path):
        raise FileNotFoundError(f"å™ªå£°H5æ–‡ä»¶ä¸å­˜åœ¨: {args.noisy_h5_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'ä½¿ç”¨è®¾å¤‡: {device}')
    
    print("=== RefineNetæ¨ç†é…ç½® ===")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"GT H5æ–‡ä»¶: {args.gt_h5_path}")
    print(f"å™ªå£°H5æ–‡ä»¶: {args.noisy_h5_path}")
    print(f"æ‰¹å¤„ç†ç‚¹æ•°: {args.sample_points}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä½“ç§¯ç»´åº¦: {args.volume_dims}")
    print(f"è¾¹ç•Œå¡«å……: {args.padding}")
    print("=" * 30)
    
    try:
        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹...")
        model = load_model(Path(args.model_path), device, args)
        print(f'æ¨¡å‹å‚æ•°æ•°é‡: {util.n_params(model)}')
        
        # åˆ›å»ºæ¨ç†æ•°æ®é›†
        print("åˆ›å»ºæ¨ç†æ•°æ®é›†...")
        dummy_pc = torch.zeros((100, 3))
        inference_dataset = H5InferenceDataset(dummy_pc, device, args)
        
        print(f"æ¨ç†æ ·æœ¬æ•°: {len(inference_dataset)}")
        
        # æ‰§è¡Œæ¨ç†
        results = []
        if args.sample_idx >= 0:
            # å¤„ç†å•ä¸ªæ ·æœ¬
            if args.sample_idx >= len(inference_dataset):
                raise IndexError(f"æ ·æœ¬ç´¢å¼•è¶…å‡ºèŒƒå›´: {args.sample_idx} >= {len(inference_dataset)}")
            
            result = inference_single_sample(
                model, inference_dataset, args.sample_idx, device, args, output_dir
            )
            results.append(result)
        else:
            # å¤„ç†æ‰€æœ‰æ ·æœ¬
            for i in range(len(inference_dataset)):
                result = inference_single_sample(
                    model, inference_dataset, i, device, args, output_dir
                )
                results.append(result)
        
        # ä¿å­˜æ¨ç†æ±‡æ€»
        summary_df = pd.DataFrame(results)
        summary_path = output_dir / 'inference_summary.csv'
        summary_df.to_csv(summary_path, index=False)
        
        print(f"\nğŸ‰ æ¨ç†å®Œæˆï¼")
        print(f"å¤„ç†æ ·æœ¬æ•°: {len(results)}")
        print(f"ç»“æœä¿å­˜åˆ°: {output_dir}")
        print(f"æ¨ç†æ±‡æ€»: {summary_path}")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main_inference()
    sys.exit(exit_code)
