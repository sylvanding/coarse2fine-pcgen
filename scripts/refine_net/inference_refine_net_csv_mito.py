#!/usr/bin/env python3
"""
RefineNet CSVç‚¹äº‘æ¨ç†è„šæœ¬

ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„RefineNetæ¨¡å‹å¯¹CSVæ ¼å¼çš„ç‚¹äº‘è¿›è¡Œä¿®æ­£æ¨ç†ã€‚
è‡ªåŠ¨å¤„ç†ç‚¹äº‘å¹³ç§»ã€è¾¹ç•Œè®¡ç®—å’Œå½’ä¸€åŒ–å‚æ•°è®¾ç½®ã€‚

å½’ä¸€åŒ–æ–¹æ³•ï¼šé‡‡ç”¨ä¸H5PairedDatasetä¸€è‡´çš„å½’ä¸€åŒ–ç­–ç•¥ï¼Œ
- å°†ç‚¹äº‘åæ ‡å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´
- è€ƒè™‘volume_dimså’Œpaddingå‚æ•°
- Zè½´ä½¿ç”¨Yè½´çš„å°ºåº¦è¿›è¡Œå½’ä¸€åŒ–ï¼ˆä¿æŒè®­ç»ƒæ—¶çš„ä¸€è‡´æ€§ï¼‰
"""

import sys
import os
from pathlib import Path
import argparse
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥RefineNetæ¨¡å—
from src.refine_net import options
from src.refine_net.models import PointNet2Generator
import src.refine_net.util as util


class CSVPointCloudProcessor:
    """CSVç‚¹äº‘å¤„ç†å™¨"""
    
    def __init__(self, csv_path: str, volume_dims: np.ndarray, padding: np.ndarray):
        """
        åˆå§‹åŒ–CSVç‚¹äº‘å¤„ç†å™¨
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            volume_dims: ä½“ç§¯ç»´åº¦ [x, y, z]
            padding: è¾¹ç•Œå¡«å…… [x, y, z]
        """
        self.csv_path = Path(csv_path)
        self.original_pc = None
        self.translated_pc = None
        self.translation_offset = None
        self.volume_dims = volume_dims
        self.padding = padding
        
    def load_and_process(self):
        """åŠ è½½å¹¶å¤„ç†CSVç‚¹äº‘æ•°æ®"""
        print(f"åŠ è½½CSVç‚¹äº‘: {self.csv_path}")
        
        # è¯»å–CSVæ–‡ä»¶
        if not self.csv_path.exists():
            raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {self.csv_path}")
        
        df = pd.read_csv(self.csv_path)
        
        # æ£€æŸ¥åˆ—å
        if not all(col in df.columns for col in ['x [nm]', 'y [nm]', 'z [nm]']):
            raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'x [nm]', 'y [nm]', 'z [nm]' åˆ—")
        
        # æå–ç‚¹äº‘åæ ‡
        self.original_pc = df[['x [nm]', 'y [nm]', 'z [nm]']].values.astype(np.float32)
        print(f"ç‚¹äº‘å½¢çŠ¶: {self.original_pc.shape}")
        
        # è®¡ç®—è¾¹ç•Œ
        min_coords = np.min(self.original_pc, axis=0)
        max_coords = np.max(self.original_pc, axis=0)
        
        print(f"åŸå§‹è¾¹ç•Œ:")
        print(f"  X: [{min_coords[0]:.2f}, {max_coords[0]:.2f}]")
        print(f"  Y: [{min_coords[1]:.2f}, {max_coords[1]:.2f}]")
        print(f"  Z: [{min_coords[2]:.2f}, {max_coords[2]:.2f}]")
        
        # è®¡ç®—å¹³ç§»åç§»é‡ï¼Œç¡®ä¿æ‰€æœ‰ç‚¹éƒ½å¤§äºç­‰äº0
        self.translation_offset = -min_coords
        
        # åº”ç”¨å¹³ç§»
        self.translated_pc = self.original_pc + self.translation_offset
        
        # é‡æ–°è®¡ç®—å¹³ç§»åçš„è¾¹ç•Œ
        translated_min = np.min(self.translated_pc, axis=0)
        translated_max = np.max(self.translated_pc, axis=0)
        
        print(f"å¹³ç§»åç§»é‡: [{self.translation_offset[0]:.2f}, {self.translation_offset[1]:.2f}, {self.translation_offset[2]:.2f}]")
        print(f"å¹³ç§»åè¾¹ç•Œ:")
        print(f"  X: [{translated_min[0]:.2f}, {translated_max[0]:.2f}]")
        print(f"  Y: [{translated_min[1]:.2f}, {translated_max[1]:.2f}]")
        print(f"  Z: [{translated_min[2]:.2f}, {translated_max[2]:.2f}]")
        
        # # è®¾ç½®volume_dimsä¸ºæœ€å¤§å€¼ï¼ˆå‘ä¸Šå–æ•´åˆ°æ•´æ•°ï¼‰
        # self.volume_dims = np.ceil(translated_max).astype(int) + self.padding
        # print(f"è®¡ç®—å¾—åˆ°çš„volume_dims: [{self.volume_dims[0]}, {self.volume_dims[1]}, {self.volume_dims[2]}]")
        
        return self.translated_pc, self.volume_dims
    
    def save_results(self, refined_pc: np.ndarray, output_dir: Path):
        """
        ä¿å­˜æ¨ç†ç»“æœ
        
        Args:
            refined_pc: ä¿®æ­£åçš„ç‚¹äº‘ (N, 3)
            output_dir: è¾“å‡ºç›®å½•
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # å°†ä¿®æ­£åçš„ç‚¹äº‘å¹³ç§»å›åŸå§‹ä½ç½®
        refined_original = refined_pc - self.translation_offset
        
        # ä¿å­˜å„ç§ç‰ˆæœ¬çš„ç‚¹äº‘
        pd.DataFrame(self.original_pc, columns=['x [nm]', 'y [nm]', 'z [nm]']).to_csv(
            output_dir / 'original.csv', index=False)
        
        # pd.DataFrame(self.translated_pc, columns=['x', 'y', 'z']).to_csv(
        #     output_dir / 'translated.csv', index=False)
        
        # pd.DataFrame(refined_pc, columns=['x', 'y', 'z']).to_csv(
        #     output_dir / 'refined_translated.csv', index=False)
        
        pd.DataFrame(refined_original, columns=['x [nm]', 'y [nm]', 'z [nm]']).to_csv(
            output_dir / 'refined_original.csv', index=False)
        
        # ä¿å­˜å¤„ç†ä¿¡æ¯
        info = {
            'original_shape': self.original_pc.shape,
            'translation_offset': self.translation_offset.tolist(),
            'volume_dims': self.volume_dims.tolist(),
            'original_bounds_min': np.min(self.original_pc, axis=0).tolist(),
            'original_bounds_max': np.max(self.original_pc, axis=0).tolist(),
            'translated_bounds_min': np.min(self.translated_pc, axis=0).tolist(),
            'translated_bounds_max': np.max(self.translated_pc, axis=0).tolist(),
        }
        
        pd.DataFrame([info]).to_csv(output_dir / 'processing_info.csv', index=False)
        
        print(f"ç»“æœä¿å­˜åˆ°: {output_dir}")
        print(f"  - original.csv: åŸå§‹ç‚¹äº‘")
        # print(f"  - translated.csv: å¹³ç§»åçš„ç‚¹äº‘")
        # print(f"  - refined_translated.csv: ä¿®æ­£åçš„ç‚¹äº‘ï¼ˆå¹³ç§»åæ ‡ç³»ï¼‰")
        print(f"  - refined_original.csv: ä¿®æ­£åçš„ç‚¹äº‘ï¼ˆåŸå§‹åæ ‡ç³»ï¼‰")
        print(f"  - processing_info.csv: å¤„ç†ä¿¡æ¯")


def load_model(model_path, device, args):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = PointNet2Generator(device, args)
    
    # åŠ è½½å®Œæ•´æ£€æŸ¥ç‚¹
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    model.to(device)
    model.eval()
    return model


def normalize_pointcloud(pc: np.ndarray, volume_dims: np.ndarray, padding: np.ndarray = None) -> tuple[np.ndarray, float]:
    """
    å½’ä¸€åŒ–ç‚¹äº‘åˆ° [-1, 1] èŒƒå›´
    å‚è€ƒH5PairedDatasetçš„å½’ä¸€åŒ–æ–¹æ³•
    
    Args:
        pc: ç‚¹äº‘æ•°æ® (N, 3)
        volume_dims: ä½“ç§¯ç»´åº¦ [x, y, z]
        padding: è¾¹ç•Œå¡«å…… [x, y, z]ï¼Œé»˜è®¤ä¸º0
        
    Returns:
        å½’ä¸€åŒ–åçš„ç‚¹äº‘ (N, 3), zè½´å¹³å‡å€¼
    """
    if padding is None:
        padding = np.zeros(3)
    
    # è®¡ç®—å®é™…ç©ºé—´èŒƒå›´: volume_dims Â± padding
    # x, y, z åæ ‡èŒƒå›´åˆ†åˆ«ä¸º [-padding[i], volume_dims[i] + padding[i]]
    x_min, x_max = -padding[0], volume_dims[0] + padding[0]
    y_min, y_max = -padding[1], volume_dims[1] + padding[1]
    z_min, z_max = -padding[2], volume_dims[2] + padding[2]
    
    # å°†ç‚¹äº‘åæ ‡ä»åŸå§‹èŒƒå›´å½’ä¸€åŒ–åˆ° [-1, 1]
    normalized = pc.copy()
    normalized[:, 0] = (normalized[:, 0] - x_min) / (x_max - x_min) * 2 - 1  # [x_min, x_max] -> [-1, 1]
    normalized[:, 1] = (normalized[:, 1] - y_min) / (y_max - y_min) * 2 - 1  # [y_min, y_max] -> [-1, 1]
    # ç‰¹æ®Šå¤„ç†ï¼šZè½´ä½¿ç”¨Yè½´çš„å°ºåº¦ï¼ˆä¸H5PairedDatasetä¿æŒä¸€è‡´ï¼‰
    normalized[:, 2] = (normalized[:, 2] - z_min) / (y_max - y_min) * 2 - 1  # [z_min, z_max] -> [-1, 1]ï¼Œä½†ä½¿ç”¨Yè½´å°ºåº¦
    
    # # ç¡®ä¿zè½´å¹³å‡å€¼ä¸º0
    # z_mean = np.mean(normalized[:, 2])
    # normalized[:, 2] = normalized[:, 2] - z_mean
    
    # return normalized.astype(np.float32), z_mean
    return normalized.astype(np.float32), np.zeros(3)


def denormalize_pointcloud(normalized_pc: np.ndarray, volume_dims: np.ndarray, padding: np.ndarray = None, z_mean: float = 0) -> np.ndarray:
    """
    åå½’ä¸€åŒ–ç‚¹äº‘ä» [-1, 1] èŒƒå›´
    å‚è€ƒH5PairedDatasetçš„åå½’ä¸€åŒ–æ–¹æ³•
    
    Args:
        normalized_pc: å½’ä¸€åŒ–çš„ç‚¹äº‘ (N, 3)
        volume_dims: ä½“ç§¯ç»´åº¦ [x, y, z]
        padding: è¾¹ç•Œå¡«å…… [x, y, z]ï¼Œé»˜è®¤ä¸º0
        z_mean: zè½´å¹³å‡å€¼
    Returns:
        åå½’ä¸€åŒ–åçš„ç‚¹äº‘ (N, 3)
    """
    if padding is None:
        padding = np.zeros(3)
    
    # è®¡ç®—å®é™…ç©ºé—´èŒƒå›´: volume_dims Â± padding
    # x, y, z åæ ‡èŒƒå›´åˆ†åˆ«ä¸º [-padding[i], volume_dims[i] + padding[i]]
    x_min, x_max = -padding[0], volume_dims[0] + padding[0]
    y_min, y_max = -padding[1], volume_dims[1] + padding[1]
    z_min, z_max = -padding[2], volume_dims[2] + padding[2]
    
    # # æŠŠzè½´ç§»å›ä¸­å¿ƒ
    # normalized_pc[:, 2] = normalized_pc[:, 2] + z_mean
    
    # ä» [-1, 1] åå½’ä¸€åŒ–åˆ°åŸå§‹åæ ‡èŒƒå›´
    denormalized = normalized_pc.copy()
    denormalized[:, 0] = (denormalized[:, 0] + 1) / 2 * (x_max - x_min) + x_min  # [-1, 1] -> [x_min, x_max]
    denormalized[:, 1] = (denormalized[:, 1] + 1) / 2 * (y_max - y_min) + y_min  # [-1, 1] -> [y_min, y_max]
    # ç‰¹æ®Šå¤„ç†ï¼šZè½´ä½¿ç”¨Yè½´çš„å°ºåº¦ï¼ˆä¸H5PairedDatasetä¿æŒä¸€è‡´ï¼‰
    denormalized[:, 2] = (denormalized[:, 2] + 1) / 2 * (y_max - y_min) + z_min  # [-1, 1] -> [z_min, z_max]ï¼Œä½†ä½¿ç”¨Yè½´å°ºåº¦
    
    return denormalized


def process_large_pointcloud(model, noisy_pc, batch_size, device):
    """
    åˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡ç‚¹äº‘
    
    Args:
        model: æ¨¡å‹
        noisy_pc: å™ªå£°ç‚¹äº‘ (3, N)
        batch_size: æ‰¹å¤„ç†å¤§å°
        device: è®¾å¤‡
        
    Returns:
        torch.Tensor: ä¿®æ­£åçš„ç‚¹äº‘ (3, N)
    """
    num_points = noisy_pc.shape[1]
    refined_batches = []
    
    for start_idx in tqdm(range(0, num_points, batch_size), desc="å¤„ç†æ‰¹æ¬¡"):
        end_idx = min(start_idx + batch_size, num_points)
        
        # è·å–æ‰¹æ¬¡
        batch = noisy_pc[:, start_idx:end_idx]  # (3, batch_size)
        
        # å¦‚æœæ‰¹æ¬¡å¤ªå°ï¼Œè¿›è¡Œpadding
        if batch.shape[1] < batch_size:
            padding_size = batch_size - batch.shape[1]
            # é‡å¤æœ€åå‡ ä¸ªç‚¹è¿›è¡Œpadding
            repeat_indices = torch.randint(0, batch.shape[1], (padding_size,), device=device)
            padding = batch[:, repeat_indices]
            batch = torch.cat([batch, padding], dim=1)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            refined_batch = model(batch.unsqueeze(0))[0]  # (3, batch_size)
        
        # ç§»é™¤padding
        if end_idx - start_idx < batch_size:
            refined_batch = refined_batch[:, :end_idx - start_idx]
        
        refined_batches.append(refined_batch)
    
    # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡
    refined_pc = torch.cat(refined_batches, dim=1)
    return refined_pc


def main_inference():
    """ä¸»æ¨ç†å‡½æ•°"""
    parser = options.get_parser('RefineNet CSVç‚¹äº‘ä¿®æ­£æ¨ç†')
    
    # è¾“å…¥è¾“å‡ºå‚æ•°
    parser.add_argument('--model-path', type=str, default='/repos/datasets/exp-data-4pi-pc-mitochondria/3d_diffusion/iteration_580000.pt',
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--csv-path', type=str, default='/repos/datasets/exp-data-4pi-pc-mitochondria/3d_diffusion/generated_sample_03_points.csv',
                        help='è¾“å…¥CSVç‚¹äº‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default='/repos/datasets/exp-data-4pi-pc-mitochondria/3d_diffusion/generated_sample_03_points_refined',
                        help='æ¨ç†ç»“æœè¾“å‡ºç›®å½•')
    # `options` has defined these parameters
    # parser.add_argument('--sample-points', type=int, default=80000,
    #                     help='æ‰¹å¤„ç†é‡‡æ ·ç‚¹æ•°')
    # parser.add_argument('--volume-dims', type=int, nargs=3, default=[8000, 8000, 300],
    #                     help='ä½“ç§¯ç»´åº¦ [x, y, z]')
    # parser.add_argument('--padding', type=int, nargs=3, default=[0, 0, 0],
    #                     help='è¾¹ç•Œå¡«å…… [x, y, z]')
    
    args = parser.parse_args()
    
    args.volume_dims = [8000, 8000, 1200]
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
    
    if not os.path.exists(args.csv_path):
        raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {args.csv_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'ä½¿ç”¨è®¾å¤‡: {device}')
    
    print("=== RefineNet CSVæ¨ç†é…ç½® ===")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"CSVæ–‡ä»¶: {args.csv_path}")
    print(f"ä½“ç§¯ç»´åº¦: {args.volume_dims}")
    print(f"è¾¹ç•Œå¡«å……: {args.padding}")
    print(f"æ‰¹å¤„ç†ç‚¹æ•°: {args.sample_points}")
    print(f"ä½“ç§¯ç»´åº¦: {args.volume_dims}")
    print(f"è¾¹ç•Œå¡«å……: {args.padding}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 30)
    
    try:
        # å¤„ç†CSVç‚¹äº‘
        print("\nå¤„ç†CSVç‚¹äº‘...")
        processor = CSVPointCloudProcessor(args.csv_path, np.array(args.volume_dims), np.array(args.padding))
        translated_pc, volume_dims = processor.load_and_process()
        
        # åŠ¨æ€è®¾ç½®å½’ä¸€åŒ–å‚æ•°
        args.volume_dims = volume_dims.tolist()
        
        print(f"\nè‡ªåŠ¨è®¾ç½®çš„å‚æ•°:")
        print(f"  volume_dims: {args.volume_dims}")
        print(f"  padding: {args.padding}")
        print(f"  å½’ä¸€åŒ–æ–¹æ³•: ä¸H5PairedDatasetä¸€è‡´ï¼ˆZè½´ä½¿ç”¨Yè½´å°ºåº¦ï¼‰")
        
        # åŠ è½½æ¨¡å‹
        print("\nåŠ è½½æ¨¡å‹...")
        model = load_model(Path(args.model_path), device, args)
        print(f'æ¨¡å‹å‚æ•°æ•°é‡: {util.n_params(model)}')
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        print("\nå‡†å¤‡è¾“å…¥æ•°æ®...")
        num_points = len(translated_pc)
        print(f"ç‚¹äº‘ç‚¹æ•°: {num_points}")
        
        # å½’ä¸€åŒ–ç‚¹äº‘
        normalized_pc, z_mean = normalize_pointcloud(translated_pc, np.array(args.volume_dims), np.array(args.padding))
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼: (N, 3) -> (3, N)
        normalized_tensor = torch.from_numpy(normalized_pc.T).to(device)  # (3, N)
        
        print(f"å½’ä¸€åŒ–åç‚¹äº‘èŒƒå›´:")
        print(f"  X: [{normalized_pc[:, 0].min():.4f}, {normalized_pc[:, 0].max():.4f}]")
        print(f"  Y: [{normalized_pc[:, 1].min():.4f}, {normalized_pc[:, 1].max():.4f}]")
        print(f"  Z: [{normalized_pc[:, 2].min():.4f}, {normalized_pc[:, 2].max():.4f}]")
        
        # æ¨ç†
        print("\næ‰§è¡Œæ¨ç†...")
        with torch.no_grad():
            if num_points > args.sample_points:
                print(f"å¤§è§„æ¨¡ç‚¹äº‘ï¼Œåˆ†æ‰¹å¤„ç†ï¼ˆæ‰¹å¤§å°: {args.sample_points}ï¼‰")
                refined_normalized = process_large_pointcloud(
                    model, normalized_tensor, args.sample_points, device
                )
            else:
                # ç›´æ¥å¤„ç†
                refined_normalized = model(normalized_tensor.unsqueeze(0))[0]  # (3, N)
        
        # åå½’ä¸€åŒ–: (3, N) -> (N, 3)
        refined_normalized_np = refined_normalized.transpose(0, 1).cpu().numpy()
        refined_translated = denormalize_pointcloud(
            refined_normalized_np, np.array(args.volume_dims), np.array(args.padding), z_mean
        )
        
        print(f"åå½’ä¸€åŒ–åç‚¹äº‘èŒƒå›´:")
        print(f"  X: [{refined_translated[:, 0].min():.2f}, {refined_translated[:, 0].max():.2f}]")
        print(f"  Y: [{refined_translated[:, 1].min():.2f}, {refined_translated[:, 1].max():.2f}]")
        print(f"  Z: [{refined_translated[:, 2].min():.2f}, {refined_translated[:, 2].max():.2f}]")
        
        # ä¿å­˜ç»“æœ
        print("\nä¿å­˜ç»“æœ...")
        processor.save_results(refined_translated, output_dir)
        
        print(f"\nğŸ‰ æ¨ç†å®Œæˆï¼")
        print(f"åŸå§‹ç‚¹äº‘: {args.csv_path}")
        print(f"å¤„ç†ç‚¹æ•°: {num_points}")
        print(f"ç»“æœä¿å­˜åˆ°: {output_dir}")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main_inference()
    sys.exit(exit_code)
