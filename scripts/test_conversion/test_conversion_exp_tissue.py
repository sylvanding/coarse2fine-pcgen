#!/usr/bin/env python3
"""
ç‚¹äº‘åˆ°ä½“ç´ å’Œä½“ç´ åˆ°ç‚¹äº‘è½¬æ¢æµ‹è¯•è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•ç‚¹äº‘åˆ°ä½“ç´ å’Œä½“ç´ åˆ°ç‚¹äº‘çš„è½¬æ¢åŠŸèƒ½ï¼ŒåŠ è½½H5æ–‡ä»¶ä¸­çš„ç¬¬ä¸€ä¸ªç‚¹äº‘æ ·æœ¬ï¼Œ
å°†å…¶è½¬æ¢ä¸º3Dä½“ç´ ç½‘æ ¼ï¼Œå¹¶ä¿å­˜ä¸ºTIFFæ–‡ä»¶ä»¥ä¾¿æŸ¥çœ‹è½¬æ¢æ•ˆæœã€‚éšåï¼Œå°†ä½“ç´ ç½‘æ ¼é‡‡æ ·å›ç‚¹äº‘ï¼Œ
å¹¶ä¿å­˜ä¸ºæ–°çš„ç‚¹äº‘æ–‡ä»¶ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/test_conversion.py --input data/sample.h5 --output test_voxel.tiff
"""

import argparse
import sys
import os
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = Path(__file__).parent
project_root = script_dir.parent
sys.path.insert(0, str(project_root))

from src.data.h5_loader import PointCloudH5Loader
from src.voxel.converter import PointCloudToVoxel

import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°
    """
    parser = argparse.ArgumentParser(
        description='æµ‹è¯•ç‚¹äº‘åˆ°ä½“ç´ çš„è½¬æ¢åŠŸèƒ½',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
    # åŸºæœ¬è½¬æ¢ï¼ˆå æœ‰ç½‘æ ¼ï¼‰
    python scripts/test_conversion.py --input data/sample.h5 --output test_occupancy.tiff
    
    # å¯†åº¦ç½‘æ ¼è½¬æ¢
    python scripts/test_conversion.py --input data/sample.h5 --output test_density.tiff --method density --voxel-size 128
    
    # é«˜æ–¯å¯†åº¦ç½‘æ ¼è½¬æ¢
    python scripts/test_conversion.py --input data/sample.h5 --output test_gaussian.tiff --method gaussian --sigma 1.5
    
    # ä½“ç´ ç½‘æ ¼ä¸Šé‡‡æ ·
    python scripts/test_conversion.py --input data/sample.h5 --output test_upsampled.tiff --upsample --upsample-factor 2.0 --upsample-method linear
    
    # ä½“ç´ é‡‡æ ·å›ç‚¹äº‘ï¼ˆæ–°çš„æ¦‚ç‡åˆ†å¸ƒæ–¹æ³•ï¼‰
    python scripts/test_conversion.py --input data/sample.h5 --output test_sampled.tiff --sample-back --sample-num-points 100000 --sample-method probabilistic
    
    # å®Œæ•´æµç¨‹ï¼šè½¬æ¢â†’ä¸Šé‡‡æ ·â†’é‡‡æ ·å›ç‚¹äº‘ï¼ˆæ¨èä½¿ç”¨æ¦‚ç‡é‡‡æ ·ï¼‰
    python scripts/test_conversion.py --input data/sample.h5 --output test_full.tiff --method gaussian --sigma 1.5 --upsample --upsample-factor 2.0 --sample-back --sample-num-points 200000 --sample-method probabilistic
    
    # è‡ªå®šä¹‰ä½“ç§¯å‚æ•°è½¬æ¢
    python scripts/test_conversion.py --input data/sample.h5 --output test_custom.tiff --volume-dims 15000 15000 3000 --padding 50 50 150
    
    # ä¿å­˜åŸå§‹ç‚¹äº‘
    python scripts/test_conversion.py --input data/sample.h5 --output test_voxel.tiff --save-original --original-output original_pointcloud.csv
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        default='/repos/datasets/tissue-datasets/pointclouds-clean-check.h5',
        type=str,
        help='è¾“å…¥H5æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--output', '-o',
        default='/repos/datasets/tissue-datasets/superresolution_results/test_voxel.tiff',
        type=str,
        help='è¾“å‡ºTIFFæ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--save-original',
        default=True,
        action='store_true',
        help='æ˜¯å¦ä¿å­˜åŸå§‹ç‚¹äº‘ä¸ºCSVæ–‡ä»¶'
    )
    
    parser.add_argument(
        '--original-output',
        type=str,
        default=None,
        help='åŸå§‹ç‚¹äº‘è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤åŸºäº--outputå‚æ•°ç”Ÿæˆï¼‰'
    )
    
    parser.add_argument(
        '--index',
        type=int,
        default=0,
        help='è¦è½¬æ¢çš„ç‚¹äº‘æ ·æœ¬ç´¢å¼• (é»˜è®¤: 0)'
    )
    
    parser.add_argument(
        '--method',
        type=str,
        choices=['occupancy', 'density', 'gaussian'],
        default='gaussian',
        help='ä½“ç´ åŒ–æ–¹æ³• (é»˜è®¤: occupancy)'
    )
    
    parser.add_argument(
        '--voxel-size',
        type=int,
        default=128,
        help='ä½“ç´ ç½‘æ ¼åˆ†è¾¨ç‡ (é»˜è®¤: 64)'
    )
    
    parser.add_argument(
        '--sigma',
        type=float,
        default=1.0,
        help='é«˜æ–¯æ–¹æ³•çš„æ ‡å‡†å·® (é»˜è®¤: 1.5)'
    )
    
    parser.add_argument(
        '--data-key',
        type=str,
        default='pointclouds',
        help='H5æ–‡ä»¶ä¸­æ•°æ®çš„é”®å (é»˜è®¤: data)'
    )
    
    parser.add_argument(
        '--padding-ratio',
        type=float,
        default=0.00,
        help='è¾¹ç•Œæ‰©å±•æ¯”ä¾‹ (é»˜è®¤: 0.00)'
    )
    
    parser.add_argument(
        '--volume-dims',
        type=float,
        nargs=3,
        default=[28000.0, 28000.0, 1600.0],
        help='ä½“ç§¯å°ºå¯¸ [x, y, z] (å•ä½: nm) (é»˜è®¤: [20000, 20000, 2500])'
    )
    
    parser.add_argument(
        '--padding',
        type=float,
        nargs=3,
        default=[0, 0, 0],
        help='ä½“ç§¯è¾¹ç•Œå¡«å…… [x, y, z] (å•ä½: nm) (é»˜è®¤: [0, 0, 100])'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        default=True,
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯'
    )
    
    # ä½“ç´ é‡‡æ ·å›ç‚¹äº‘å‚æ•°
    parser.add_argument(
        '--sample-back',
        default=True,
        action='store_true',
        help='æ˜¯å¦å°†ä½“ç´ ç½‘æ ¼é‡‡æ ·å›ç‚¹äº‘'
    )
    
    parser.add_argument(
        '--sample-num-points',
        type=int,
        default=int(1e5),
        help='é‡‡æ ·å›ç‚¹äº‘çš„ç›®æ ‡ç‚¹æ•°ï¼ŒNoneæ—¶è¿”å›æ‰€æœ‰è¶…è¿‡é˜ˆå€¼çš„ä½“ç´ '
    )
    
    parser.add_argument(
        '--sample-threshold',
        type=float,
        default=0.0,
        help='ä½“ç´ å€¼é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„ä½“ç´ è¢«è§†ä¸ºåŒ…å«ç‚¹ (é»˜è®¤: 0.0)'
    )
    
    parser.add_argument(
        '--sample-method',
        type=str,
        choices=['probabilistic', 'center', 'random', 'weighted'],
        default='probabilistic',
        help='é‡‡æ ·æ–¹æ³• (é»˜è®¤: probabilistic - æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·ï¼Œé¿å…ç½‘æ ¼åŒ–æ•ˆåº”)'
    )
    
    # ä½“ç´ ä¸Šé‡‡æ ·å‚æ•°
    parser.add_argument(
        '--upsample',
        default=False,
        action='store_true',
        help='æ˜¯å¦å¯¹ä½“ç´ ç½‘æ ¼è¿›è¡Œä¸Šé‡‡æ ·'
    )
    
    parser.add_argument(
        '--upsample-factor',
        type=float,
        default=2.0,
        help='ä¸Šé‡‡æ ·å€æ•° (é»˜è®¤: 2.0)'
    )
    
    parser.add_argument(
        '--upsample-method',
        type=str,
        choices=['linear', 'nearest', 'cubic'],
        default='linear',
        help='ä¸Šé‡‡æ ·æ’å€¼æ–¹æ³• (é»˜è®¤: linear)'
    )
    
    return parser.parse_args()


def validate_inputs(args):
    """
    éªŒè¯è¾“å…¥å‚æ•°
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    
    Raises:
        FileNotFoundError: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: å‚æ•°å€¼æ— æ•ˆ
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        logger.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        os.makedirs(output_dir, exist_ok=True)
    
    # éªŒè¯å‚æ•°èŒƒå›´
    if args.voxel_size <= 0:
        raise ValueError("ä½“ç´ å¤§å°å¿…é¡»å¤§äº0")
    
    if args.index < 0:
        raise ValueError("æ ·æœ¬ç´¢å¼•å¿…é¡»éè´Ÿ")
    
    if not 0 <= args.padding_ratio <= 1:
        raise ValueError("padding_ratioå¿…é¡»åœ¨[0, 1]èŒƒå›´å†…")
    
    if args.sigma <= 0:
        raise ValueError("sigmaå¿…é¡»å¤§äº0")
    
    # éªŒè¯é‡‡æ ·å‚æ•°
    if args.sample_back:
        if args.sample_threshold < 0 or args.sample_threshold > 1:
            raise ValueError("sample_thresholdå¿…é¡»åœ¨[0, 1]èŒƒå›´å†…")
        if args.sample_num_points is not None and args.sample_num_points <= 0:
            raise ValueError("sample_num_pointså¿…é¡»å¤§äº0")
    
    # éªŒè¯ä¸Šé‡‡æ ·å‚æ•°
    if args.upsample:
        if args.upsample_factor <= 1.0:
            raise ValueError("upsample_factorå¿…é¡»å¤§äº1.0")


def analyze_point_cloud(point_cloud: np.ndarray) -> dict:
    """
    åˆ†æç‚¹äº‘çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        point_cloud (np.ndarray): ç‚¹äº‘æ•°æ®
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    stats = {
        'num_points': len(point_cloud),
        'min_coords': np.min(point_cloud, axis=0),
        'max_coords': np.max(point_cloud, axis=0),
        'mean_coords': np.mean(point_cloud, axis=0),
        'std_coords': np.std(point_cloud, axis=0),
        'range_coords': np.max(point_cloud, axis=0) - np.min(point_cloud, axis=0)
    }
    
    return stats


def analyze_voxel_grid(voxel_grid: np.ndarray) -> dict:
    """
    åˆ†æä½“ç´ ç½‘æ ¼çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        voxel_grid (np.ndarray): ä½“ç´ ç½‘æ ¼
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    stats = {
        'shape': voxel_grid.shape,
        'total_voxels': voxel_grid.size,
        'occupied_voxels': np.sum(voxel_grid > 0),
        'occupancy_ratio': np.sum(voxel_grid > 0) / voxel_grid.size,
        'min_value': np.min(voxel_grid),
        'max_value': np.max(voxel_grid),
        'mean_value': np.mean(voxel_grid),
        'std_value': np.std(voxel_grid)
    }
    
    return stats


def compare_point_clouds(original_stats: dict, sampled_stats: dict) -> dict:
    """
    æ¯”è¾ƒåŸå§‹ç‚¹äº‘å’Œé‡‡æ ·ç‚¹äº‘çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        original_stats (dict): åŸå§‹ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯
        sampled_stats (dict): é‡‡æ ·ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        dict: æ¯”è¾ƒç»“æœ
    """
    comparison = {
        'point_count_ratio': sampled_stats['num_points'] / original_stats['num_points'],
        'range_diff': sampled_stats['range_coords'] - original_stats['range_coords'],
        'mean_diff': sampled_stats['mean_coords'] - original_stats['mean_coords'],
        'std_diff': sampled_stats['std_coords'] - original_stats['std_coords']
    }
    
    return comparison


def print_statistics(point_stats: dict, voxel_stats: dict, 
                    sampled_stats: dict = None, comparison: dict = None,
                    upsampled_stats: dict = None):
    """
    æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        point_stats (dict): åŸå§‹ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯
        voxel_stats (dict): ä½“ç´ ç»Ÿè®¡ä¿¡æ¯
        sampled_stats (dict, optional): é‡‡æ ·ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯
        comparison (dict, optional): ç‚¹äº‘æ¯”è¾ƒç»“æœ
        upsampled_stats (dict, optional): ä¸Šé‡‡æ ·ä½“ç´ ç»Ÿè®¡ä¿¡æ¯
    """
    print("\n" + "="*60)
    print("åŸå§‹ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯:")
    print("="*60)
    print(f"ç‚¹æ•°: {point_stats['num_points']:,}")
    print("åæ ‡èŒƒå›´:")
    print(f"  X: [{point_stats['min_coords'][0]:.3f}, {point_stats['max_coords'][0]:.3f}] (èŒƒå›´: {point_stats['range_coords'][0]:.3f})")
    print(f"  Y: [{point_stats['min_coords'][1]:.3f}, {point_stats['max_coords'][1]:.3f}] (èŒƒå›´: {point_stats['range_coords'][1]:.3f})")
    print(f"  Z: [{point_stats['min_coords'][2]:.3f}, {point_stats['max_coords'][2]:.3f}] (èŒƒå›´: {point_stats['range_coords'][2]:.3f})")
    print(f"å¹³å‡åæ ‡: ({point_stats['mean_coords'][0]:.3f}, {point_stats['mean_coords'][1]:.3f}, {point_stats['mean_coords'][2]:.3f})")
    
    print("\n" + "="*60)
    print("ä½“ç´ ç½‘æ ¼ç»Ÿè®¡ä¿¡æ¯:")
    print("="*60)
    print(f"ç½‘æ ¼shape: {voxel_stats['shape']}")
    print(f"æ€»ä½“ç´ æ•°: {voxel_stats['total_voxels']:,}")
    print(f"å æœ‰ä½“ç´ æ•°: {voxel_stats['occupied_voxels']:,}")
    print(f"å æœ‰ç‡: {voxel_stats['occupancy_ratio']:.4f}")
    print(f"ä½“ç´ å€¼èŒƒå›´: [{voxel_stats['min_value']:.3f}, {voxel_stats['max_value']:.3f}]")
    print(f"å¹³å‡ä½“ç´ å€¼: {voxel_stats['mean_value']:.3f}")
    print(f"ä½“ç´ å€¼æ ‡å‡†å·®: {voxel_stats['std_value']:.3f}")
    
    if upsampled_stats:
        print("\n" + "="*60)
        print("ä¸Šé‡‡æ ·ä½“ç´ ç½‘æ ¼ç»Ÿè®¡ä¿¡æ¯:")
        print("="*60)
        print(f"ç½‘æ ¼shape: {upsampled_stats['shape']}")
        print(f"æ€»ä½“ç´ æ•°: {upsampled_stats['total_voxels']:,}")
        print(f"å æœ‰ä½“ç´ æ•°: {upsampled_stats['occupied_voxels']:,}")
        print(f"å æœ‰ç‡: {upsampled_stats['occupancy_ratio']:.4f}")
        print(f"ä½“ç´ å€¼èŒƒå›´: [{upsampled_stats['min_value']:.3f}, {upsampled_stats['max_value']:.3f}]")
        print(f"å¹³å‡ä½“ç´ å€¼: {upsampled_stats['mean_value']:.3f}")
        print(f"ä½“ç´ å€¼æ ‡å‡†å·®: {upsampled_stats['std_value']:.3f}")
    
    if sampled_stats:
        print("\n" + "="*60)
        print("é‡‡æ ·ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯:")
        print("="*60)
        print(f"ç‚¹æ•°: {sampled_stats['num_points']:,}")
        print("åæ ‡èŒƒå›´:")
        print(f"  X: [{sampled_stats['min_coords'][0]:.3f}, {sampled_stats['max_coords'][0]:.3f}] (èŒƒå›´: {sampled_stats['range_coords'][0]:.3f})")
        print(f"  Y: [{sampled_stats['min_coords'][1]:.3f}, {sampled_stats['max_coords'][1]:.3f}] (èŒƒå›´: {sampled_stats['range_coords'][1]:.3f})")
        print(f"  Z: [{sampled_stats['min_coords'][2]:.3f}, {sampled_stats['max_coords'][2]:.3f}] (èŒƒå›´: {sampled_stats['range_coords'][2]:.3f})")
        print(f"å¹³å‡åæ ‡: ({sampled_stats['mean_coords'][0]:.3f}, {sampled_stats['mean_coords'][1]:.3f}, {sampled_stats['mean_coords'][2]:.3f})")
    
    if comparison:
        print("\n" + "="*60)
        print("ç‚¹äº‘å¯¹æ¯”åˆ†æ:")
        print("="*60)
        print(f"ç‚¹æ•°æ¯”ä¾‹: {comparison['point_count_ratio']:.4f}")
        print("åæ ‡èŒƒå›´å˜åŒ–:")
        print(f"  Î”X: {comparison['range_diff'][0]:.3f}")
        print(f"  Î”Y: {comparison['range_diff'][1]:.3f}")
        print(f"  Î”Z: {comparison['range_diff'][2]:.3f}")
        print("å¹³å‡åæ ‡åç§»:")
        print(f"  Î”X: {comparison['mean_diff'][0]:.3f}")
        print(f"  Î”Y: {comparison['mean_diff'][1]:.3f}")
        print(f"  Î”Z: {comparison['mean_diff'][2]:.3f}")
    
    print("="*60)


def test_conversion_pipeline(args):
    """
    æ‰§è¡Œå®Œæ•´çš„è½¬æ¢æµ‹è¯•æµç¨‹
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    try:
        # 1. åŠ è½½H5æ•°æ®
        logger.info("æ­£åœ¨åŠ è½½H5æ•°æ®æ–‡ä»¶...")
        loader = PointCloudH5Loader(args.input, data_key=args.data_key)
        
        # æ£€æŸ¥æ ·æœ¬ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
        if args.index >= loader.num_samples:
            raise IndexError(
                f"æ ·æœ¬ç´¢å¼• {args.index} è¶…å‡ºèŒƒå›´ï¼Œ"
                f"æ–‡ä»¶ä¸­å…±æœ‰ {loader.num_samples} ä¸ªæ ·æœ¬"
            )
        
        # 2. åŠ è½½æŒ‡å®šçš„ç‚¹äº‘æ ·æœ¬
        logger.info(f"æ­£åœ¨åŠ è½½æ ·æœ¬ {args.index}...")
        point_cloud = loader.load_single_cloud(args.index)
        
        # 3. ä¿å­˜åŸå§‹ç‚¹äº‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.save_original:
            # ç¡®å®šåŸå§‹ç‚¹äº‘è¾“å‡ºè·¯å¾„
            if args.original_output:
                original_output = args.original_output
            else:
                # åŸºäºè¾“å‡ºæ–‡ä»¶è·¯å¾„ç”ŸæˆåŸå§‹ç‚¹äº‘æ–‡ä»¶å
                original_output = args.output.replace('.tiff', '_original.csv').replace('.tif', '_original.csv')
            
            logger.info(f"æ­£åœ¨ä¿å­˜åŸå§‹ç‚¹äº‘åˆ°: {original_output}")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            original_output_dir = os.path.dirname(original_output)
            if original_output_dir and not os.path.exists(original_output_dir):
                os.makedirs(original_output_dir, exist_ok=True)
            
            # ä¿å­˜åŸå§‹ç‚¹äº‘
            converter_temp = PointCloudToVoxel()  # ä¸´æ—¶åˆ›å»ºè½¬æ¢å™¨ç”¨äºä¿å­˜åŠŸèƒ½
            converter_temp.save_point_cloud(point_cloud, original_output)
        
        # 4. åˆ†æç‚¹äº‘
        point_stats = analyze_point_cloud(point_cloud)
        
        # 5. åˆ›å»ºä½“ç´ è½¬æ¢å™¨
        logger.info(f"åˆ›å»ºä½“ç´ è½¬æ¢å™¨ (æ–¹æ³•: {args.method}, å¤§å°: {args.voxel_size})")
        logger.info(f"ä½“ç§¯å°ºå¯¸: {args.volume_dims} nm")
        logger.info(f"å¡«å……: {args.padding} nm")
        converter = PointCloudToVoxel(
            voxel_size=args.voxel_size,
            method=args.method,
            padding_ratio=args.padding_ratio,
            volume_dims=args.volume_dims,
            padding=args.padding
        )
        
        # 6. æ‰§è¡Œè½¬æ¢
        logger.info("æ­£åœ¨æ‰§è¡Œç‚¹äº‘åˆ°ä½“ç´ çš„è½¬æ¢...")
        if args.method == 'gaussian':
            voxel_grid = converter.convert(point_cloud, sigma=args.sigma)
        else:
            voxel_grid = converter.convert(point_cloud)
        
        # 7. åˆ†æä½“ç´ ç½‘æ ¼
        voxel_stats = analyze_voxel_grid(voxel_grid)
        
        # 8. ä½“ç´ ä¸Šé‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
        upsampled_grid = None
        upsampled_stats = None
        if args.upsample:
            logger.info(f"æ­£åœ¨è¿›è¡Œä½“ç´ ä¸Šé‡‡æ · (å€æ•°: {args.upsample_factor}, æ–¹æ³•: {args.upsample_method})")
            upsampled_grid = converter.upsample_voxel_grid(
                voxel_grid, 
                scale_factor=args.upsample_factor, 
                method=args.upsample_method
            )
            upsampled_stats = analyze_voxel_grid(upsampled_grid)
            
            # ä¿å­˜ä¸Šé‡‡æ ·çš„ä½“ç´ ç½‘æ ¼
            upsampled_output = args.output.replace('.tiff', '_upsampled.tiff')
            logger.info(f"æ­£åœ¨ä¿å­˜ä¸Šé‡‡æ ·ä½“ç´ ç½‘æ ¼åˆ°: {upsampled_output}")
            converter.save_as_tiff(upsampled_grid, upsampled_output)
        
        # 9. ä½“ç´ é‡‡æ ·å›ç‚¹äº‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
        sampled_point_cloud = None
        sampled_stats = None
        comparison = None
        if args.sample_back:
            logger.info(f"æ­£åœ¨å°†ä½“ç´ é‡‡æ ·å›ç‚¹äº‘ (æ–¹æ³•: {args.sample_method}, é˜ˆå€¼: {args.sample_threshold})")
            # é€‰æ‹©è¦é‡‡æ ·çš„ä½“ç´ ç½‘æ ¼ï¼ˆä¸Šé‡‡æ ·åçš„æˆ–åŸå§‹çš„ï¼‰
            grid_to_sample = upsampled_grid if upsampled_grid is not None else voxel_grid
            
            sampled_point_cloud = converter.voxel_to_points(
                grid_to_sample,
                threshold=args.sample_threshold,
                num_points=args.sample_num_points,
                method=args.sample_method
            )
            
            if len(sampled_point_cloud) > 0:
                sampled_stats = analyze_point_cloud(sampled_point_cloud)
                comparison = compare_point_clouds(point_stats, sampled_stats)
                
                # ä¿å­˜é‡‡æ ·çš„ç‚¹äº‘ä¸ºCSVæ ¼å¼
                sampled_output = args.output.replace('.tiff', '_sampled.csv')
                logger.info(f"æ­£åœ¨ä¿å­˜é‡‡æ ·ç‚¹äº‘åˆ°: {sampled_output}")
                converter.save_point_cloud(sampled_point_cloud, sampled_output)
            else:
                logger.warning("é‡‡æ ·å¾—åˆ°çš„ç‚¹äº‘ä¸ºç©º")
        
        # 10. ä¿å­˜åŸå§‹ä½“ç´ ç½‘æ ¼TIFFæ–‡ä»¶
        logger.info(f"æ­£åœ¨ä¿å­˜ä½“ç´ ç½‘æ ¼åˆ°: {args.output}")
        converter.save_as_tiff(voxel_grid, args.output)
        
        # 11. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if args.verbose:
            print_statistics(point_stats, voxel_stats, sampled_stats, comparison, upsampled_stats)
        
        # 12. ä¿å­˜è½¬æ¢ä¿¡æ¯
        conversion_info = converter.get_conversion_info()
        info_file = args.output.replace('.tiff', '_info.txt').replace('.tif', '_info.txt')
        
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write("ç‚¹äº‘åˆ°ä½“ç´ è½¬æ¢ä¿¡æ¯\n")
            f.write("="*50 + "\n\n")
            f.write(f"è¾“å…¥æ–‡ä»¶: {args.input}\n")
            f.write(f"æ ·æœ¬ç´¢å¼•: {args.index}\n")
            f.write(f"è¾“å‡ºæ–‡ä»¶: {args.output}\n")
            
            if args.save_original:
                if args.original_output:
                    original_output = args.original_output
                else:
                    original_output = args.output.replace('.tiff', '_original.csv').replace('.tif', '_original.csv')
                f.write(f"åŸå§‹ç‚¹äº‘æ–‡ä»¶: {original_output}\n")
            
            f.write("\n")
            
            f.write("è½¬æ¢å‚æ•°:\n")
            for key, value in conversion_info.items():
                f.write(f"  {key}: {value}\n")
            
            if args.upsample:
                f.write("\nä¸Šé‡‡æ ·å‚æ•°:\n")
                f.write(f"  upsample_factor: {args.upsample_factor}\n")
                f.write(f"  upsample_method: {args.upsample_method}\n")
            
            if args.sample_back:
                f.write("\né‡‡æ ·å‚æ•°:\n")
                f.write(f"  sample_threshold: {args.sample_threshold}\n")
                f.write(f"  sample_num_points: {args.sample_num_points}\n")
                f.write(f"  sample_method: {args.sample_method}\n")
            
            f.write("\nåŸå§‹ç‚¹äº‘ç»Ÿè®¡:\n")
            for key, value in point_stats.items():
                f.write(f"  {key}: {value}\n")
            
            f.write("\nä½“ç´ ç»Ÿè®¡:\n")
            for key, value in voxel_stats.items():
                f.write(f"  {key}: {value}\n")
            
            if upsampled_stats:
                f.write("\nä¸Šé‡‡æ ·ä½“ç´ ç»Ÿè®¡:\n")
                for key, value in upsampled_stats.items():
                    f.write(f"  {key}: {value}\n")
            
            if sampled_stats:
                f.write("\né‡‡æ ·ç‚¹äº‘ç»Ÿè®¡:\n")
                for key, value in sampled_stats.items():
                    f.write(f"  {key}: {value}\n")
            
            if comparison:
                f.write("\nç‚¹äº‘å¯¹æ¯”ç»“æœ:\n")
                for key, value in comparison.items():
                    f.write(f"  {key}: {value}\n")
        
        logger.info(f"è½¬æ¢ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")
        
        # 13. è¾“å‡ºæ€»ç»“
        print("\nâœ… è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“Š è¾“å…¥: {point_stats['num_points']:,} ä¸ªç‚¹")
        print(f"ğŸ“¦ ä½“ç´ ç½‘æ ¼: {voxel_stats['shape']}")
        print(f"ğŸ’¾ TIFFæ–‡ä»¶: {args.output}")
        
        if args.save_original:
            if args.original_output:
                original_output = args.original_output
            else:
                original_output = args.output.replace('.tiff', '_original.csv').replace('.tif', '_original.csv')
            print(f"ğŸ“„ åŸå§‹ç‚¹äº‘: {original_output}")
        
        if args.upsample and upsampled_grid is not None:
            upsampled_output = args.output.replace('.tiff', '_upsampled.tiff').replace('.tif', '_upsampled.tif')
            print(f"ğŸ” ä¸Šé‡‡æ ·ç½‘æ ¼: {upsampled_stats['shape']}")
            print(f"ğŸ’¾ ä¸Šé‡‡æ ·TIFF: {upsampled_output}")
        
        if args.sample_back and sampled_point_cloud is not None and len(sampled_point_cloud) > 0:
            sampled_output = args.output.replace('.tiff', '_sampled.csv')
            print(f"ğŸ¯ é‡‡æ ·ç‚¹äº‘: {len(sampled_point_cloud):,} ä¸ªç‚¹")
            print(f"ğŸ’¾ é‡‡æ ·CSV: {sampled_output}")
        
        print(f"ğŸ“‹ ä¿¡æ¯æ–‡ä»¶: {info_file}")
        
    except Exception as e:
        logger.error(f"è½¬æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        if args.verbose:
            logging.getLogger().setLevel(logging.DEBUG)
        
        # éªŒè¯è¾“å…¥
        validate_inputs(args)
        
        # æ‰§è¡Œè½¬æ¢æµ‹è¯•
        test_conversion_pipeline(args)
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
