#!/usr/bin/env python3
"""
æ‰¹é‡H5ç‚¹äº‘è½¬æ¢è„šæœ¬(for å®éªŒæ•°æ®)

æ­¤è„šæœ¬ç”¨äºæ‰¹é‡å¤„ç†H5æ–‡ä»¶ä¸­çš„æ‰€æœ‰ç‚¹äº‘æ ·æœ¬ï¼Œå°†æ¯ä¸ªç‚¹äº‘è½¬æ¢ä¸ºä½“ç´ ç½‘æ ¼ï¼Œ
ç„¶åé‡‡æ ·å›ç‚¹äº‘ï¼Œæœ€ç»ˆå°†æ‰€æœ‰ç”Ÿæˆçš„ç‚¹äº‘ä¿å­˜ä¸ºæ–°çš„H5æ–‡ä»¶ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/batch_h5_conversion.py --input data/input.h5 --output data/output.h5
"""

import argparse
import sys
import os
import time
import numpy as np
import h5py
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = Path(__file__).parent
project_root = script_dir.parent
sys.path.insert(0, str(project_root))

from src.data.h5_loader import PointCloudH5Loader
from src.voxel.converter import PointCloudToVoxel

import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°

    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°
    """
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡å¤„ç†H5æ–‡ä»¶ä¸­çš„æ‰€æœ‰ç‚¹äº‘æ ·æœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
    # åŸºæœ¬æ‰¹é‡è½¬æ¢ï¼ˆé«˜æ–¯æ–¹æ³•ï¼‰
    python scripts/batch_h5_conversion.py --input data/input.h5 --output data/output.h5
    
    # è‡ªå®šä¹‰ä½“ç´ åŒ–å‚æ•°
    python scripts/batch_h5_conversion.py --input data/input.h5 --output data/output.h5 --method density --voxel-size 128
    
    # åŒ…å«ä¸Šé‡‡æ ·çš„å®Œæ•´æµç¨‹
    python scripts/batch_h5_conversion.py --input data/input.h5 --output data/output.h5 --upsample --upsample-factor 2.0 --sample-num-points 200000
    
    # è‡ªå®šä¹‰ä½“ç§¯å‚æ•°
    python scripts/batch_h5_conversion.py --input data/input.h5 --output data/output.h5 --volume-dims 15000 15000 3000 --padding 50 50 150
        """,
    )

    parser.add_argument(
        "--input",
        "-i",
        default="/repos/datasets/batch_simulation_microtubule_20251017_2048.h5",
        type=str,
        help="è¾“å…¥H5æ–‡ä»¶è·¯å¾„",
    )

    parser.add_argument(
        "--output",
        "-o",
        default="/repos/datasets/batch_simulation_microtubule_20251017_2048_noised.h5",
        type=str,
        help="è¾“å‡ºH5æ–‡ä»¶è·¯å¾„",
    )

    parser.add_argument(
        "--method",
        type=str,
        choices=["occupancy", "density", "gaussian"],
        default="gaussian",
        help="ä½“ç´ åŒ–æ–¹æ³• (é»˜è®¤: gaussian)",
    )

    parser.add_argument(
        "--voxel-size", type=int, default=256, help="ä½“ç´ ç½‘æ ¼åˆ†è¾¨ç‡ (é»˜è®¤: 256)"
    )

    parser.add_argument(
        "--sigma", type=float, default=1.0, help="é«˜æ–¯æ–¹æ³•çš„æ ‡å‡†å·® (é»˜è®¤: 1.0)"
    )

    parser.add_argument(
        "--data-key",
        type=str,
        default="point_clouds",
        help="H5æ–‡ä»¶ä¸­æ•°æ®çš„é”®å (é»˜è®¤: point_clouds)",
    )

    parser.add_argument(
        "--output-key",
        type=str,
        default="point_clouds",
        help="è¾“å‡ºH5æ–‡ä»¶ä¸­æ•°æ®çš„é”®å (é»˜è®¤: point_clouds)",
    )

    parser.add_argument(
        "--padding-ratio", type=float, default=0.00, help="è¾¹ç•Œæ‰©å±•æ¯”ä¾‹ (é»˜è®¤: 0.00)"
    )

    parser.add_argument(
        "--volume-dims",
        type=float,
        nargs=3,
        default=[20000, 20000, 600],
        help="ä½“ç§¯å°ºå¯¸ [x, y, z] (å•ä½: nm) (é»˜è®¤: [20000, 20000, 2500])",
    )

    parser.add_argument(
        "--padding",
        type=float,
        nargs=3,
        default=[0, 0, 200],
        help="ä½“ç§¯è¾¹ç•Œå¡«å…… [x, y, z] (å•ä½: nm) (é»˜è®¤: [0, 0, 100])",
    )

    # ä½“ç´ é‡‡æ ·å›ç‚¹äº‘å‚æ•°
    parser.add_argument(
        "--sample-num-points",
        type=int,
        default=100000,
        help="é‡‡æ ·å›ç‚¹äº‘çš„ç›®æ ‡ç‚¹æ•° (é»˜è®¤: 100000)",
    )

    parser.add_argument(
        "--sample-threshold",
        type=float,
        default=0.0,
        help="ä½“ç´ å€¼é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„ä½“ç´ è¢«è§†ä¸ºåŒ…å«ç‚¹ (é»˜è®¤: 0.0)",
    )

    parser.add_argument(
        "--sample-method",
        type=str,
        choices=["probabilistic", "center", "random", "weighted"],
        default="probabilistic",
        help="é‡‡æ ·æ–¹æ³• (é»˜è®¤: probabilistic)",
    )

    # ä½“ç´ ä¸Šé‡‡æ ·å‚æ•°
    parser.add_argument(
        "--upsample", action="store_true", help="æ˜¯å¦å¯¹ä½“ç´ ç½‘æ ¼è¿›è¡Œä¸Šé‡‡æ ·"
    )

    parser.add_argument(
        "--upsample-factor", type=float, default=1.0, help="ä¸Šé‡‡æ ·å€æ•° (é»˜è®¤: 2.0)"
    )

    parser.add_argument(
        "--upsample-method",
        type=str,
        choices=["linear", "nearest", "cubic"],
        default="linear",
        help="ä¸Šé‡‡æ ·æ’å€¼æ–¹æ³• (é»˜è®¤: linear)",
    )

    # å¤„ç†å‚æ•°
    parser.add_argument(
        "--batch-size", type=int, default=1, help="æ‰¹å¤„ç†å¤§å°ï¼Œç”¨äºå†…å­˜ç®¡ç† (é»˜è®¤: 1)"
    )

    parser.add_argument(
        "--start-index", type=int, default=0, help="å¼€å§‹å¤„ç†çš„æ ·æœ¬ç´¢å¼• (é»˜è®¤: 0)"
    )

    parser.add_argument(
        "--end-index",
        type=int,
        default=None,
        help="ç»“æŸå¤„ç†çš„æ ·æœ¬ç´¢å¼•ï¼ŒNoneè¡¨ç¤ºå¤„ç†åˆ°æœ€å (é»˜è®¤: None)",
    )

    parser.add_argument("--verbose", "-v", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯")

    parser.add_argument(
        "--skip-errors", action="store_true", help="è·³è¿‡é”™è¯¯çš„æ ·æœ¬ç»§ç»­å¤„ç†"
    )

    return parser.parse_args()


def validate_inputs(args):
    """
    éªŒè¯è¾“å…¥å‚æ•°

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°

    Raises:
        FileNotFoundError: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: å‚æ•°å€¼æ— æ•ˆ
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")

    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        logger.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        os.makedirs(output_dir, exist_ok=True)

    # éªŒè¯å‚æ•°èŒƒå›´
    if args.voxel_size <= 0:
        raise ValueError("ä½“ç´ å¤§å°å¿…é¡»å¤§äº0")

    if not 0 <= args.padding_ratio <= 1:
        raise ValueError("padding_ratioå¿…é¡»åœ¨[0, 1]èŒƒå›´å†…")

    if args.sigma <= 0:
        raise ValueError("sigmaå¿…é¡»å¤§äº0")

    if args.sample_threshold < 0 or args.sample_threshold > 1:
        raise ValueError("sample_thresholdå¿…é¡»åœ¨[0, 1]èŒƒå›´å†…")

    if args.sample_num_points <= 0:
        raise ValueError("sample_num_pointså¿…é¡»å¤§äº0")

    if args.upsample and args.upsample_factor <= 1.0:
        raise ValueError("upsample_factorå¿…é¡»å¤§äº1.0")

    if args.batch_size <= 0:
        raise ValueError("batch_sizeå¿…é¡»å¤§äº0")

    if args.start_index < 0:
        raise ValueError("start_indexå¿…é¡»éè´Ÿ")


def process_single_point_cloud(
    point_cloud: np.ndarray, converter: PointCloudToVoxel, args
) -> np.ndarray:
    """
    å¤„ç†å•ä¸ªç‚¹äº‘æ ·æœ¬

    Args:
        point_cloud (np.ndarray): è¾“å…¥ç‚¹äº‘
        converter (PointCloudToVoxel): ä½“ç´ è½¬æ¢å™¨
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        np.ndarray: ç”Ÿæˆçš„ç‚¹äº‘
    """
    # 1. ç‚¹äº‘è½¬ä½“ç´ 
    if args.method == "gaussian":
        voxel_grid = converter.convert(point_cloud, sigma=args.sigma)
    else:
        voxel_grid = converter.convert(point_cloud)

    # 2. ä½“ç´ ä¸Šé‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.upsample:
        voxel_grid = converter.upsample_voxel_grid(
            voxel_grid, scale_factor=args.upsample_factor, method=args.upsample_method
        )

    # 3. ä½“ç´ é‡‡æ ·å›ç‚¹äº‘
    generated_point_cloud = converter.voxel_to_points(
        voxel_grid,
        threshold=args.sample_threshold,
        num_points=args.sample_num_points,
        method=args.sample_method,
    )

    return generated_point_cloud


def get_max_points_count(generated_clouds: list) -> int:
    """
    è·å–ç”Ÿæˆç‚¹äº‘ä¸­çš„æœ€å¤§ç‚¹æ•°ï¼Œç”¨äºåˆ›å»ºç»Ÿä¸€çš„æ•°ç»„å½¢çŠ¶

    Args:
        generated_clouds (list): ç”Ÿæˆçš„ç‚¹äº‘åˆ—è¡¨

    Returns:
        int: æœ€å¤§ç‚¹æ•°
    """
    return max(len(cloud) for cloud in generated_clouds if len(cloud) > 0)


def pad_point_clouds(generated_clouds: list, max_points: int) -> np.ndarray:
    """
    å°†ç‚¹äº‘å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦

    Args:
        generated_clouds (list): ç”Ÿæˆçš„ç‚¹äº‘åˆ—è¡¨
        max_points (int): ç›®æ ‡ç‚¹æ•°

    Returns:
        np.ndarray: å¡«å……åçš„ç‚¹äº‘æ•°ç»„ (num_samples, max_points, 3)
    """
    num_samples = len(generated_clouds)
    padded_clouds = np.zeros((num_samples, max_points, 3), dtype=np.float32)

    for i, cloud in enumerate(generated_clouds):
        if len(cloud) > 0:
            # å¦‚æœç‚¹äº‘é•¿åº¦è¶…è¿‡max_pointsï¼Œéšæœºé‡‡æ ·
            if len(cloud) > max_points:
                indices = np.random.choice(len(cloud), max_points, replace=False)
                padded_clouds[i] = cloud[indices]
            else:
                # å¦åˆ™ç›´æ¥å¡«å……ï¼Œå¤šä½™ä½ç½®ä¿æŒ0
                padded_clouds[i, : len(cloud)] = cloud

    return padded_clouds


def batch_h5_conversion(args):
    """
    æ‰§è¡Œæ‰¹é‡H5ç‚¹äº‘è½¬æ¢

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    try:
        # 1. åŠ è½½è¾“å…¥H5æ•°æ®
        logger.info(f"æ­£åœ¨åŠ è½½H5æ•°æ®æ–‡ä»¶: {args.input}")
        loader = PointCloudH5Loader(args.input, data_key=args.data_key)

        # å°è¯•è¯»å–sample_ids
        sample_ids = None
        try:
            with h5py.File(args.input, "r") as f:
                if "sample_ids" in f:
                    sample_ids = f["sample_ids"][:]
                    logger.info(f"å‘ç°sample_idsæ•°æ®ï¼Œå…± {len(sample_ids)} ä¸ªæ ·æœ¬ID")
                else:
                    logger.warning("è¾“å…¥æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°sample_idsï¼Œå°†ä½¿ç”¨ç´¢å¼•ä½œä¸ºID")
        except Exception as e:
            logger.warning(f"è¯»å–sample_idsæ—¶å‡ºé”™: {e}")
            sample_ids = None

        # ç¡®å®šå¤„ç†èŒƒå›´
        total_samples = loader.num_samples
        total_samples = 3
        start_idx = args.start_index
        end_idx = args.end_index if args.end_index is not None else total_samples
        end_idx = min(end_idx, total_samples)

        if start_idx >= total_samples:
            raise ValueError(f"start_index {start_idx} è¶…å‡ºæ ·æœ¬æ•°é‡ {total_samples}")

        logger.info(f"æ•°æ®é›†åŒ…å« {total_samples} ä¸ªæ ·æœ¬")
        logger.info(
            f"å¤„ç†èŒƒå›´: {start_idx} åˆ° {end_idx - 1} (å…± {end_idx - start_idx} ä¸ªæ ·æœ¬)"
        )

        # 2. åˆ›å»ºä½“ç´ è½¬æ¢å™¨
        logger.info(f"åˆ›å»ºä½“ç´ è½¬æ¢å™¨ (æ–¹æ³•: {args.method}, å¤§å°: {args.voxel_size})")
        converter = PointCloudToVoxel(
            voxel_size=args.voxel_size,
            method=args.method,
            padding_ratio=args.padding_ratio,
            volume_dims=args.volume_dims,
            padding=args.padding,
        )

        # 3. æ‰¹é‡å¤„ç†ç‚¹äº‘
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†ç‚¹äº‘...")
        generated_clouds = []
        failed_indices = []
        processing_stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "empty_results": 0,
        }

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        progress_bar = tqdm(
            range(start_idx, end_idx, args.batch_size), desc="å¤„ç†ä¸­", unit="batch"
        )

        for batch_start in progress_bar:
            batch_end = min(batch_start + args.batch_size, end_idx)
            batch_indices = list(range(batch_start, batch_end))

            for idx in batch_indices:
                processing_stats["total_processed"] += 1

                try:
                    # åŠ è½½ç‚¹äº‘
                    point_cloud = loader.load_single_cloud(idx)

                    # å¤„ç†ç‚¹äº‘
                    generated_cloud = process_single_point_cloud(
                        point_cloud, converter, args
                    )

                    if len(generated_cloud) == 0:
                        processing_stats["empty_results"] += 1
                        if args.verbose:
                            logger.warning(f"æ ·æœ¬ {idx} ç”Ÿæˆçš„ç‚¹äº‘ä¸ºç©º")

                        if not args.skip_errors:
                            raise ValueError(f"æ ·æœ¬ {idx} ç”Ÿæˆçš„ç‚¹äº‘ä¸ºç©º")

                    generated_clouds.append(generated_cloud)
                    processing_stats["successful"] += 1

                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    progress_bar.set_postfix(
                        {
                            "Success": processing_stats["successful"],
                            "Failed": processing_stats["failed"],
                            "Empty": processing_stats["empty_results"],
                        }
                    )

                except Exception as e:
                    processing_stats["failed"] += 1
                    failed_indices.append(idx)

                    if args.skip_errors:
                        logger.warning(f"å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                        # æ·»åŠ ç©ºçš„ç‚¹äº‘å ä½
                        generated_clouds.append(np.empty((0, 3)))
                        continue
                    else:
                        logger.error(f"å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
                        raise

        progress_bar.close()

        # 4. æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„ç»“æœ
        if processing_stats["successful"] == 0:
            raise RuntimeError("æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬")

        logger.info(
            f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {processing_stats['successful']}, "
            f"å¤±è´¥: {processing_stats['failed']}, "
            f"ç©ºç»“æœ: {processing_stats['empty_results']}"
        )

        if failed_indices:
            logger.warning(f"å¤±è´¥çš„æ ·æœ¬ç´¢å¼•: {failed_indices}")

        # 5. å‡†å¤‡ä¿å­˜æ•°æ®
        logger.info("æ­£åœ¨å‡†å¤‡è¾“å‡ºæ•°æ®...")

        # è·å–æœ€å¤§ç‚¹æ•°å¹¶å¡«å……
        valid_clouds = [cloud for cloud in generated_clouds if len(cloud) > 0]
        if not valid_clouds:
            raise RuntimeError("æ²¡æœ‰æœ‰æ•ˆçš„ç”Ÿæˆç‚¹äº‘")

        max_points = get_max_points_count(valid_clouds)
        logger.info(f"æœ€å¤§ç‚¹æ•°: {max_points}")

        # å¡«å……ç‚¹äº‘åˆ°ç»Ÿä¸€å½¢çŠ¶
        padded_clouds = pad_point_clouds(generated_clouds, max_points)
        # padded_clouds = generated_clouds

        # 6. å‡†å¤‡å¯¹åº”çš„sample_ids
        output_sample_ids = None
        if sample_ids is not None:
            # æå–å¤„ç†èŒƒå›´å†…çš„sample_ids
            output_sample_ids = sample_ids[start_idx:end_idx]
            logger.info(f"æå–å¯¹åº”çš„sample_ids: {len(output_sample_ids)} ä¸ª")
        else:
            # ä½¿ç”¨ç´¢å¼•ä½œä¸ºID
            output_sample_ids = np.arange(start_idx, end_idx)
            logger.info(f"ä½¿ç”¨ç´¢å¼•ä½œä¸ºsample_ids: {start_idx} åˆ° {end_idx - 1}")

        # 7. ä¿å­˜åˆ°H5æ–‡ä»¶
        logger.info(f"æ­£åœ¨ä¿å­˜åˆ°H5æ–‡ä»¶: {args.output}")

        with h5py.File(args.output, "w") as f:
            # ä¿å­˜ä¸»è¦æ•°æ®
            dataset = f.create_dataset(
                args.output_key,
                data=padded_clouds,
                compression="gzip",
                compression_opts=9,
            )

            # ä¿å­˜sample_idsï¼ˆä¿æŒä¸åŸå§‹æ–‡ä»¶çš„å¯¹åº”å…³ç³»ï¼‰
            f.create_dataset(
                "sample_ids",
                data=output_sample_ids,
                compression="gzip",
                compression_opts=9,
            )

            # ä¿å­˜å…ƒæ•°æ®
            f.attrs["original_file"] = args.input
            f.attrs["original_data_key"] = args.data_key
            f.attrs["total_samples"] = len(generated_clouds)
            f.attrs["max_points_per_sample"] = max_points
            f.attrs["processing_range"] = f"{start_idx}-{end_idx - 1}"
            f.attrs["successful_samples"] = processing_stats["successful"]
            f.attrs["failed_samples"] = processing_stats["failed"]
            f.attrs["empty_results"] = processing_stats["empty_results"]

            # ä¿å­˜è½¬æ¢å‚æ•°
            conv_params = f.create_group("conversion_parameters")
            conv_params.attrs["method"] = args.method
            conv_params.attrs["voxel_size"] = args.voxel_size
            conv_params.attrs["sigma"] = args.sigma
            conv_params.attrs["padding_ratio"] = args.padding_ratio
            conv_params.attrs["volume_dims"] = args.volume_dims
            conv_params.attrs["padding"] = args.padding
            conv_params.attrs["sample_threshold"] = args.sample_threshold
            conv_params.attrs["sample_num_points"] = args.sample_num_points
            conv_params.attrs["sample_method"] = args.sample_method
            conv_params.attrs["upsample"] = args.upsample
            conv_params.attrs["upsample_factor"] = args.upsample_factor
            conv_params.attrs["upsample_method"] = args.upsample_method

            # ä¿å­˜å¤±è´¥ç´¢å¼•ï¼ˆå¦‚æœæœ‰ï¼‰
            if failed_indices:
                f.create_dataset("failed_indices", data=failed_indices)

            # ä¿å­˜å®é™…ç‚¹æ•°ä¿¡æ¯
            actual_point_counts = [len(cloud) for cloud in generated_clouds]
            f.create_dataset("actual_point_counts", data=actual_point_counts)

        # 8. è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 60)
        print("âœ… æ‰¹é‡è½¬æ¢å®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
        print(f"ğŸ“Š å¤„ç†æ ·æœ¬: {processing_stats['total_processed']}")
        print(f"âœ… æˆåŠŸ: {processing_stats['successful']}")
        print(f"âŒ å¤±è´¥: {processing_stats['failed']}")
        print(f"ğŸ”¸ ç©ºç»“æœ: {processing_stats['empty_results']}")
        print(f"ğŸ“¦ è¾“å‡ºå½¢çŠ¶: {padded_clouds.shape}")
        print(f"ğŸ¯ æœ€å¤§ç‚¹æ•°: {max_points}")
        print(
            f"ğŸ”— Sample IDs: {'ä¿ç•™åŸå§‹å¯¹åº”å…³ç³»' if sample_ids is not None else 'ä½¿ç”¨ç´¢å¼•ID'}"
        )

        if failed_indices:
            print(
                f"âš ï¸  å¤±è´¥ç´¢å¼•: {failed_indices[:10]}{'...' if len(failed_indices) > 10 else ''}"
            )

        print("=" * 60)

        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report_file = args.output.replace(".h5", "_report.txt")
        with open(report_file, "w", encoding="utf-8") as f:
            f.write("æ‰¹é‡H5ç‚¹äº‘è½¬æ¢æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥æ–‡ä»¶: {args.input}\n")
            f.write(f"è¾“å‡ºæ–‡ä»¶: {args.output}\n")
            f.write(f"å¤„ç†èŒƒå›´: {start_idx} åˆ° {end_idx - 1}\n\n")

            f.write("å¤„ç†ç»Ÿè®¡:\n")
            f.write(f"  æ€»å¤„ç†æ ·æœ¬: {processing_stats['total_processed']}\n")
            f.write(f"  æˆåŠŸæ ·æœ¬: {processing_stats['successful']}\n")
            f.write(f"  å¤±è´¥æ ·æœ¬: {processing_stats['failed']}\n")
            f.write(f"  ç©ºç»“æœæ ·æœ¬: {processing_stats['empty_results']}\n\n")

            f.write("è½¬æ¢å‚æ•°:\n")
            f.write(f"  method: {args.method}\n")
            f.write(f"  voxel_size: {args.voxel_size}\n")
            f.write(f"  sigma: {args.sigma}\n")
            f.write(f"  sample_method: {args.sample_method}\n")
            f.write(f"  sample_num_points: {args.sample_num_points}\n")
            f.write(f"  upsample: {args.upsample}\n")
            if args.upsample:
                f.write(f"  upsample_factor: {args.upsample_factor}\n")
                f.write(f"  upsample_method: {args.upsample_method}\n")

            f.write(f"\nè¾“å‡ºæ•°æ®å½¢çŠ¶: {padded_clouds.shape}\n")
            f.write(f"æœ€å¤§ç‚¹æ•°: {max_points}\n")
            f.write(
                f"Sample IDs: {'ä¿ç•™åŸå§‹å¯¹åº”å…³ç³»' if sample_ids is not None else 'ä½¿ç”¨ç´¢å¼•ID'}\n"
            )

            if failed_indices:
                f.write(f"\nå¤±è´¥çš„æ ·æœ¬ç´¢å¼•:\n")
                for idx in failed_indices:
                    f.write(f"  {idx}\n")

            if output_sample_ids is not None:
                f.write(
                    f"\nSample IDsèŒƒå›´: {output_sample_ids[0]} åˆ° {output_sample_ids[-1]}\n"
                )

        logger.info(f"å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    except Exception as e:
        logger.error(f"æ‰¹é‡è½¬æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()

        # è®¾ç½®æ—¥å¿—çº§åˆ«
        if args.verbose:
            logging.getLogger().setLevel(logging.DEBUG)

        # éªŒè¯è¾“å…¥
        validate_inputs(args)

        # æ‰§è¡Œæ‰¹é‡è½¬æ¢
        batch_h5_conversion(args)

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


def test_h5_correspondence(
    gt_file: str,
    noisy_file: str,
    sample_index: int = 0,
    output_dir: str = "test_correspondence",
    data_key: str = "point_clouds",
    args: argparse.Namespace = None,
):
    """
    æµ‹è¯•GTå’Œè½¬æ¢åH5æ–‡ä»¶çš„å¯¹åº”å…³ç³»

    Args:
        gt_file (str): åŸå§‹GT H5æ–‡ä»¶è·¯å¾„
        noisy_file (str): è½¬æ¢åçš„H5æ–‡ä»¶è·¯å¾„
        sample_index (int): è¦æµ‹è¯•çš„æ ·æœ¬ç´¢å¼•
        output_dir (str): è¾“å‡ºç›®å½•
        data_key (str): æ•°æ®é”®å
    """
    try:
        import tifffile

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # è¯»å–GTæ–‡ä»¶
        logger.info(f"è¯»å–GTæ–‡ä»¶: {gt_file}")
        gt_loader = PointCloudH5Loader(gt_file, data_key=args.data_key)

        # æ£€æŸ¥æ ·æœ¬ç´¢å¼•
        if sample_index >= gt_loader.num_samples:
            raise ValueError(
                f"æ ·æœ¬ç´¢å¼• {sample_index} è¶…å‡ºGTæ–‡ä»¶æ ·æœ¬æ•° {gt_loader.num_samples}"
            )

        # è¯»å–GTç‚¹äº‘
        gt_point_cloud = gt_loader.load_single_cloud(sample_index)

        # è¯»å–GTçš„sample_ids
        gt_sample_ids = None
        try:
            with h5py.File(gt_file, "r") as f:
                if "sample_ids" in f:
                    gt_sample_ids = f["sample_ids"][:]
                    logger.info(f"GTæ–‡ä»¶sample_idsæ•°é‡: {len(gt_sample_ids)}")
        except Exception as e:
            logger.warning(f"è¯»å–GT sample_idså¤±è´¥: {e}")

        # è¯»å–è½¬æ¢åæ–‡ä»¶
        logger.info(f"è¯»å–è½¬æ¢åæ–‡ä»¶: {noisy_file}")
        noisy_loader = PointCloudH5Loader(noisy_file, data_key=args.output_key)

        # æ£€æŸ¥æ ·æœ¬ç´¢å¼•
        if sample_index >= noisy_loader.num_samples:
            raise ValueError(
                f"æ ·æœ¬ç´¢å¼• {sample_index} è¶…å‡ºè½¬æ¢åæ–‡ä»¶æ ·æœ¬æ•° {noisy_loader.num_samples}"
            )

        # è¯»å–è½¬æ¢åç‚¹äº‘
        noisy_point_cloud = noisy_loader.load_single_cloud(sample_index)
        noisy_point_cloud = np.clip(
            noisy_point_cloud,
            [0 - args.padding[0], 0 - args.padding[1], 0 - args.padding[2]],
            [
                args.volume_dims[0] + args.padding[0] - 1,
                args.volume_dims[1] + args.padding[1] - 1,
                args.volume_dims[2] + args.padding[2] - 1,
            ],
        )

        # è¯»å–è½¬æ¢åçš„sample_ids
        noisy_sample_ids = None
        try:
            with h5py.File(noisy_file, "r") as f:
                if "sample_ids" in f:
                    noisy_sample_ids = f["sample_ids"][:]
                    logger.info(f"è½¬æ¢åæ–‡ä»¶sample_idsæ•°é‡: {len(noisy_sample_ids)}")
        except Exception as e:
            logger.warning(f"è¯»å–è½¬æ¢å sample_idså¤±è´¥: {e}")

        # éªŒè¯sample_idså¯¹åº”å…³ç³»
        if gt_sample_ids is not None and noisy_sample_ids is not None:
            if sample_index < len(gt_sample_ids) and sample_index < len(
                noisy_sample_ids
            ):
                gt_id = gt_sample_ids[sample_index]
                noisy_id = noisy_sample_ids[sample_index]
                if gt_id == noisy_id:
                    logger.info(f"âœ… Sample IDå¯¹åº”å…³ç³»æ­£ç¡®: {gt_id}")
                else:
                    logger.warning(
                        f"âš ï¸ Sample IDä¸åŒ¹é…! GT: {gt_id}, è½¬æ¢å: {noisy_id}"
                    )
            else:
                logger.warning("Sample IDç´¢å¼•è¶…å‡ºèŒƒå›´")
        else:
            logger.warning("æ— æ³•éªŒè¯Sample IDå¯¹åº”å…³ç³»")

        # ä¿å­˜ç‚¹äº‘ä¸ºCSVæ–‡ä»¶
        logger.info("ä¿å­˜ç‚¹äº‘ä¸ºCSVæ–‡ä»¶...")

        # ä¿å­˜GTç‚¹äº‘
        gt_csv_path = os.path.join(output_dir, f"gt_sample_{sample_index}.csv")
        with open(gt_csv_path, "w", encoding="utf-8") as f:
            f.write("x[nm],y[nm],z[nm]\n")
            for point in gt_point_cloud:
                f.write(f"{point[0]:.1f},{point[1]:.1f},{point[2]:.1f}\n")
        logger.info(f"GTç‚¹äº‘CSVå·²ä¿å­˜: {gt_csv_path}")

        # ä¿å­˜è½¬æ¢åç‚¹äº‘
        if len(noisy_point_cloud) > 0:
            noisy_csv_path = os.path.join(
                output_dir, f"noisy_sample_{sample_index}.csv"
            )
            with open(noisy_csv_path, "w", encoding="utf-8") as f:
                f.write("x[nm],y[nm],z[nm]\n")
                for point in noisy_point_cloud:
                    f.write(f"{point[0]:.1f},{point[1]:.1f},{point[2]:.1f}\n")
            logger.info(f"è½¬æ¢åç‚¹äº‘CSVå·²ä¿å­˜: {noisy_csv_path}")
        else:
            logger.warning("è½¬æ¢åç‚¹äº‘ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆCSVæ–‡ä»¶")

        # è½¬æ¢ç‚¹äº‘ä¸ºä½“ç´ ç”¨äºå¯è§†åŒ–
        logger.info("è½¬æ¢ç‚¹äº‘ä¸ºä½“ç´ ç½‘æ ¼ç”¨äºå¯è§†åŒ–...")
        converter = PointCloudToVoxel(
            voxel_size=args.voxel_size,
            method="gaussian",
            padding_ratio=args.padding_ratio,
            volume_dims=args.volume_dims,
            padding=args.padding,
        )

        # GTç‚¹äº‘è½¬ä½“ç´ 
        gt_voxel = converter.convert(gt_point_cloud, sigma=args.sigma)
        gt_output = os.path.join(output_dir, f"gt_sample_{sample_index}.tiff")
        converter.save_as_tiff(gt_voxel, gt_output)
        logger.info(f"GTä½“ç´ å·²ä¿å­˜: {gt_output}")

        # è½¬æ¢åç‚¹äº‘è½¬ä½“ç´ 
        if len(noisy_point_cloud) > 0:
            noisy_voxel = converter.convert(noisy_point_cloud, sigma=args.sigma)
            noisy_output = os.path.join(output_dir, f"noisy_sample_{sample_index}.tiff")
            converter.save_as_tiff(noisy_voxel, noisy_output)
            logger.info(f"è½¬æ¢åä½“ç´ å·²ä¿å­˜: {noisy_output}")
        else:
            logger.warning("è½¬æ¢åç‚¹äº‘ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆä½“ç´ ")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æ ·æœ¬ {sample_index} å¯¹åº”å…³ç³»æµ‹è¯•ç»“æœ")
        print("=" * 60)
        print(f"GTç‚¹äº‘ç‚¹æ•°: {len(gt_point_cloud):,}")
        print(f"è½¬æ¢åç‚¹äº‘ç‚¹æ•°: {len(noisy_point_cloud):,}")

        if gt_sample_ids is not None and noisy_sample_ids is not None:
            if sample_index < len(gt_sample_ids) and sample_index < len(
                noisy_sample_ids
            ):
                print(f"GT Sample ID: {gt_sample_ids[sample_index]}")
                print(f"è½¬æ¢å Sample ID: {noisy_sample_ids[sample_index]}")
                print(
                    f"IDåŒ¹é…: {'âœ…' if gt_sample_ids[sample_index] == noisy_sample_ids[sample_index] else 'âŒ'}"
                )

        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡å·®å¼‚
        gt_stats = {
            "min": np.min(gt_point_cloud, axis=0),
            "max": np.max(gt_point_cloud, axis=0),
            "mean": np.mean(gt_point_cloud, axis=0),
            "std": np.std(gt_point_cloud, axis=0),
        }

        if len(noisy_point_cloud) > 0:
            noisy_stats = {
                "min": np.min(noisy_point_cloud, axis=0),
                "max": np.max(noisy_point_cloud, axis=0),
                "mean": np.mean(noisy_point_cloud, axis=0),
                "std": np.std(noisy_point_cloud, axis=0),
            }

            print(f"\nåæ ‡èŒƒå›´å˜åŒ–:")
            for i, axis in enumerate(["X", "Y", "Z"]):
                gt_range = gt_stats["max"][i] - gt_stats["min"][i]
                noisy_range = noisy_stats["max"][i] - noisy_stats["min"][i]
                print(
                    f"  {axis}: GT={gt_range:.2f}, è½¬æ¢å={noisy_range:.2f}, å·®å¼‚={noisy_range - gt_range:.2f}"
                )

            print(f"\nå¹³å‡åæ ‡åç§»:")
            mean_diff = noisy_stats["mean"] - gt_stats["mean"]
            for i, axis in enumerate(["X", "Y", "Z"]):
                print(f"  {axis}: {mean_diff[i]:.2f}")

        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
        print(f"  - GTç‚¹äº‘CSV: gt_sample_{sample_index}.csv")
        if len(noisy_point_cloud) > 0:
            print(f"  - è½¬æ¢åç‚¹äº‘CSV: noisy_sample_{sample_index}.csv")
        print(f"  - GTä½“ç´ TIFF: gt_sample_{sample_index}.tiff")
        if len(noisy_point_cloud) > 0:
            print(f"  - è½¬æ¢åä½“ç´ TIFF: noisy_sample_{sample_index}.tiff")
        print("=" * 60)

        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        report_file = os.path.join(
            output_dir, f"correspondence_test_sample_{sample_index}.txt"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(f"H5æ–‡ä»¶å¯¹åº”å…³ç³»æµ‹è¯•æŠ¥å‘Š - æ ·æœ¬ {sample_index}\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"GTæ–‡ä»¶: {gt_file}\n")
            f.write(f"è½¬æ¢åæ–‡ä»¶: {noisy_file}\n")
            f.write(f"æµ‹è¯•æ ·æœ¬ç´¢å¼•: {sample_index}\n\n")

            f.write("ç‚¹äº‘ç»Ÿè®¡:\n")
            f.write(f"  GTç‚¹æ•°: {len(gt_point_cloud):,}\n")
            f.write(f"  è½¬æ¢åç‚¹æ•°: {len(noisy_point_cloud):,}\n")
            f.write(
                f"  ç‚¹æ•°æ¯”ä¾‹: {len(noisy_point_cloud) / len(gt_point_cloud):.4f}\n\n"
            )

            if gt_sample_ids is not None and noisy_sample_ids is not None:
                if sample_index < len(gt_sample_ids) and sample_index < len(
                    noisy_sample_ids
                ):
                    f.write("Sample IDéªŒè¯:\n")
                    f.write(f"  GT Sample ID: {gt_sample_ids[sample_index]}\n")
                    f.write(f"  è½¬æ¢å Sample ID: {noisy_sample_ids[sample_index]}\n")
                    f.write(
                        f"  IDåŒ¹é…: {'æ˜¯' if gt_sample_ids[sample_index] == noisy_sample_ids[sample_index] else 'å¦'}\n\n"
                    )

            f.write("è¾“å‡ºæ–‡ä»¶:\n")
            f.write(f"  GTç‚¹äº‘CSV: gt_sample_{sample_index}.csv\n")
            if len(noisy_point_cloud) > 0:
                f.write(f"  è½¬æ¢åç‚¹äº‘CSV: noisy_sample_{sample_index}.csv\n")
            f.write(f"  GTä½“ç´ : gt_sample_{sample_index}.tiff\n")
            if len(noisy_point_cloud) > 0:
                f.write(f"  è½¬æ¢åä½“ç´ : noisy_sample_{sample_index}.tiff\n")

        logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    import sys

    # --- test mode ---
    sys.argv.append("test")
    sys.argv.append("/repos/datasets/batch_simulation_microtubule_20251017_2048.h5")
    sys.argv.append(
        "/repos/datasets/batch_simulation_microtubule_20251017_2048_noised.h5"
    )
    sys.argv.append("1")

    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼ï¼špython batch_h5_conversion.py test <gt_file> <noisy_file> [sample_index]
        if len(sys.argv) < 4:
            print(
                "æµ‹è¯•ç”¨æ³•: python batch_h5_conversion.py test <gt_file> <noisy_file> [sample_index]"
            )
            print("ç¤ºä¾‹: python batch_h5_conversion.py test gt.h5 noisy.h5 0")
            sys.exit(1)

        gt_file = sys.argv[2]
        noisy_file = sys.argv[3]
        sample_index = int(sys.argv[4]) if len(sys.argv) > 4 else 0

        # å¤‡ä»½åŸå§‹ sys.argvï¼Œåªè§£ææ­£å¸¸å‚æ•°
        original_argv = sys.argv[:]
        sys.argv = [sys.argv[0]]  # åªä¿ç•™è„šæœ¬å
        args = parse_arguments()
        sys.argv = original_argv  # æ¢å¤åŸå§‹ argv

        logger.info(f"å¼€å§‹æµ‹è¯•H5æ–‡ä»¶å¯¹åº”å…³ç³»...")
        test_h5_correspondence(gt_file, noisy_file, sample_index, args=args)
    else:
        # æ­£å¸¸æ‰¹é‡è½¬æ¢æ¨¡å¼
        main()
